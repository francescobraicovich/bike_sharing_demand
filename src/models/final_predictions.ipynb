{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[310], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# open final data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/processed/final_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get the list of columns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_1940/1086435320.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n",
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_1940/1086435320.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_testing_data['datetime'] = pd.to_datetime(model_testing_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "model_testing_data = data[data['dataset'] == 'test']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n",
    "model_testing_data['datetime'] = pd.to_datetime(model_testing_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime            0\n",
      "season              0\n",
      "holiday             0\n",
      "workingday          0\n",
      "weather             0\n",
      "                 ... \n",
      "casual, -2        238\n",
      "registered, +2    286\n",
      "registered, -2    238\n",
      "count, +2         286\n",
      "count, -2         238\n",
      "Length: 63, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(model_testing_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 0\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': model_testing_data[original_columns + positive_columns]\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': model_testing_data[original_columns + negative_columns]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[308], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/displayhook.py:268\u001b[0m, in \u001b[0;36mDisplayHook.__call__\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_displayhook()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_output_prompt()\n\u001b[0;32m--> 268\u001b[0m format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_format_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_user_ns(result)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_exec_result(result)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/displayhook.py:157\u001b[0m, in \u001b[0;36mDisplayHook.compute_format_data\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_format_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, result):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute format data of the object to be displayed.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    The format data is a generalization of the :func:`repr` of an object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:182\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    180\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:226\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# lookup registered printer\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m         printer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:397\u001b[0m, in \u001b[0;36mBaseFormatter.lookup\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers[obj_id]\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# then lookup by type\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_by_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:427\u001b[0m, in \u001b[0;36mBaseFormatter.lookup_by_type\u001b[0;34m(self, typ)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pretty\u001b[38;5;241m.\u001b[39m_get_mro(typ):\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_in_deferred_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    428\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers[\u001b[38;5;28mcls\u001b[39m]\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# If we have reached here, the lookup failed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:555\u001b[0m, in \u001b[0;36mBaseFormatter._in_deferred_types\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered value for \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(typ))\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m old\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_in_deferred_types\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    556\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m    Check if the given class is specified in the deferred type registry.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m    Successful matches will be moved to the regular type registry for future use.\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not training_data['positive']['X'].isna().values.any()\n",
    "assert not training_data['negative']['X'].isna().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, trees, hidden_layer_sizes, max_iter_no_change, rf, mlp, max_iter=2000):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf[i]:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp[i]:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes[i], verbose=True, n_iter_no_change=max_iter_no_change[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf, mlp):\n",
    "\n",
    "    #print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in target_columns]\n",
    "        else:\n",
    "            target_name_columns = target_columns\n",
    "            target_name = target\n",
    "\n",
    "        print(f'Maximum of the target: {train_data['positive']['X'][target].max()}')\n",
    "        print(f'Avergae of the target: {train_data['positive']['X'][target].mean()}')\n",
    "        print('')\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target].copy()\n",
    "            #drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "            \n",
    "\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "            \n",
    "\n",
    "            #print(f'Columns used: {df.columns}')\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf[i]:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \n",
    "\n",
    "            if mlp[i]:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags2(df, mask, direction, target, prediction, train_columns):\n",
    "\n",
    "    if 'original' in target:\n",
    "        target_name = target[:-len('_original')]\n",
    "        target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "\n",
    "    else:\n",
    "        target_name = target\n",
    "        target_name_columns = train_columns\n",
    "\n",
    "    data = df.copy()\n",
    "    masked_data = data[mask].copy()\n",
    "\n",
    "    # find the unique days in the data\n",
    "    days = masked_data['datetime'].dt.day.unique()\n",
    "    assert len(days) == 1\n",
    "    day = days[0]\n",
    "\n",
    "    prediction_to_insert = np.array(data[target_name])\n",
    "    prediction_to_insert[mask] = prediction\n",
    "\n",
    "    data.loc[:, target_name] = prediction_to_insert\n",
    "\n",
    "    lags = [1, 2]\n",
    "    sign = '-' if direction == 'negative' else '+'\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        time_delta = pd.Timedelta(days=lag) if direction == 'negative' else pd.Timedelta(days=-lag)\n",
    "        lagged_dates = masked_data['datetime'] + time_delta\n",
    "\n",
    "        column_name = f'{target_name}, {sign}{lag}'\n",
    "\n",
    "        # create a pandas df\n",
    "        lagged_data = pd.DataFrame({\n",
    "            'datetime': lagged_dates,\n",
    "            'new_col': prediction\n",
    "        })\n",
    "\n",
    "        previous_columns = data.columns\n",
    "        print(f'Previous columns: {previous_columns}')\n",
    "\n",
    "        # merge the data with the lagged data using datetime as index, if the datetime is not in the data, do not add it\n",
    "        data = data.merge(lagged_data, on='datetime', how='left')\n",
    "\n",
    "        merge_mask = data[new_col] !=  np.nan & data[column_name] == np.nan\n",
    "\n",
    "        data.loc[merge_mask, column_name] = data[merge_mask, 'new_col'].copy()\n",
    "\n",
    "        data = data.drop(columns = ['new_col'])\n",
    "\n",
    "\n",
    "        new_columns = data.columns\n",
    "        print(f'New columns: {new_columns}')\n",
    "\n",
    "        print(f'Number of rows in lagged data: {len(lagged_data)}')\n",
    "        print(f'Number of rows in masked data: {len(masked_data)}')\n",
    "\n",
    "        assert len(lagged_data) == len(masked_data)\n",
    "\n",
    "        column_to_use = f'{target_name}, {sign}{lag}'\n",
    "\n",
    "        #assert data[mask, column_to_use].isna().sum() == np.sum(mask)\n",
    "\n",
    "        data.loc[mask, column_to_use] = prediction\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction, train_columns):\n",
    "    saved_target = target\n",
    "    #print('target: ', saved_target)\n",
    "    # find the column in which the prediction is stored\n",
    "    for col in train_columns:\n",
    "        if target == col:\n",
    "            if 'original' in col:\n",
    "                target = col[:-len('_original')]\n",
    "                cols_to_insert_prediction = [target]\n",
    "            else:\n",
    "                cols_to_insert_prediction = [target]\n",
    "        elif target in col:\n",
    "            cols_to_insert_prediction = [target]\n",
    "        elif col in target:\n",
    "            target = col\n",
    "            col = saved_target\n",
    "            cols_to_insert_prediction = [col]\n",
    "    \n",
    "    \n",
    "    #print('train columns: ', train_columns)\n",
    "    #print('cols to insert prediction: ', cols_to_insert_prediction)\n",
    "    #print('new target: ', target)\n",
    "\n",
    "    prediction_array = np.array(df[saved_target])\n",
    "    #print('length of prediction array: ', len(prediction_array))\n",
    "    prediction_array[mask] = prediction\n",
    "    #print('Nans in prediction array: ', np.isnan(prediction_array).sum())\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    for col in cols_to_insert_prediction:\n",
    "        df[col] = prediction_array\n",
    "        #print('Nans in column: ', col, df[col].isnull().sum())\n",
    "        \n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            print('Inserting prediction for lag: ', lag, ' in coumns: ', cols_to_insert_prediction)\n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print(f'Predictions inserted from datetime: {lagged_datetime[lagged_mask].iloc[0]} to {lagged_datetime[lagged_mask].iloc[-1]}')\n",
    "            print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "\n",
    "            lagged_col = f'{target}, '+sign+str(abs(lag))\n",
    "\n",
    "            print('lagged col: ', lagged_col)\n",
    "            # print df.loc with datetime column too\n",
    "            print(df.loc[datetime_mask, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "            len_prediction = len(prediction_to_store)\n",
    "            len_space_to_insert = len(df.loc[datetime_mask, lagged_col])\n",
    "\n",
    "            # assert the length of the prediction to store is equal to the length of the space to insert\n",
    "            assert len_prediction == len_space_to_insert\n",
    "            \n",
    "            df.loc[datetime_mask, lagged_col] = prediction_to_store\n",
    "            print(df.loc[datetime_mask, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "\n",
    "            sample_index = 17215\n",
    "            print(df.loc[sample_index, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "\n",
    "            assert df.loc[datetime_mask, lagged_col].isnull().sum() == 0\n",
    "            print('\\n\\n')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for direction in directions:\n",
    "\n",
    "        for i, target in enumerate(test_columns):\n",
    "            if 'original' in target:\n",
    "                target_name = target[:-len('_original')]\n",
    "                target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "            else:\n",
    "                target_name_columns = train_columns\n",
    "                target_name = target\n",
    "\n",
    "        df = training_data[direction]['y'].copy()\n",
    "        #drop_columns =  [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "        drop_columns = columns_not_to_use\n",
    "\n",
    "        # set the target column to NaN\n",
    "        df[target] = np.nan\n",
    "        #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "        if direction == 'negative':\n",
    "            start_day = 20\n",
    "        elif direction == 'positive':\n",
    "            start_day = 31\n",
    "\n",
    "        day = start_day\n",
    "        days_predicted = 0\n",
    "\n",
    "        while days_predicted < test_period:\n",
    "\n",
    "            # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "            mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "            print('Day: ', day)\n",
    "\n",
    "            # get the data\n",
    "            df_days = df[mask].copy()\n",
    "            #print('Number of rows: ', df_days.shape[0])\n",
    "            df_days = df_days.drop(drop_columns, axis=1)\n",
    "            #print('Columns used: ', df_days.columns)\n",
    "\n",
    "            print(f'Missing values: {df_days.isna().sum()}')\n",
    "\n",
    "            try:\n",
    "                assert not df_days.isna().values.any()\n",
    "            except:\n",
    "                # print the rows and columns with NaN values\n",
    "                subset = df_days[df_days.isna().any(axis=1)]\n",
    "                columns_with_nan = subset.columns[subset.isna().any()].tolist()\n",
    "\n",
    "                print(f'subset: {subset[columns_with_nan]}')\n",
    "                raise ValueError('There are NaN values in the data')\n",
    "\n",
    "            for i, target in enumerate(test_columns):\n",
    "\n",
    "                print(f'Predicting for {target}')\n",
    "                \n",
    "                if day == start_day:\n",
    "                    df[target] = np.nan\n",
    "                \n",
    "                if 'original' in target:\n",
    "                    target_name = target[:-len('_original')]\n",
    "                    target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "                else:\n",
    "                    target_name_columns = train_columns\n",
    "                    target_name = target\n",
    "\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "\n",
    "                    # use target statistics to scale the predictions\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                df = store_prediction_with_lags2(df, mask, direction, target, prediction, train_columns)\n",
    "\n",
    "            day += 1\n",
    "            days_predicted += 1\n",
    "\n",
    "        # store the predictions in the dictionary\n",
    "        for target in test_columns:\n",
    "            # assert there are no NaN values in the predictions\n",
    "            assert not df[target].isna().values.any()\n",
    "\n",
    "            predictions[f'{direction}_{target}'] = df[target]\n",
    "\n",
    "        dataframes[direction] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def predict_pipelines(fitted_pipelines, training_data, \\n                        train_columns, test_columns, \\n                        columns_not_to_use, test_period, \\n                        maximum_day, directions, rf=True, mlp=False):\\n    \\n    print('\\nPredicting pipelines...')\\n\\n    #\\xa0create a dictionary to store the predictions\\n    predictions = {}\\n    dataframes = {}\\n\\n    df_positive = training_data['positive']['y'].copy()\\n    df_negative = training_data['negative']['y'].copy()\\n    \\n\\n    for i, target in enumerate(test_columns):\\n        print(f'Predicting pipelines for {target}')\\n\\n        if 'original' in target:\\n            target_name = target[:-len('_original')]\\n            target_name_columns = [col[:-len('_original')] for col in train_columns]\\n        else:\\n            target_name_columns = train_columns\\n            target_name = target\\n        for direction in directions:\\n            #print(f'Direction: {direction}')\\n\\n            # get the y data\\n            if direction == 'positive':\\n                df = df_positive\\n            elif direction == 'negative':\\n                df = df_negative\\n            \\n            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\\n            drop_columns = columns_not_to_use\\n\\n            #\\xa0set the target column to NaN\\n            df[target] = np.nan\\n            #print('Initial nan values: ', df[target].isna().sum())\\n\\n            if direction == 'negative':\\n                start_day = 20\\n            elif direction == 'positive':\\n                start_day = 31\\n\\n            day = start_day\\n            days_predicted = 0\\n\\n            while days_predicted < test_period:\\n                # mask the data to select all rows corresponding to a day of the month equal to start_day\\n                mask = df['datetime'].apply(lambda x: x.day == day)\\n                print('Day: ', day)\\n\\n                # get the data\\n                df_days = df[mask].copy()\\n                #print('Number of rows: ', df_days.shape[0])\\n                df_days = df_days.drop(drop_columns, axis=1)\\n                #print('Columns used: ', df_days.columns)\\n\\n                print(f'Missing values: {df_days.isna().sum()}')\\n                assert not df_days.isna().values.any()\\n                \\n                # get the pipeline\\n                if rf[i]:\\n                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\\n                if mlp[i]:\\n                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\\n\\n                # take the mean of the predictions if both pipelines are used\\n                if rf[i] and mlp[i]:\\n                    prediction1 = pipeline1.predict(df_days)\\n                    prediction2 = pipeline2.predict(df_days)\\n\\n                    #\\xa0use target statistics to scale the predictions\\n                    prediction = (prediction1 + prediction2) / 2\\n                elif rf[i]:\\n                    prediction = pipeline1.predict(df_days)\\n                elif mlp[i]:\\n                    prediction = pipeline2.predict(df_days)\\n\\n                #\\xa0assert the lenght of the prediction is equal to the lenght of the mask\\n                #print('Length of prediction: ', len(prediction))\\n                assert len(prediction) == np.sum(mask)\\n\\n                # store the prediction\\n                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\\n            \\n                if direction == 'negative':\\n                    day += 1\\n\\n                elif direction == 'positive':\\n                    day -= 1\\n\\n                days_predicted += 1\\n                #print('current nan values: ', df[target].isna().sum())\\n                #print('')\\n\\n            prediction = df[target]\\n            \\n            if prediction.isna().sum() > 0:\\n                #\\xa0print the dates with missing values\\n                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\\n\\n            # assert there are no missing values\\n            assert not prediction.isna().values.any()\\n            #print('\\n\\n')\\n            #\\xa0store the predictions\\n            predictions[f'{direction}_{target_name}'] = prediction\\n\\n            # store the dataframe\\n            dataframes[f'{direction}_{target_name}'] = df\\n\\n    return predictions, dataframes\""
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    df_positive = training_data['positive']['y'].copy()\n",
    "    df_negative = training_data['negative']['y'].copy()\n",
    "    \n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "        for direction in directions:\n",
    "            #print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            if direction == 'positive':\n",
    "                df = df_positive\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative\n",
    "            \n",
    "            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "            #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = 20\n",
    "            elif direction == 'positive':\n",
    "                start_day = 31\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "                print('Day: ', day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                #print('Number of rows: ', df_days.shape[0])\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                #print('Columns used: ', df_days.columns)\n",
    "\n",
    "                print(f'Missing values: {df_days.isna().sum()}')\n",
    "                assert not df_days.isna().values.any()\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "\n",
    "                    # use target statistics to scale the predictions\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                #print('Length of prediction: ', len(prediction))\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "                #print('current nan values: ', df[target].isna().sum())\n",
    "                #print('')\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "            #print('\\n\\n')\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_training_columns(fitted_pipelines, training_data, train_columns, \n",
    "                                test_columns, columns_not_to_use, directions, rf, mlp):\n",
    "    print('\\nSubstituting training columns...')\n",
    "\n",
    "    df_positive = training_data['positive']['X'].copy()\n",
    "    df_negative = training_data['negative']['X'].copy()\n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Substituting training columns for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            if direction == 'positive':\n",
    "                df = df_positive.copy()\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative.copy()\n",
    "\n",
    "            drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "\n",
    "            # get the pipeline\n",
    "            if rf[i]:\n",
    "                pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "            if mlp[i]:\n",
    "                pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "            # get the data\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "            if rf[i] and mlp[i]:\n",
    "                prediction1 = pipeline1.predict(df)\n",
    "                prediction2 = pipeline2.predict(df)\n",
    "\n",
    "                prediction = (prediction1 + prediction2) / 2\n",
    "\n",
    "            elif rf[i]:\n",
    "                prediction = pipeline1.predict(df)\n",
    "            elif mlp[i]:\n",
    "                prediction = pipeline2.predict(df)\n",
    "\n",
    "            print(f'Maximum of the prediction: {prediction.max()}')\n",
    "            print(f'Avergae of the prediction: {prediction.mean()}')\n",
    "\n",
    "            # store the prediction in the df_positive or df_negative\n",
    "            if direction == 'positive':\n",
    "                df_positive[target_name] = prediction\n",
    "            elif direction == 'negative':\n",
    "                df_negative[target_name] = prediction\n",
    "    \n",
    "    # put everything back into the training data\n",
    "    training_data['positive']['X'] = df_positive\n",
    "    training_data['negative']['X'] = df_negative\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predictions(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    print('\\nMerging predictions...')\n",
    "\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "    \n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        target_prediction = np.zeros(data['positive']['y'].shape[0])\n",
    "\n",
    "        for i, direction in enumerate(directions):\n",
    "            print(f'Merging predictions for {direction}_{target_name}')\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "            print('maximum value: ', prediction.max())\n",
    "            print('average value: ', prediction.mean())\n",
    "            #print('prediction: ', prediction[:5], prediction[-5:], '\\n')\n",
    "\n",
    "            # plot the prediction\n",
    "            #plt.figure(figsize=(10, 5))\n",
    "            #plt.plot(prediction)\n",
    "            #plt.title(f'{direction}_{target_name}')\n",
    "            #plt.show()\n",
    "            #plt.pause(0.1)\n",
    "\n",
    "            df = data[direction]['y'].copy()\n",
    "\n",
    "            # find all unique months in the 'datetime' column, with same months in different years counting as different months\n",
    "            df['month_year'] = list(zip(df['datetime'].dt.year, df['datetime'].dt.month))\n",
    "\n",
    "            months = df['month_year'].unique()\n",
    "\n",
    "            weights = np.zeros(df.shape[0])\n",
    "            \n",
    "            for month in months:\n",
    "                # mask the data to select all rows corresponding to a month\n",
    "                mask = df['month_year'] == month\n",
    "                if i == 0:\n",
    "                    weights[mask] = np.linspace(1, 0, np.sum(mask))\n",
    "                    \n",
    "                else:\n",
    "                    weights[mask] = np.linspace(0, 1,  np.sum(mask))\n",
    "\n",
    "            #print('weights: ', weights[:5], weights[-5:], '\\n')\n",
    "            #print('shape of prediction: ', prediction.shape)\n",
    "            #print('shape of total prediction: ', target_prediction.shape)\n",
    "            #print('shape of weights: ', weights.shape)\n",
    "            \n",
    "            # apply the weights to the prediction\n",
    "            target_prediction += prediction * weights\n",
    "\n",
    "        # plot the target prediction\n",
    "        #plt.figure(figsize=(10, 5))\n",
    "        #plt.plot(target_prediction)\n",
    "        #plt.title(f'{target_name}')\n",
    "        #plt.show()\n",
    "        #plt.pause(0.1)\n",
    "\n",
    "        print('Maximum of the target prediction: ', target_prediction.max())\n",
    "        print('Average of the target prediction: ', target_prediction.mean())\n",
    "        #print('target prediction: ', target_prediction[:5], target_prediction[-5:], '\\n\\n')\n",
    "        # subsitute the target column with the prediction\n",
    "        for direction in directions:\n",
    "            # insert the prediction into the dataframe\n",
    "            data[direction]['y'].loc[:, target_name] = target_prediction\n",
    "            \n",
    "            \n",
    "            #data[direction]['y'][target] = target_prediction\n",
    "            # assert there are no missing values\n",
    "            assert not data[direction]['y'][target_name].isna().values.any()\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines2(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = data[direction]['y'][target].copy()\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # subsitute the target column with the prediction\n",
    "            data[direction]['y'][target] = prediction\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "Day:  20\n",
      "Missing values: season            0\n",
      "holiday           0\n",
      "workingday        0\n",
      "temp              0\n",
      "atemp             0\n",
      "humidity          0\n",
      "windspeed         0\n",
      "hour              0\n",
      "dayofyear         0\n",
      "weekofyear        0\n",
      "dayofweek         0\n",
      "windspeed, -1     0\n",
      "atemp, -1         0\n",
      "humidity, -1      0\n",
      "holiday, -1       0\n",
      "workingday, -1    0\n",
      "weather, -1       0\n",
      "temp, -1          0\n",
      "casual, -1        0\n",
      "registered, -1    0\n",
      "count, -1         0\n",
      "windspeed, -2     0\n",
      "atemp, -2         0\n",
      "humidity, -2      0\n",
      "holiday, -2       0\n",
      "workingday, -2    0\n",
      "weather, -2       0\n",
      "temp, -2          0\n",
      "casual, -2        0\n",
      "registered, -2    0\n",
      "count, -2         0\n",
      "dtype: int64\n",
      "Predicting for casual_original\n",
      "Previous columns: Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
      "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
      "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
      "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
      "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
      "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
      "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
      "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
      "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[302], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m fitted_pipelines \u001b[38;5;241m=\u001b[39m fit_pipelines(pipelines, training_data, \n\u001b[1;32m     32\u001b[0m                                  train_columns, columns_not_to_use,\n\u001b[1;32m     33\u001b[0m                                  directions, rf\u001b[38;5;241m=\u001b[39mrf, mlp\u001b[38;5;241m=\u001b[39mmlp)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# predict the pipelines\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m predictions, dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pipelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitted_pipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmaximum_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# evaluate the pipelines\u001b[39;00m\n\u001b[1;32m     42\u001b[0m training_data_registered \u001b[38;5;241m=\u001b[39m merge_predictions(predictions, training_data, test_columns, directions)\n",
      "Cell \u001b[0;32mIn[297], line 93\u001b[0m, in \u001b[0;36mpredict_pipelines\u001b[0;34m(fitted_pipelines, training_data, train_columns, test_columns, columns_not_to_use, test_period, maximum_day, directions, rf, mlp)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mlp[i]:\n\u001b[1;32m     91\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m pipeline2\u001b[38;5;241m.\u001b[39mpredict(df_days)\n\u001b[0;32m---> 93\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mstore_prediction_with_lags2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m day \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     96\u001b[0m days_predicted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[295], line 46\u001b[0m, in \u001b[0;36mstore_prediction_with_lags2\u001b[0;34m(df, mask, direction, target, prediction, train_columns)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# merge the data with the lagged data using datetime as index, if the datetime is not in the data, do not add it\u001b[39;00m\n\u001b[1;32m     44\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmerge(lagged_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m merge_mask \u001b[38;5;241m=\u001b[39m data[\u001b[43mnew_col\u001b[49m] \u001b[38;5;241m!=\u001b[39m  np\u001b[38;5;241m.\u001b[39mnan \u001b[38;5;241m&\u001b[39m data[column_name] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     48\u001b[0m data\u001b[38;5;241m.\u001b[39mloc[column_name]\n\u001b[1;32m     52\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_col' is not defined"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'weather', 'year']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "n = 20\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(150, 150, 150), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 12\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "\n",
    "train_columns = ['casual_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "training_data_registered = merge_predictions(predictions, training_data, test_columns, directions)\n",
    "\n",
    "# TODO: solve the prediction order doing for the first day all features, for the second day all features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data_registered, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data_registered, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count = merge_predictions(predictions, training_data_registered, test_columns, directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count2 = merge_predictions(predictions, triaining_data_count, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count2['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count2['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count2, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count2, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count3 = merge_predictions(predictions, triaining_data_count2, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count3['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count3['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count3, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count3, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count4 = merge_predictions(predictions, triaining_data_count3, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count4['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count4['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
