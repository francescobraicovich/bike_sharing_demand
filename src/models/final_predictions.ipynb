{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_2732/1086435320.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n",
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_2732/1086435320.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_testing_data['datetime'] = pd.to_datetime(model_testing_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "model_testing_data = data[data['dataset'] == 'test']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n",
    "model_testing_data['datetime'] = pd.to_datetime(model_testing_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime            0\n",
      "season              0\n",
      "holiday             0\n",
      "workingday          0\n",
      "weather             0\n",
      "                 ... \n",
      "casual, -2        238\n",
      "registered, +2    286\n",
      "registered, -2    238\n",
      "count, +2         286\n",
      "count, -2         238\n",
      "Length: 63, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(model_testing_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 0\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': model_testing_data[original_columns + positive_columns]\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': model_testing_data[original_columns + negative_columns]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not training_data['positive']['X'].isna().values.any()\n",
    "assert not training_data['negative']['X'].isna().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, trees, hidden_layer_sizes, max_iter_no_change, rf, mlp, max_iter=2000):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf[i]:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp[i]:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes[i], verbose=True, n_iter_no_change=max_iter_no_change[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf, mlp):\n",
    "\n",
    "    #print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in target_columns]\n",
    "        else:\n",
    "            target_name_columns = target_columns\n",
    "            target_name = target\n",
    "\n",
    "        print(f'Maximum of the target: {train_data['positive']['X'][target].max()}')\n",
    "        print(f'Avergae of the target: {train_data['positive']['X'][target].mean()}')\n",
    "        print('')\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target].copy()\n",
    "            #drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "            \n",
    "\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "            \n",
    "            #print(f'Columns used: {df.columns}')\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf[i]:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \n",
    "\n",
    "            if mlp[i]:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags2(df, mask, direction, target, prediction, train_columns):\n",
    "\n",
    "    initial_length = len(df)\n",
    "\n",
    "    if 'original' in target:\n",
    "        target_name = target[:-len('_original')]\n",
    "        target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "\n",
    "    else:\n",
    "        target_name = target\n",
    "        target_name_columns = train_columns\n",
    "\n",
    "    data_to_store = df.copy()\n",
    "    # find the unique days in the data\n",
    "    days = data_to_store['datetime'][mask].dt.day.unique()\n",
    "    assert len(days) == 1\n",
    "    day = days[0]\n",
    "\n",
    "    prediction_to_insert = np.array(data_to_store[target_name])\n",
    "    prediction_to_insert[mask] = prediction\n",
    "\n",
    "    data_to_store.loc[:, target_name] = prediction_to_insert\n",
    "\n",
    "    assert not data_to_store[mask][target_name].isna().any()\n",
    "\n",
    "    lags = [1, 2]\n",
    "    sign = '-' if direction == 'negative' else '+'\n",
    "\n",
    "\n",
    "    for lag in lags:\n",
    "\n",
    "        time_delta = pd.Timedelta(days=lag) if direction == 'negative' else pd.Timedelta(days=-lag)\n",
    "        lagged_dates = data_to_store['datetime'][mask] + time_delta\n",
    "        column_name = f'{target_name}, {sign}{lag}'\n",
    "\n",
    "\n",
    "        # create a pandas df\n",
    "        lagged_data = pd.DataFrame({\n",
    "            'datetime': lagged_dates,\n",
    "            'new_col': prediction\n",
    "        })\n",
    "\n",
    "        # merge the data with the lagged data using datetime as index, if the datetime is not in the data, do not add it\n",
    "        data_to_store = data_to_store.merge(lagged_data, on='datetime', how='left')\n",
    "\n",
    "        missing_mask = data_to_store[column_name].isna()\n",
    "        new_values = ~ data_to_store['new_col'].isna()\n",
    "\n",
    "        merge_mask = missing_mask & new_values\n",
    "\n",
    "        #print('Dates to merge:', data_to_store['datetime'][merge_mask])\n",
    "\n",
    "        data_to_store.loc[merge_mask, column_name] = data_to_store['new_col'][merge_mask].copy()\n",
    "        data_to_store = data_to_store.drop(columns = ['new_col'])\n",
    "        \n",
    "        new_day = day + lag if direction == 'negative' else day - lag\n",
    "        new_day_mask = data_to_store['datetime'].dt.day == new_day\n",
    "\n",
    "        if data_to_store[column_name][new_day_mask].isna().sum() > 0:\n",
    "            #print(f'Missing values: ', data_to_store[column_name][new_day_mask].isna().sum())\n",
    "            interpolated_values = data_to_store[column_name][new_day_mask].interpolate(method='linear')\n",
    "            data_to_store.loc[new_day_mask, column_name] = interpolated_values\n",
    "\n",
    "        assert data_to_store[column_name][new_day_mask].isna().sum() == 0\n",
    "        assert len(data_to_store) == initial_length\n",
    "\n",
    "        columns_to_print = [col for col in data_to_store.columns if 'casual' in col]\n",
    "        columns_to_print.append('datetime')\n",
    "        \n",
    "        indices_top_print = np.arange(6327, 6331)\n",
    "\n",
    "        data_to_store = data_to_store.reset_index(drop = True)\n",
    "\n",
    "    # reset the index\n",
    "    data_to_store = data_to_store.reset_index(drop = True)\n",
    "\n",
    "    assert len(data_to_store) == initial_length\n",
    "\n",
    "    return data_to_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction, train_columns):\n",
    "    saved_target = target\n",
    "    #print('target: ', saved_target)\n",
    "    # find the column in which the prediction is stored\n",
    "    for col in train_columns:\n",
    "        if target == col:\n",
    "            if 'original' in col:\n",
    "                target = col[:-len('_original')]\n",
    "                cols_to_insert_prediction = [target]\n",
    "            else:\n",
    "                cols_to_insert_prediction = [target]\n",
    "        elif target in col:\n",
    "            cols_to_insert_prediction = [target]\n",
    "        elif col in target:\n",
    "            target = col\n",
    "            col = saved_target\n",
    "            cols_to_insert_prediction = [col]\n",
    "    \n",
    "    \n",
    "    #print('train columns: ', train_columns)\n",
    "    #print('cols to insert prediction: ', cols_to_insert_prediction)\n",
    "    #print('new target: ', target)\n",
    "\n",
    "    prediction_array = np.array(df[saved_target])\n",
    "    #print('length of prediction array: ', len(prediction_array))\n",
    "    prediction_array[mask] = prediction\n",
    "    #print('Nans in prediction array: ', np.isnan(prediction_array).sum())\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    for col in cols_to_insert_prediction:\n",
    "        df[col] = prediction_array\n",
    "        #print('Nans in column: ', col, df[col].isnull().sum())\n",
    "        \n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            print('Inserting prediction for lag: ', lag, ' in coumns: ', cols_to_insert_prediction)\n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print(f'Predictions inserted from datetime: {lagged_datetime[lagged_mask].iloc[0]} to {lagged_datetime[lagged_mask].iloc[-1]}')\n",
    "            print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "\n",
    "            lagged_col = f'{target}, '+sign+str(abs(lag))\n",
    "\n",
    "            #print('lagged col: ', lagged_col)\n",
    "            # print df.loc with datetime column too\n",
    "            #print(df.loc[datetime_mask, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "            len_prediction = len(prediction_to_store)\n",
    "            len_space_to_insert = len(df.loc[datetime_mask, lagged_col])\n",
    "\n",
    "            # assert the length of the prediction to store is equal to the length of the space to insert\n",
    "            assert len_prediction == len_space_to_insert\n",
    "            \n",
    "            df.loc[datetime_mask, lagged_col] = prediction_to_store\n",
    "            #print(df.loc[datetime_mask, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "\n",
    "            sample_index = 17215\n",
    "            #print(df.loc[sample_index, [col for col in df.columns if 'datetime' in col or target in col]])\n",
    "\n",
    "            assert df.loc[datetime_mask, lagged_col].isnull().sum() == 0\n",
    "            print('\\n\\n')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for direction in directions:\n",
    "\n",
    "        for i, target in enumerate(test_columns):\n",
    "            if 'original' in target:\n",
    "                target_name = target[:-len('_original')]\n",
    "                target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "            else:\n",
    "                target_name_columns = train_columns\n",
    "                target_name = target\n",
    "\n",
    "        df = training_data[direction]['y'].copy()\n",
    "        #drop_columns =  [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "        drop_columns = [col for col in columns_not_to_use if col != 'datetime']\n",
    "\n",
    "        # set the target column to NaN\n",
    "        df[target] = np.nan\n",
    "        #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "        if direction == 'negative':\n",
    "            start_day = 20\n",
    "        elif direction == 'positive':\n",
    "            start_day = 31\n",
    "\n",
    "        day = start_day\n",
    "        days_predicted = 0\n",
    "\n",
    "        while days_predicted < test_period:\n",
    "\n",
    "            # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "            mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "\n",
    "            # convert the mask to a numpy array\n",
    "            mask = mask.values\n",
    "\n",
    "            # get the data\n",
    "            df_days = df[mask]\n",
    "\n",
    "            #print('Number of rows: ', df_days.shape[0])\n",
    "            df_days = df_days.drop(drop_columns, axis=1)\n",
    "\n",
    "            #assert 'datetime' not in drop_columns\n",
    "            \n",
    "            #df_days.interpolate(method='linear', axis=0, inplace=True)\n",
    "            #print('Day: ', day)\n",
    "            \n",
    "            \"\"\"try:\n",
    "                assert not df_days.isna().values.any()\n",
    "            except:\n",
    "                # print the rows and columns with NaN values\n",
    "                subset = df_days[df_days.isna().any(axis=1)]\n",
    "                columns_with_nan = subset.columns[subset.isna().any()].tolist()\n",
    "\n",
    "                print(f'subset: {subset[columns_with_nan + ['datetime']]}')\n",
    "                raise ValueError('There are NaN values in the data')\"\"\"\n",
    "\n",
    "            \n",
    "            df_days = df_days.drop(columns = ['datetime'])\n",
    "    \n",
    "            #print('Columns used: ', df_days.columns)\n",
    "            #print(f'Missing values: {df_days.isna().sum()}')\n",
    "\n",
    "            for i, target in enumerate(test_columns):\n",
    "                #print(target)\n",
    "                \n",
    "                if day == start_day:\n",
    "                    df[target] = np.nan\n",
    "                \n",
    "                if 'original' in target:\n",
    "                    target_name = target[:-len('_original')]\n",
    "                    target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "                else:\n",
    "                    target_name_columns = train_columns\n",
    "                    target_name = target\n",
    "\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "\n",
    "                    # use target statistics to scale the predictions\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                df = store_prediction_with_lags2(df, mask, direction, target, prediction, train_columns)\n",
    "\n",
    "            #print('')\n",
    "\n",
    "            if direction == 'negative':\n",
    "                day += 1\n",
    "            elif direction == 'positive':\n",
    "                day -= 1\n",
    "            days_predicted += 1\n",
    "\n",
    "        for col in target_name_columns:\n",
    "            prediction = df[col].copy()\n",
    "\n",
    "            original_col = col + '_original'\n",
    "            original_data = training_data[direction]['X'][original_col].copy()\n",
    "\n",
    "            original_mean = original_data.mean()\n",
    "            original_std = original_data.std()\n",
    "\n",
    "            standardised_prediction = (prediction - prediction.mean()) / prediction.std()\n",
    "            prediction = standardised_prediction * original_std + original_mean\n",
    "\n",
    "            df.loc[:, col] = prediction\n",
    "\n",
    "            assert not prediction.isna().any()\n",
    "            predictions[f'{direction}_{col}'] = prediction\n",
    "\n",
    "        dataframes[direction] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def predict_pipelines(fitted_pipelines, training_data, \\n                        train_columns, test_columns, \\n                        columns_not_to_use, test_period, \\n                        maximum_day, directions, rf=True, mlp=False):\\n    \\n    print('\\nPredicting pipelines...')\\n\\n    #\\xa0create a dictionary to store the predictions\\n    predictions = {}\\n    dataframes = {}\\n\\n    df_positive = training_data['positive']['y'].copy()\\n    df_negative = training_data['negative']['y'].copy()\\n    \\n\\n    for i, target in enumerate(test_columns):\\n        print(f'Predicting pipelines for {target}')\\n\\n        if 'original' in target:\\n            target_name = target[:-len('_original')]\\n            target_name_columns = [col[:-len('_original')] for col in train_columns]\\n        else:\\n            target_name_columns = train_columns\\n            target_name = target\\n        for direction in directions:\\n            #print(f'Direction: {direction}')\\n\\n            # get the y data\\n            if direction == 'positive':\\n                df = df_positive\\n            elif direction == 'negative':\\n                df = df_negative\\n            \\n            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\\n            drop_columns = columns_not_to_use\\n\\n            #\\xa0set the target column to NaN\\n            df[target] = np.nan\\n            #print('Initial nan values: ', df[target].isna().sum())\\n\\n            if direction == 'negative':\\n                start_day = 20\\n            elif direction == 'positive':\\n                start_day = 31\\n\\n            day = start_day\\n            days_predicted = 0\\n\\n            while days_predicted < test_period:\\n                # mask the data to select all rows corresponding to a day of the month equal to start_day\\n                mask = df['datetime'].apply(lambda x: x.day == day)\\n                print('Day: ', day)\\n\\n                # get the data\\n                df_days = df[mask].copy()\\n                #print('Number of rows: ', df_days.shape[0])\\n                df_days = df_days.drop(drop_columns, axis=1)\\n                #print('Columns used: ', df_days.columns)\\n\\n                print(f'Missing values: {df_days.isna().sum()}')\\n                assert not df_days.isna().values.any()\\n                \\n                # get the pipeline\\n                if rf[i]:\\n                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\\n                if mlp[i]:\\n                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\\n\\n                # take the mean of the predictions if both pipelines are used\\n                if rf[i] and mlp[i]:\\n                    prediction1 = pipeline1.predict(df_days)\\n                    prediction2 = pipeline2.predict(df_days)\\n\\n                    #\\xa0use target statistics to scale the predictions\\n                    prediction = (prediction1 + prediction2) / 2\\n                elif rf[i]:\\n                    prediction = pipeline1.predict(df_days)\\n                elif mlp[i]:\\n                    prediction = pipeline2.predict(df_days)\\n\\n                #\\xa0assert the lenght of the prediction is equal to the lenght of the mask\\n                #print('Length of prediction: ', len(prediction))\\n                assert len(prediction) == np.sum(mask)\\n\\n                # store the prediction\\n                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\\n            \\n                if direction == 'negative':\\n                    day += 1\\n\\n                elif direction == 'positive':\\n                    day -= 1\\n\\n                days_predicted += 1\\n                #print('current nan values: ', df[target].isna().sum())\\n                #print('')\\n\\n            prediction = df[target]\\n            \\n            if prediction.isna().sum() > 0:\\n                #\\xa0print the dates with missing values\\n                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\\n\\n            # assert there are no missing values\\n            assert not prediction.isna().values.any()\\n            #print('\\n\\n')\\n            #\\xa0store the predictions\\n            predictions[f'{direction}_{target_name}'] = prediction\\n\\n            # store the dataframe\\n            dataframes[f'{direction}_{target_name}'] = df\\n\\n    return predictions, dataframes\""
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    df_positive = training_data['positive']['y'].copy()\n",
    "    df_negative = training_data['negative']['y'].copy()\n",
    "    \n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "        for direction in directions:\n",
    "            #print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            if direction == 'positive':\n",
    "                df = df_positive\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative\n",
    "            \n",
    "            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "            #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = 20\n",
    "            elif direction == 'positive':\n",
    "                start_day = 31\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "                print('Day: ', day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                #print('Number of rows: ', df_days.shape[0])\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                #print('Columns used: ', df_days.columns)\n",
    "\n",
    "                print(f'Missing values: {df_days.isna().sum()}')\n",
    "                assert not df_days.isna().values.any()\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "\n",
    "                    # use target statistics to scale the predictions\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                #print('Length of prediction: ', len(prediction))\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "                #print('current nan values: ', df[target].isna().sum())\n",
    "                #print('')\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "            #print('\\n\\n')\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_training_columns(fitted_pipelines, training_data, train_columns, \n",
    "                                test_columns, columns_not_to_use, directions, rf, mlp):\n",
    "    print('\\nSubstituting training columns...')\n",
    "\n",
    "    df_positive = training_data['positive']['X'].copy()\n",
    "    df_negative = training_data['negative']['X'].copy()\n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Substituting training columns for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            if direction == 'positive':\n",
    "                df = df_positive.copy()\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative.copy()\n",
    "\n",
    "            drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            drop_columns = columns_not_to_use\n",
    "\n",
    "            # get the pipeline\n",
    "            if rf[i]:\n",
    "                pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "            if mlp[i]:\n",
    "                pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "            # get the data\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "            if rf[i] and mlp[i]:\n",
    "                prediction1 = pipeline1.predict(df)\n",
    "                prediction2 = pipeline2.predict(df)\n",
    "\n",
    "                prediction = (prediction1 + prediction2) / 2\n",
    "\n",
    "            elif rf[i]:\n",
    "                prediction = pipeline1.predict(df)\n",
    "            elif mlp[i]:\n",
    "                prediction = pipeline2.predict(df)\n",
    "\n",
    "            print(f'Maximum of the prediction: {prediction.max()}')\n",
    "            print(f'Avergae of the prediction: {prediction.mean()}')\n",
    "\n",
    "            # store the prediction in the df_positive or df_negative\n",
    "            if direction == 'positive':\n",
    "                df_positive[target_name] = prediction\n",
    "            elif direction == 'negative':\n",
    "                df_negative[target_name] = prediction\n",
    "    \n",
    "    # put everything back into the training data\n",
    "    training_data['positive']['X'] = df_positive\n",
    "    training_data['negative']['X'] = df_negative\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_predictions(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    print('\\nMerging predictions...')\n",
    "    \n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        target_prediction = np.zeros(data['positive']['y'].shape[0])\n",
    "\n",
    "        for i, direction in enumerate(directions):\n",
    "            print(f'Merging predictions for {direction}_{target_name}')\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "            print('maximum value: ', prediction.max())\n",
    "            print('average value: ', prediction.mean())\n",
    "            #print('prediction: ', prediction[:5], prediction[-5:], '\\n')\n",
    "\n",
    "            # plot the prediction\n",
    "            #plt.figure(figsize=(10, 5))\n",
    "            #plt.plot(prediction)\n",
    "            #plt.title(f'{direction}_{target_name}')\n",
    "            #plt.show()\n",
    "            #plt.pause(0.1)\n",
    "\n",
    "            df = data[direction]['y'].copy()\n",
    "            print('shape of df: ', df.shape)\n",
    "\n",
    "            # find all unique months in the 'datetime' column, with same months in different years counting as different months\n",
    "            df['month_year'] = list(zip(df['datetime'].dt.year, df['datetime'].dt.month))\n",
    "\n",
    "            months = df['month_year'].unique()\n",
    "\n",
    "            weights = np.zeros(df.shape[0])\n",
    "            \n",
    "            for month in months:\n",
    "                # mask the data to select all rows corresponding to a month\n",
    "                mask = df['month_year'] == month\n",
    "                if i == 0:\n",
    "                    weights[mask] = np.linspace(1, 0, np.sum(mask))\n",
    "                else:\n",
    "                    weights[mask] = np.linspace(0, 1,  np.sum(mask))\n",
    "\n",
    "            #print('weights: ', weights[:5], weights[-5:], '\\n')\n",
    "            #print('shape of prediction: ', prediction.shape)\n",
    "            print('shape of total prediction: ', target_prediction.shape)\n",
    "            print('shape of weights: ', weights.shape)\n",
    "            \n",
    "            # apply the weights to the prediction\n",
    "            target_prediction += prediction * weights\n",
    "\n",
    "        # plot the target prediction\n",
    "        #plt.figure(figsize=(10, 5))\n",
    "        #plt.plot(target_prediction)\n",
    "        #plt.title(f'{target_name}')\n",
    "        #plt.show()\n",
    "        #plt.pause(0.1)\n",
    "\n",
    "        print('Maximum of the target prediction: ', target_prediction.max())\n",
    "        print('Average of the target prediction: ', target_prediction.mean())\n",
    "        print('')\n",
    "        #print('target prediction: ', target_prediction[:5], target_prediction[-5:], '\\n\\n')\n",
    "        # subsitute the target column with the prediction\n",
    "        for direction in directions:\n",
    "            # insert the prediction into the dataframe\n",
    "            #print('target prediction type: ', type(target_prediction))\n",
    "            data[direction]['y'].loc[:, target_name] = np.copy(target_prediction)\n",
    "            #print('Nans in the target prediction: ', np.sum(target_prediction == pd.NA))\n",
    "            \n",
    "            #data[direction]['y'][target] = target_prediction\n",
    "            # assert there are no missing values\n",
    "            #print('Nans in the target prediction: ', data[direction]['y'][target_name].isna().sum())\n",
    "            # print the dates with missing values\n",
    "            #print('Dates with missing values: ', data[direction]['y'][target_name][data[direction]['y'][target_name].isna()])\n",
    "            assert not data[direction]['y'][target_name].isna().values.any()\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines2(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = data[direction]['y'][target].copy()\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # subsitute the target column with the prediction\n",
    "            data[direction]['y'][target] = prediction\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'weather', 'year']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "n = 20\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(150, 150, 150), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 12\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "training_data_registered = merge_predictions(predictions, training_data, test_columns, directions)\n",
    "\n",
    "# TODO: solve the prediction order doing for the first day all features, for the second day all features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n",
      "Fitting pipelines for registered_original\n",
      "Maximum of the target: 886.0\n",
      "Avergae of the target: 155.4472253564468\n",
      "\n",
      "Fitting pipelines for count_original\n",
      "Maximum of the target: 977.0\n",
      "Avergae of the target: 191.47112524361472\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "\n",
      "Merging predictions...\n",
      "Merging predictions for negative_casual\n",
      "maximum value:  600.1718074440588\n",
      "average value:  36.3763606490039\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_casual\n",
      "maximum value:  590.8011794270951\n",
      "average value:  36.023899887167914\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  591.062381601784\n",
      "Average of the target prediction:  36.164151759792425\n",
      "\n",
      "Merging predictions for negative_registered\n",
      "maximum value:  1566.677395816826\n",
      "average value:  156.31238447319782\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_registered\n",
      "maximum value:  1653.3600307305048\n",
      "average value:  155.4472253564468\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1630.7251477599834\n",
      "Average of the target prediction:  168.0943895746347\n",
      "\n",
      "Merging predictions for negative_count\n",
      "maximum value:  1673.5812092507852\n",
      "average value:  192.68874512220168\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_count\n",
      "maximum value:  1846.4480795418274\n",
      "average value:  191.4711252436147\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1824.3868192663203\n",
      "Average of the target prediction:  203.9648587565524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data_registered, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data_registered, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count = merge_predictions(predictions, training_data_registered, test_columns, directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n",
      "Fitting pipelines for registered_original\n",
      "Maximum of the target: 886.0\n",
      "Avergae of the target: 155.4472253564468\n",
      "\n",
      "Fitting pipelines for count_original\n",
      "Maximum of the target: 977.0\n",
      "Avergae of the target: 191.47112524361472\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "\n",
      "Merging predictions...\n",
      "Merging predictions for negative_casual\n",
      "maximum value:  595.1910871489043\n",
      "average value:  36.3763606490039\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_casual\n",
      "maximum value:  597.9489203906179\n",
      "average value:  36.023899887167914\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  597.6758036063699\n",
      "Average of the target prediction:  36.20944562978634\n",
      "\n",
      "Merging predictions for negative_registered\n",
      "maximum value:  1628.4128975726244\n",
      "average value:  156.31238447319782\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_registered\n",
      "maximum value:  1620.2314085916926\n",
      "average value:  155.4472253564468\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1620.4024501731753\n",
      "Average of the target prediction:  155.73253691668367\n",
      "\n",
      "Merging predictions for negative_count\n",
      "maximum value:  1874.7106858990514\n",
      "average value:  192.68874512220162\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_count\n",
      "maximum value:  1900.8788290231969\n",
      "average value:  191.47112524361478\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1900.3317598289639\n",
      "Average of the target prediction:  192.01718724708257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count2 = merge_predictions(predictions, triaining_data_count, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count2['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count2['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n",
      "Fitting pipelines for registered_original\n",
      "Maximum of the target: 886.0\n",
      "Avergae of the target: 155.4472253564468\n",
      "\n",
      "Fitting pipelines for count_original\n",
      "Maximum of the target: 977.0\n",
      "Avergae of the target: 191.47112524361472\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "\n",
      "Merging predictions...\n",
      "Merging predictions for negative_casual\n",
      "maximum value:  616.8108209010074\n",
      "average value:  36.37636064900389\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_casual\n",
      "maximum value:  605.615059169547\n",
      "average value:  36.02389988716791\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  604.3006003738587\n",
      "Average of the target prediction:  36.097107704135894\n",
      "\n",
      "Merging predictions for negative_registered\n",
      "maximum value:  1963.1388748866925\n",
      "average value:  156.31238447319777\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_registered\n",
      "maximum value:  1983.3835648728025\n",
      "average value:  155.44722535644684\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1982.17372284176\n",
      "Average of the target prediction:  155.54436873355445\n",
      "\n",
      "Merging predictions for negative_count\n",
      "maximum value:  2066.313245763187\n",
      "average value:  192.68874512220168\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_count\n",
      "maximum value:  2104.6152758779467\n",
      "average value:  191.4711252436147\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  2063.919834770448\n",
      "Average of the target prediction:  191.5785453243162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count2, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count2, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count3 = merge_predictions(predictions, triaining_data_count2, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count3['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count3['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Maximum of the target: 367.0\n",
      "Avergae of the target: 36.023899887167914\n",
      "\n",
      "Fitting pipelines for registered_original\n",
      "Maximum of the target: 886.0\n",
      "Avergae of the target: 155.4472253564468\n",
      "\n",
      "Fitting pipelines for count_original\n",
      "Maximum of the target: 977.0\n",
      "Avergae of the target: 191.47112524361472\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "\n",
      "Merging predictions...\n",
      "Merging predictions for negative_casual\n",
      "maximum value:  600.9441522906674\n",
      "average value:  36.3763606490039\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_casual\n",
      "maximum value:  600.6332816290715\n",
      "average value:  36.023899887167914\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  599.7796933576753\n",
      "Average of the target prediction:  36.09767144803554\n",
      "\n",
      "Merging predictions for negative_registered\n",
      "maximum value:  1914.9551595089033\n",
      "average value:  156.31238447319777\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_registered\n",
      "maximum value:  1942.071607060189\n",
      "average value:  155.44722535644684\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  1939.5294620527138\n",
      "Average of the target prediction:  155.68466265255645\n",
      "\n",
      "Merging predictions for negative_count\n",
      "maximum value:  2104.4096251577785\n",
      "average value:  192.68874512220168\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Merging predictions for positive_count\n",
      "maximum value:  2120.036855455169\n",
      "average value:  191.47112524361472\n",
      "shape of df:  (6493, 43)\n",
      "shape of total prediction:  (6493,)\n",
      "shape of weights:  (6493,)\n",
      "Maximum of the target prediction:  2119.2201012584414\n",
      "Average of the target prediction:  191.7520574020666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset']\n",
    "smoothed_columns = []\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [n, n, n]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count3, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count3, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "triaining_data_count4 = merge_predictions(predictions, triaining_data_count3, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the predictions for count from the training data\n",
    "count_predictions = triaining_data_count4['positive']['y']['count']\n",
    "\n",
    "# save the predictions into a csv file with the datetime column\n",
    "count_predictions = pd.concat([triaining_data_count4['positive']['y']['datetime'], count_predictions], axis=1)\n",
    "\n",
    "count_predictions.to_csv('../../data/processed/count_predictions3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>78.547512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>84.036977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>85.203285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>81.654897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>60.270158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>2011-01-20 05:00:00</td>\n",
       "      <td>52.389292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2011-01-20 06:00:00</td>\n",
       "      <td>79.683771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2011-01-20 07:00:00</td>\n",
       "      <td>112.608661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2011-01-20 08:00:00</td>\n",
       "      <td>163.883517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2011-01-20 09:00:00</td>\n",
       "      <td>170.969929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2011-01-20 10:00:00</td>\n",
       "      <td>114.771241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2011-01-20 11:00:00</td>\n",
       "      <td>133.127496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2011-01-20 12:00:00</td>\n",
       "      <td>131.300359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2011-01-20 13:00:00</td>\n",
       "      <td>139.335553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2011-01-20 14:00:00</td>\n",
       "      <td>139.289268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2011-01-20 15:00:00</td>\n",
       "      <td>141.441862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2011-01-20 16:00:00</td>\n",
       "      <td>181.377906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2011-01-20 17:00:00</td>\n",
       "      <td>209.215186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2011-01-20 18:00:00</td>\n",
       "      <td>252.574339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2011-01-20 19:00:00</td>\n",
       "      <td>181.180108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>2011-01-20 20:00:00</td>\n",
       "      <td>152.250490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2011-01-20 21:00:00</td>\n",
       "      <td>148.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2011-01-20 22:00:00</td>\n",
       "      <td>133.871470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2011-01-20 23:00:00</td>\n",
       "      <td>85.183132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2011-01-21 00:00:00</td>\n",
       "      <td>85.487743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2011-01-21 01:00:00</td>\n",
       "      <td>84.299417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2011-01-21 02:00:00</td>\n",
       "      <td>86.396561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2011-01-21 03:00:00</td>\n",
       "      <td>66.384730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2011-01-21 04:00:00</td>\n",
       "      <td>54.058119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2011-01-21 05:00:00</td>\n",
       "      <td>58.541466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2011-01-21 06:00:00</td>\n",
       "      <td>77.722887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2011-01-21 07:00:00</td>\n",
       "      <td>137.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2011-01-21 08:00:00</td>\n",
       "      <td>171.411303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>2011-01-21 09:00:00</td>\n",
       "      <td>168.515512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2011-01-21 10:00:00</td>\n",
       "      <td>124.892048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2011-01-21 11:00:00</td>\n",
       "      <td>124.301293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>2011-01-21 12:00:00</td>\n",
       "      <td>111.360498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2011-01-21 13:00:00</td>\n",
       "      <td>131.409189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2011-01-21 14:00:00</td>\n",
       "      <td>130.042322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2011-01-21 15:00:00</td>\n",
       "      <td>129.316358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2011-01-21 16:00:00</td>\n",
       "      <td>134.419544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2011-01-21 17:00:00</td>\n",
       "      <td>134.070969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2011-01-21 18:00:00</td>\n",
       "      <td>129.403637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2011-01-21 19:00:00</td>\n",
       "      <td>136.793346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2011-01-21 20:00:00</td>\n",
       "      <td>128.884376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2011-01-21 21:00:00</td>\n",
       "      <td>144.760723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2011-01-21 22:00:00</td>\n",
       "      <td>133.849087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2011-01-21 23:00:00</td>\n",
       "      <td>134.694045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2011-01-22 00:00:00</td>\n",
       "      <td>105.927253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2011-01-22 01:00:00</td>\n",
       "      <td>93.874599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime       count\n",
       "431 2011-01-20 00:00:00   78.547512\n",
       "432 2011-01-20 01:00:00   84.036977\n",
       "433 2011-01-20 02:00:00   85.203285\n",
       "434 2011-01-20 03:00:00   81.654897\n",
       "435 2011-01-20 04:00:00   60.270158\n",
       "436 2011-01-20 05:00:00   52.389292\n",
       "437 2011-01-20 06:00:00   79.683771\n",
       "438 2011-01-20 07:00:00  112.608661\n",
       "439 2011-01-20 08:00:00  163.883517\n",
       "440 2011-01-20 09:00:00  170.969929\n",
       "441 2011-01-20 10:00:00  114.771241\n",
       "442 2011-01-20 11:00:00  133.127496\n",
       "443 2011-01-20 12:00:00  131.300359\n",
       "444 2011-01-20 13:00:00  139.335553\n",
       "445 2011-01-20 14:00:00  139.289268\n",
       "446 2011-01-20 15:00:00  141.441862\n",
       "447 2011-01-20 16:00:00  181.377906\n",
       "448 2011-01-20 17:00:00  209.215186\n",
       "449 2011-01-20 18:00:00  252.574339\n",
       "450 2011-01-20 19:00:00  181.180108\n",
       "451 2011-01-20 20:00:00  152.250490\n",
       "452 2011-01-20 21:00:00  148.044872\n",
       "453 2011-01-20 22:00:00  133.871470\n",
       "454 2011-01-20 23:00:00   85.183132\n",
       "455 2011-01-21 00:00:00   85.487743\n",
       "456 2011-01-21 01:00:00   84.299417\n",
       "457 2011-01-21 02:00:00   86.396561\n",
       "458 2011-01-21 03:00:00   66.384730\n",
       "459 2011-01-21 04:00:00   54.058119\n",
       "460 2011-01-21 05:00:00   58.541466\n",
       "461 2011-01-21 06:00:00   77.722887\n",
       "462 2011-01-21 07:00:00  137.000589\n",
       "463 2011-01-21 08:00:00  171.411303\n",
       "464 2011-01-21 09:00:00  168.515512\n",
       "465 2011-01-21 10:00:00  124.892048\n",
       "466 2011-01-21 11:00:00  124.301293\n",
       "467 2011-01-21 12:00:00  111.360498\n",
       "468 2011-01-21 13:00:00  131.409189\n",
       "469 2011-01-21 14:00:00  130.042322\n",
       "470 2011-01-21 15:00:00  129.316358\n",
       "471 2011-01-21 16:00:00  134.419544\n",
       "472 2011-01-21 17:00:00  134.070969\n",
       "473 2011-01-21 18:00:00  129.403637\n",
       "474 2011-01-21 19:00:00  136.793346\n",
       "475 2011-01-21 20:00:00  128.884376\n",
       "476 2011-01-21 21:00:00  144.760723\n",
       "477 2011-01-21 22:00:00  133.849087\n",
       "478 2011-01-21 23:00:00  134.694045\n",
       "479 2011-01-22 00:00:00  105.927253\n",
       "480 2011-01-22 01:00:00   93.874599"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>78.547512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>84.036977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>85.203285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>81.654897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>60.270158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>2011-01-20 05:00:00</td>\n",
       "      <td>52.389292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2011-01-20 06:00:00</td>\n",
       "      <td>79.683771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2011-01-20 07:00:00</td>\n",
       "      <td>112.608661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2011-01-20 08:00:00</td>\n",
       "      <td>163.883517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2011-01-20 09:00:00</td>\n",
       "      <td>170.969929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2011-01-20 10:00:00</td>\n",
       "      <td>114.771241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2011-01-20 11:00:00</td>\n",
       "      <td>133.127496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2011-01-20 12:00:00</td>\n",
       "      <td>131.300359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2011-01-20 13:00:00</td>\n",
       "      <td>139.335553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2011-01-20 14:00:00</td>\n",
       "      <td>139.289268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2011-01-20 15:00:00</td>\n",
       "      <td>141.441862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2011-01-20 16:00:00</td>\n",
       "      <td>181.377906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2011-01-20 17:00:00</td>\n",
       "      <td>209.215186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2011-01-20 18:00:00</td>\n",
       "      <td>252.574339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2011-01-20 19:00:00</td>\n",
       "      <td>181.180108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>2011-01-20 20:00:00</td>\n",
       "      <td>152.250490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2011-01-20 21:00:00</td>\n",
       "      <td>148.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2011-01-20 22:00:00</td>\n",
       "      <td>133.871470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2011-01-20 23:00:00</td>\n",
       "      <td>85.183132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2011-01-21 00:00:00</td>\n",
       "      <td>85.487743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2011-01-21 01:00:00</td>\n",
       "      <td>84.299417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2011-01-21 02:00:00</td>\n",
       "      <td>86.396561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2011-01-21 03:00:00</td>\n",
       "      <td>66.384730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2011-01-21 04:00:00</td>\n",
       "      <td>54.058119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2011-01-21 05:00:00</td>\n",
       "      <td>58.541466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2011-01-21 06:00:00</td>\n",
       "      <td>77.722887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2011-01-21 07:00:00</td>\n",
       "      <td>137.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2011-01-21 08:00:00</td>\n",
       "      <td>171.411303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>2011-01-21 09:00:00</td>\n",
       "      <td>168.515512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2011-01-21 10:00:00</td>\n",
       "      <td>124.892048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2011-01-21 11:00:00</td>\n",
       "      <td>124.301293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>2011-01-21 12:00:00</td>\n",
       "      <td>111.360498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2011-01-21 13:00:00</td>\n",
       "      <td>131.409189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2011-01-21 14:00:00</td>\n",
       "      <td>130.042322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2011-01-21 15:00:00</td>\n",
       "      <td>129.316358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2011-01-21 16:00:00</td>\n",
       "      <td>134.419544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2011-01-21 17:00:00</td>\n",
       "      <td>134.070969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2011-01-21 18:00:00</td>\n",
       "      <td>129.403637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2011-01-21 19:00:00</td>\n",
       "      <td>136.793346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2011-01-21 20:00:00</td>\n",
       "      <td>128.884376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2011-01-21 21:00:00</td>\n",
       "      <td>144.760723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2011-01-21 22:00:00</td>\n",
       "      <td>133.849087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2011-01-21 23:00:00</td>\n",
       "      <td>134.694045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2011-01-22 00:00:00</td>\n",
       "      <td>105.927253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2011-01-22 01:00:00</td>\n",
       "      <td>93.874599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime       count\n",
       "431 2011-01-20 00:00:00   78.547512\n",
       "432 2011-01-20 01:00:00   84.036977\n",
       "433 2011-01-20 02:00:00   85.203285\n",
       "434 2011-01-20 03:00:00   81.654897\n",
       "435 2011-01-20 04:00:00   60.270158\n",
       "436 2011-01-20 05:00:00   52.389292\n",
       "437 2011-01-20 06:00:00   79.683771\n",
       "438 2011-01-20 07:00:00  112.608661\n",
       "439 2011-01-20 08:00:00  163.883517\n",
       "440 2011-01-20 09:00:00  170.969929\n",
       "441 2011-01-20 10:00:00  114.771241\n",
       "442 2011-01-20 11:00:00  133.127496\n",
       "443 2011-01-20 12:00:00  131.300359\n",
       "444 2011-01-20 13:00:00  139.335553\n",
       "445 2011-01-20 14:00:00  139.289268\n",
       "446 2011-01-20 15:00:00  141.441862\n",
       "447 2011-01-20 16:00:00  181.377906\n",
       "448 2011-01-20 17:00:00  209.215186\n",
       "449 2011-01-20 18:00:00  252.574339\n",
       "450 2011-01-20 19:00:00  181.180108\n",
       "451 2011-01-20 20:00:00  152.250490\n",
       "452 2011-01-20 21:00:00  148.044872\n",
       "453 2011-01-20 22:00:00  133.871470\n",
       "454 2011-01-20 23:00:00   85.183132\n",
       "455 2011-01-21 00:00:00   85.487743\n",
       "456 2011-01-21 01:00:00   84.299417\n",
       "457 2011-01-21 02:00:00   86.396561\n",
       "458 2011-01-21 03:00:00   66.384730\n",
       "459 2011-01-21 04:00:00   54.058119\n",
       "460 2011-01-21 05:00:00   58.541466\n",
       "461 2011-01-21 06:00:00   77.722887\n",
       "462 2011-01-21 07:00:00  137.000589\n",
       "463 2011-01-21 08:00:00  171.411303\n",
       "464 2011-01-21 09:00:00  168.515512\n",
       "465 2011-01-21 10:00:00  124.892048\n",
       "466 2011-01-21 11:00:00  124.301293\n",
       "467 2011-01-21 12:00:00  111.360498\n",
       "468 2011-01-21 13:00:00  131.409189\n",
       "469 2011-01-21 14:00:00  130.042322\n",
       "470 2011-01-21 15:00:00  129.316358\n",
       "471 2011-01-21 16:00:00  134.419544\n",
       "472 2011-01-21 17:00:00  134.070969\n",
       "473 2011-01-21 18:00:00  129.403637\n",
       "474 2011-01-21 19:00:00  136.793346\n",
       "475 2011-01-21 20:00:00  128.884376\n",
       "476 2011-01-21 21:00:00  144.760723\n",
       "477 2011-01-21 22:00:00  133.849087\n",
       "478 2011-01-21 23:00:00  134.694045\n",
       "479 2011-01-22 00:00:00  105.927253\n",
       "480 2011-01-22 01:00:00   93.874599"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_predictions.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
