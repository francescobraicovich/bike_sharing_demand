{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_86133/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_train_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_train_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, rf=True, mlp=False):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=2)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target}_pipeline_rf'] = globals()[f'{direction}_{target}_pipeline_rf']\n",
    "            \n",
    "            if mlp:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=2000, hidden_layer_sizes=(100, 100), verbose=True, n_iter_no_change=25)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target}_pipeline_mlp'] = globals()[f'{direction}_{target}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf = True, mlp = False):\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_coumns = columns_not_to_use + target_columns\n",
    "            df = df.drop(drop_coumns, axis=1)\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target}_pipeline_rf']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                print(f'{direction}_{target}_pipeline_rf')\n",
    "                print(feature_importances)\n",
    "\n",
    "            if mlp:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction):\n",
    "\n",
    "    # store the prediction in the masked dataframe\n",
    "    df.loc[mask, target] = prediction\n",
    "\n",
    "    lags = [1, 2]\n",
    "\n",
    "    if direction == 'negative':\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime + pd.DateOffset(days=lag)\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        lagged_mask = df['datetime'].isin(lagged_datetime)\n",
    "\n",
    "        # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "        df.loc[lagged_mask, target] = prediction\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                      target_columns, columns_not_to_use, \n",
    "                      test_period, lag_period, maximum_day,\n",
    "                      directions, rf=True, mlp=False):\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            df = training_data[direction]['y'].copy()\n",
    "            drop_columns = columns_not_to_use + target_columns\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "\n",
    "                # get the pipeline\n",
    "                if rf:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target}_pipeline_rf']\n",
    "                if mlp:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf and mlp:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "            prediction = df[target]\n",
    "\n",
    "            # assert there are no NaN values in the prediction\n",
    "            assert not prediction.isna().values.any()\n",
    "\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target}'] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation[f'{direction}_{target}'] = {\n",
    "                'mse': mse,\n",
    "                'mae': mae\n",
    "            }\n",
    "\n",
    "            # print the evaluation metrics\n",
    "            print(f'{direction}_{target}')\n",
    "            print(f'MSE: {mse}')\n",
    "            print(f'MAE: {mae}')\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual\n",
      "negative_casual_pipeline_rf\n",
      "           feature  importance\n",
      "19      casual, -1    0.557664\n",
      "2       workingday    0.118125\n",
      "6         humidity    0.077284\n",
      "29      casual, -2    0.043451\n",
      "11       dayofweek    0.032330\n",
      "8             hour    0.032056\n",
      "5            atemp    0.015320\n",
      "7        windspeed    0.013417\n",
      "21       count, -1    0.012643\n",
      "4             temp    0.009240\n",
      "9        dayofyear    0.008803\n",
      "12   windspeed, -1    0.008563\n",
      "24    humidity, -2    0.007599\n",
      "31       count, -2    0.006545\n",
      "28        temp, -2    0.006195\n",
      "18        temp, -1    0.006068\n",
      "30  registered, -2    0.005526\n",
      "22   windspeed, -2    0.005416\n",
      "23       atemp, -2    0.005289\n",
      "20  registered, -1    0.004942\n",
      "14    humidity, -1    0.004834\n",
      "10      weekofyear    0.004406\n",
      "13       atemp, -1    0.004382\n",
      "16  workingday, -1    0.002913\n",
      "3          weather    0.001977\n",
      "17     weather, -1    0.001574\n",
      "27     weather, -2    0.001522\n",
      "1          holiday    0.001055\n",
      "0           season    0.000616\n",
      "15     holiday, -1    0.000135\n",
      "26  workingday, -2    0.000102\n",
      "25     holiday, -2    0.000009\n",
      "positive_casual_pipeline_rf\n",
      "           feature  importance\n",
      "19      casual, +1    0.567740\n",
      "11       dayofweek    0.176018\n",
      "6         humidity    0.048234\n",
      "29      casual, +2    0.027983\n",
      "5            atemp    0.027551\n",
      "8             hour    0.021676\n",
      "21       count, +1    0.020049\n",
      "4             temp    0.016617\n",
      "20  registered, +1    0.010960\n",
      "7        windspeed    0.008757\n",
      "14    humidity, +1    0.008238\n",
      "9        dayofyear    0.008080\n",
      "24    humidity, +2    0.006937\n",
      "17     weather, +1    0.006302\n",
      "22   windspeed, +2    0.006284\n",
      "12   windspeed, +1    0.004738\n",
      "31       count, +2    0.004582\n",
      "18        temp, +1    0.004414\n",
      "30  registered, +2    0.004201\n",
      "1          holiday    0.003976\n",
      "13       atemp, +1    0.003348\n",
      "28        temp, +2    0.003000\n",
      "23       atemp, +2    0.002366\n",
      "10      weekofyear    0.002010\n",
      "2       workingday    0.001963\n",
      "27     weather, +2    0.001611\n",
      "3          weather    0.001307\n",
      "0           season    0.000364\n",
      "25     holiday, +2    0.000343\n",
      "26  workingday, +2    0.000276\n",
      "16  workingday, +1    0.000060\n",
      "15     holiday, +1    0.000014\n",
      "Fitting pipelines for registered\n",
      "negative_registered_pipeline_rf\n",
      "           feature  importance\n",
      "20  registered, -1    0.659822\n",
      "8             hour    0.101074\n",
      "2       workingday    0.034184\n",
      "11       dayofweek    0.027063\n",
      "30  registered, -2    0.020669\n",
      "19      casual, -1    0.020529\n",
      "3          weather    0.017274\n",
      "21       count, -1    0.015268\n",
      "6         humidity    0.014702\n",
      "23       atemp, -2    0.007818\n",
      "5            atemp    0.007499\n",
      "9        dayofyear    0.006896\n",
      "16  workingday, -1    0.006852\n",
      "22   windspeed, -2    0.006803\n",
      "17     weather, -1    0.005498\n",
      "29      casual, -2    0.005391\n",
      "4             temp    0.005305\n",
      "14    humidity, -1    0.005285\n",
      "31       count, -2    0.005119\n",
      "24    humidity, -2    0.004711\n",
      "12   windspeed, -1    0.004582\n",
      "7        windspeed    0.004324\n",
      "28        temp, -2    0.003917\n",
      "13       atemp, -1    0.002614\n",
      "10      weekofyear    0.002123\n",
      "18        temp, -1    0.001900\n",
      "27     weather, -2    0.000982\n",
      "1          holiday    0.000765\n",
      "15     holiday, -1    0.000643\n",
      "26  workingday, -2    0.000255\n",
      "0           season    0.000093\n",
      "25     holiday, -2    0.000040\n",
      "positive_registered_pipeline_rf\n",
      "           feature  importance\n",
      "21       count, +1    0.522670\n",
      "20  registered, +1    0.170923\n",
      "8             hour    0.071297\n",
      "11       dayofweek    0.047092\n",
      "6         humidity    0.021545\n",
      "2       workingday    0.018691\n",
      "3          weather    0.017696\n",
      "31       count, +2    0.014544\n",
      "19      casual, +1    0.012941\n",
      "17     weather, +1    0.010784\n",
      "29      casual, +2    0.010464\n",
      "30  registered, +2    0.009312\n",
      "14    humidity, +1    0.008933\n",
      "9        dayofyear    0.006901\n",
      "24    humidity, +2    0.006693\n",
      "12   windspeed, +1    0.006595\n",
      "7        windspeed    0.005159\n",
      "22   windspeed, +2    0.004822\n",
      "5            atemp    0.004501\n",
      "13       atemp, +1    0.004404\n",
      "4             temp    0.003878\n",
      "16  workingday, +1    0.003710\n",
      "26  workingday, +2    0.003410\n",
      "18        temp, +1    0.003127\n",
      "23       atemp, +2    0.002866\n",
      "10      weekofyear    0.002023\n",
      "28        temp, +2    0.001788\n",
      "27     weather, +2    0.001493\n",
      "0           season    0.001036\n",
      "25     holiday, +2    0.000395\n",
      "1          holiday    0.000239\n",
      "15     holiday, +1    0.000068\n",
      "Fitting pipelines for count\n",
      "negative_count_pipeline_rf\n",
      "           feature  importance\n",
      "21       count, -1    0.663465\n",
      "8             hour    0.083277\n",
      "6         humidity    0.036155\n",
      "20  registered, -1    0.027468\n",
      "29      casual, -2    0.021407\n",
      "11       dayofweek    0.020558\n",
      "30  registered, -2    0.012920\n",
      "3          weather    0.011923\n",
      "19      casual, -1    0.011591\n",
      "5            atemp    0.011282\n",
      "31       count, -2    0.010756\n",
      "14    humidity, -1    0.009808\n",
      "24    humidity, -2    0.009246\n",
      "4             temp    0.007866\n",
      "7        windspeed    0.007126\n",
      "9        dayofyear    0.006577\n",
      "17     weather, -1    0.005706\n",
      "22   windspeed, -2    0.005493\n",
      "12   windspeed, -1    0.004973\n",
      "2       workingday    0.004962\n",
      "23       atemp, -2    0.004647\n",
      "13       atemp, -1    0.004330\n",
      "18        temp, -1    0.003810\n",
      "28        temp, -2    0.003244\n",
      "10      weekofyear    0.002708\n",
      "1          holiday    0.002560\n",
      "16  workingday, -1    0.002557\n",
      "15     holiday, -1    0.001596\n",
      "0           season    0.000953\n",
      "27     weather, -2    0.000612\n",
      "26  workingday, -2    0.000409\n",
      "25     holiday, -2    0.000016\n",
      "positive_count_pipeline_rf\n",
      "           feature  importance\n",
      "21       count, +1    0.713587\n",
      "8             hour    0.046943\n",
      "6         humidity    0.033260\n",
      "11       dayofweek    0.032204\n",
      "19      casual, +1    0.020657\n",
      "20  registered, +1    0.019979\n",
      "5            atemp    0.013772\n",
      "3          weather    0.013713\n",
      "29      casual, +2    0.012752\n",
      "30  registered, +2    0.012208\n",
      "31       count, +2    0.009232\n",
      "22   windspeed, +2    0.008887\n",
      "14    humidity, +1    0.008859\n",
      "4             temp    0.007223\n",
      "17     weather, +1    0.006959\n",
      "12   windspeed, +1    0.005742\n",
      "9        dayofyear    0.005424\n",
      "24    humidity, +2    0.004570\n",
      "7        windspeed    0.004091\n",
      "23       atemp, +2    0.003626\n",
      "13       atemp, +1    0.002508\n",
      "2       workingday    0.002508\n",
      "18        temp, +1    0.002406\n",
      "28        temp, +2    0.001853\n",
      "25     holiday, +2    0.001386\n",
      "16  workingday, +1    0.001340\n",
      "26  workingday, +2    0.001281\n",
      "1          holiday    0.001204\n",
      "10      weekofyear    0.001115\n",
      "27     weather, +2    0.000343\n",
      "0           season    0.000291\n",
      "15     holiday, +1    0.000077\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 32)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m fitted_pipelines \u001b[38;5;241m=\u001b[39m fit_pipelines(pipelines, training_data, \n\u001b[1;32m     16\u001b[0m                                  target_columns, columns_not_to_use,\n\u001b[1;32m     17\u001b[0m                                  directions, rf\u001b[38;5;241m=\u001b[39mrf, mlp\u001b[38;5;241m=\u001b[39mmlp)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# predict the pipelines\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m predictions, dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pipelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitted_pipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtest_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximum_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# evaluate the pipelines\u001b[39;00m\n\u001b[1;32m     26\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m evaluate_pipelines(predictions, training_data, target_columns, directions)\n",
      "Cell \u001b[0;32mIn[152], line 46\u001b[0m, in \u001b[0;36mpredict_pipelines\u001b[0;34m(fitted_pipelines, training_data, target_columns, columns_not_to_use, test_period, lag_period, maximum_day, directions, rf, mlp)\u001b[0m\n\u001b[1;32m     44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m (prediction1 \u001b[38;5;241m+\u001b[39m prediction2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rf:\n\u001b[0;32m---> 46\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mlp:\n\u001b[1;32m     48\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m pipeline2\u001b[38;5;241m.\u001b[39mpredict(df_days)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1072\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1073\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1079\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 32)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year', 'casual', 'registered', 'count']\n",
    "target_columns = ['casual', 'registered', 'count']\n",
    "directions = ['negative', 'positive']\n",
    "rf = True\n",
    "mlp = False\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(target_columns, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 target_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                target_columns, columns_not_to_use, \n",
    "                                test_period, lag_period, maximum_day,\n",
    "                                directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation = evaluate_pipelines(predictions, training_data, target_columns, directions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m target_columns:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m direction \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# train the first pipeline\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirection\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_pipeline_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirection\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirection\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_train_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# save the most important feature into a dataframe\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         feature_importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pipeline_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_,\n\u001b[1;32m     16\u001b[0m             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_not_to_use, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m     17\u001b[0m             columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 475\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# select columns not to be used in the model\n",
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year', 'casual', 'registered', 'count']\n",
    "\n",
    "# train the models\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:\n",
    "        # train the first pipeline\n",
    "        globals()[f'{direction}_{target}_pipeline_1'].fit(\n",
    "            globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1),\n",
    "            globals()[f'{direction}_train_data'][target]\n",
    "        )\n",
    "\n",
    "        # save the most important feature into a dataframe\n",
    "        feature_importances = pd.DataFrame(\n",
    "            globals()[f'{direction}_{target}_pipeline_1'].named_steps['regressor'].feature_importances_,\n",
    "            index=globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1).columns,\n",
    "            columns=['importance']\n",
    "        )\n",
    "\n",
    "        # sort the dataframe\n",
    "        feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # print the most important features\n",
    "        print(f'{direction} {target} most important features:')\n",
    "        print(feature_importances)\n",
    "        print('')\n",
    "\n",
    "        # train the second pipeline\n",
    "        globals()[f'{direction}_{target}_mlp_pipeline_2'].fit(\n",
    "            globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1),\n",
    "            globals()[f'{direction}_train_data'][target]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate the models\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtarget_columns\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m direction \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# print the statetment of target and direction\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_columns' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:\n",
    "\n",
    "        # print the statetment of target and direction\n",
    "        print(f'{direction} {target} evaluation')\n",
    "\n",
    "        # predict the target using the first pipeline\n",
    "        globals()[f'{direction}_{target}_predictions_1'] = globals()[f'{direction}_{target}_pipeline_1'].predict(\n",
    "            globals()[f'{direction}_test_data'].drop(columns=columns_not_to_use, axis=1)\n",
    "        )\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae_1'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions_1'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE RF: {globals()[f\"{direction}_{target}_mae_1\"]}')\n",
    "\n",
    "        # predict the target using the second pipeline\n",
    "        globals()[f'{direction}_{target}_predictions_2'] = globals()[f'{direction}_{target}_mlp_pipeline_2'].predict(\n",
    "            globals()[f'{direction}_test_data'].drop(columns=columns_not_to_use, axis=1)\n",
    "        )\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae_2'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions_2'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE MLP: {globals()[f\"{direction}_{target}_mae_2\"]}')\n",
    "\n",
    "        # take the average of the two predictions\n",
    "        globals()[f'{direction}_{target}_predictions'] = (globals()[f'{direction}_{target}_predictions_1'] + globals()[f'{direction}_{target}_predictions_2']) / 2\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE AVG: {globals()[f\"{direction}_{target}_mae\"]}')\n",
    "        print('')\n",
    "\n",
    "    # combine predictions from both directions\n",
    "    globals()[f'{target}_predictions'] = (globals()[f'positive_{target}_predictions'] + globals()[f'negative_{target}_predictions']) / 2\n",
    "\n",
    "    # calculate the mean absolute error\n",
    "    globals()[f'{target}_mae'] = mean_absolute_error(data[data['dataset'] == 'test'][target], globals()[f'{target}_predictions'])\n",
    "\n",
    "    # print the results\n",
    "    print(f'{target} MAE: {globals()[f\"{target}_mae\"]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3423881943.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[187], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    for direction in ['positive', 'negative']:\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models:\n",
    "\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
