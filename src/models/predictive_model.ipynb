{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_93588/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_test_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_test_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, trees, hidden_layer_sizes, max_iter_no_change, rf, mlp, max_iter=2000):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf[i]:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp[i]:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes[i], verbose=True, n_iter_no_change=max_iter_no_change[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf, mlp):\n",
    "\n",
    "    #print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in target_columns]\n",
    "        else:\n",
    "            target_name_columns = target_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            #drop_columns = columns_not_to_use\n",
    "\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "            print(f'Columns used: {df.columns}')\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf[i]:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \"\"\"\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \"\"\"\n",
    "\n",
    "            if mlp[i]:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction, train_columns):\n",
    "    saved_target = target\n",
    "    #print('target: ', saved_target)\n",
    "    # find the column in which the prediction is stored\n",
    "    for col in train_columns:\n",
    "        if target == col:\n",
    "            if 'original' in col:\n",
    "                target = col[:-len('_original')]\n",
    "                cols_to_insert_prediction = [target, col]\n",
    "            else:\n",
    "                cols_to_insert_prediction = [target]\n",
    "        elif target in col:\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "        elif col in target:\n",
    "            target = col\n",
    "            col = saved_target\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "    \n",
    "    \n",
    "    #print('train columns: ', train_columns)\n",
    "    #print('cols to insert prediction: ', cols_to_insert_prediction)\n",
    "    #print('new target: ', target)\n",
    "\n",
    "    prediction_array = np.array(df[saved_target])\n",
    "    #print('length of prediction array: ', len(prediction_array))\n",
    "    prediction_array[mask] = prediction\n",
    "    #print('Nans in prediction array: ', np.isnan(prediction_array).sum())\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    for col in cols_to_insert_prediction:\n",
    "        df[col] = prediction_array\n",
    "        #print('Nans in column: ', col, df[col].isnull().sum())\n",
    "        \n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        #print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        #print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            #print('Inserting prediction for lag: ', lag, ' in coumns: ', cols_to_insert_prediction)\n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print(f'Predictions inserted from datetime: {lagged_datetime[lagged_mask].iloc[0]} to {lagged_datetime[lagged_mask].iloc[-1]}')\n",
    "            #print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "\n",
    "            lagged_col = f'{target}, '+sign+str(abs(lag))\n",
    "            #print('lagged col: ', lagged_col)\n",
    "            df.loc[datetime_mask, lagged_col] = prediction_to_store\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    df_positive = training_data['positive']['y'].copy()\n",
    "    df_negative = training_data['negative']['y'].copy()\n",
    "    \n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "            #print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            if direction == 'positive':\n",
    "                df = df_positive\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative\n",
    "            \n",
    "            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            #drop_columns = columns_not_to_use\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "            #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "                #print('Day: ', day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                #print('Number of rows: ', df_days.shape[0])\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                #print('Length of prediction: ', len(prediction))\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "                #print('current nan values: ', df[target].isna().sum())\n",
    "                #print('')\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "            #print('\\n\\n')\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines2(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "    \n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = data[direction]['y'][target].copy()\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "            # subsitute the target column with the prediction\n",
    "            data[direction]['y'][target] = prediction\n",
    "\n",
    "            # assert the mse and mae are 0\n",
    "            y = data[direction]['y'][target].copy()\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "            assert mse == 0\n",
    "            assert mae == 0\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "    return evaluation, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'year', 'hour', 'dayofyear', 'weekofyear',\n",
      "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
      "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
      "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
      "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
      "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'year', 'hour', 'dayofyear', 'weekofyear',\n",
      "       'dayofweek', 'windspeed, +1', 'atemp, +1', 'humidity, +1',\n",
      "       'holiday, +1', 'workingday, +1', 'weather, +1', 'temp, +1',\n",
      "       'casual, +1', 'registered, +1', 'count, +1', 'windspeed, +2',\n",
      "       'atemp, +2', 'humidity, +2', 'holiday, +2', 'workingday, +2',\n",
      "       'weather, +2', 'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "                         mse        mae\n",
      "negative_casual  1318.403838  17.548154\n",
      "positive_casual  1354.971841  17.901664\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [25, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "\n",
    "train_columns = ['casual_original']\n",
    "test_columns = ['casual_original']\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, training_data_registered = evaluate_pipelines2(predictions, training_data, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor direction in directions:\\n    for target in test_columns:\\n\\n        saved_target = target\\n        \\n        if 'original' in target:\\n            target = target[:-len('_original')]\\n        \\n        df = training_data[direction]['y'].copy()\\n\\n        #\\xa0find the key of the prediction\\n        for key in predictions.keys():\\n            if direction in key and target in key:\\n                saved_key = key\\n\\n        #\\xa0get the prediction\\n        prediction = predictions[saved_key]\\n\\n        # calculate the mse and mae\\n        mse = mean_squared_error(df[target], prediction)\\n        mae = mean_absolute_error(df[target], prediction)\\n        \\n        print(f'{direction}_{target} mse: {mse}')\\n        print(f'{direction}_{target} mae: {mae}')\\n\\n        #\\xa0store the prediction\\n        df[target] = prediction\\n\\n        #\\xa0store the dataframe\\n        training_data[direction]['y'] = df\\n\""
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the predictions into the training data\n",
    "\"\"\"\n",
    "for direction in directions:\n",
    "    for target in test_columns:\n",
    "\n",
    "        saved_target = target\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target = target[:-len('_original')]\n",
    "        \n",
    "        df = training_data[direction]['y'].copy()\n",
    "\n",
    "        # find the key of the prediction\n",
    "        for key in predictions.keys():\n",
    "            if direction in key and target in key:\n",
    "                saved_key = key\n",
    "\n",
    "        # get the prediction\n",
    "        prediction = predictions[saved_key]\n",
    "\n",
    "        # calculate the mse and mae\n",
    "        mse = mean_squared_error(df[target], prediction)\n",
    "        mae = mean_absolute_error(df[target], prediction)\n",
    "        \n",
    "        print(f'{direction}_{target} mse: {mse}')\n",
    "        print(f'{direction}_{target} mae: {mae}')\n",
    "\n",
    "        # store the prediction\n",
    "        df[target] = prediction\n",
    "\n",
    "        # store the dataframe\n",
    "        training_data[direction]['y'] = df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'year', 'hour', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
      "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
      "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
      "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
      "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'year', 'hour', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1', 'humidity, +1',\n",
      "       'holiday, +1', 'workingday, +1', 'weather, +1', 'temp, +1',\n",
      "       'casual, +1', 'registered, +1', 'count, +1', 'windspeed, +2',\n",
      "       'atemp, +2', 'humidity, +2', 'holiday, +2', 'workingday, +2',\n",
      "       'weather, +2', 'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for registered_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'year', 'hour', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
      "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
      "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
      "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
      "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'year', 'hour', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1', 'humidity, +1',\n",
      "       'holiday, +1', 'workingday, +1', 'weather, +1', 'temp, +1',\n",
      "       'casual, +1', 'registered, +1', 'count, +1', 'windspeed, +2',\n",
      "       'atemp, +2', 'humidity, +2', 'holiday, +2', 'workingday, +2',\n",
      "       'weather, +2', 'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "                              mse        mae\n",
      "negative_casual        896.534856  18.428451\n",
      "positive_casual        797.777182  16.803834\n",
      "negative_registered  17320.504543  81.736124\n",
      "positive_registered  11259.569679  66.559198\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['registered', 'count']\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data_registered, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data_registered, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count = evaluate_pipelines2(predictions, training_data_registered, test_columns, directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1',\n",
      "       'humidity, -1', 'holiday, -1', 'workingday, -1', 'weather, -1',\n",
      "       'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1',\n",
      "       'humidity, +1', 'holiday, +1', 'workingday, +1', 'weather, +1',\n",
      "       'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for registered_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1',\n",
      "       'humidity, -1', 'holiday, -1', 'workingday, -1', 'weather, -1',\n",
      "       'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1',\n",
      "       'humidity, +1', 'holiday, +1', 'workingday, +1', 'weather, +1',\n",
      "       'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for count_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1',\n",
      "       'humidity, -1', 'holiday, -1', 'workingday, -1', 'weather, -1',\n",
      "       'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'year', 'hour',\n",
      "       'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1',\n",
      "       'humidity, +1', 'holiday, +1', 'workingday, +1', 'weather, +1',\n",
      "       'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "Predicting pipelines for count_original\n",
      "                              mse         mae\n",
      "negative_casual          0.688393    0.443561\n",
      "positive_casual          1.003563    0.402972\n",
      "negative_registered   5202.007809   51.834121\n",
      "positive_registered   9294.287423   64.057656\n",
      "negative_count       61081.836422  178.069204\n",
      "positive_count       56780.265242  171.440314\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['count']\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [True, True, True]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count2 = evaluate_pipelines2(predictions, triaining_data_count, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for registered_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for count_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "Predicting pipelines for count_original\n",
      "                             mse        mae\n",
      "negative_casual         0.511634   0.375958\n",
      "positive_casual         0.707570   0.352758\n",
      "negative_registered   874.143461  23.372432\n",
      "positive_registered   540.721645  18.279991\n",
      "negative_count       7249.986959  35.280499\n",
      "positive_count       7402.984618  35.514919\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count2, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count2, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count3 = evaluate_pipelines2(predictions, triaining_data_count2, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for registered_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for count_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, -1',\n",
      "       'atemp, -1', 'humidity, -1', 'holiday, -1', 'workingday, -1',\n",
      "       'weather, -1', 'temp, -1', 'casual, -1', 'registered, -1', 'count, -1',\n",
      "       'windspeed, -2', 'atemp, -2', 'humidity, -2', 'holiday, -2',\n",
      "       'workingday, -2', 'weather, -2', 'temp, -2', 'casual, -2',\n",
      "       'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'casual', 'registered', 'count', 'year',\n",
      "       'hour', 'dayofyear', 'weekofyear', 'dayofweek', 'windspeed, +1',\n",
      "       'atemp, +1', 'humidity, +1', 'holiday, +1', 'workingday, +1',\n",
      "       'weather, +1', 'temp, +1', 'casual, +1', 'registered, +1', 'count, +1',\n",
      "       'windspeed, +2', 'atemp, +2', 'humidity, +2', 'holiday, +2',\n",
      "       'workingday, +2', 'weather, +2', 'temp, +2', 'casual, +2',\n",
      "       'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "Predicting pipelines for count_original\n",
      "                           mse       mae\n",
      "negative_casual       0.521333  0.314287\n",
      "positive_casual       0.765052  0.411922\n",
      "negative_registered   9.152492  2.183027\n",
      "positive_registered  16.968252  3.086031\n",
      "negative_count        7.935982  1.250481\n",
      "positive_count       10.396841  1.478397\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [True, True, True]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count3, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count3, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count4 = evaluate_pipelines2(predictions, triaining_data_count3, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348      34.98\n",
      "349      21.86\n",
      "350      15.92\n",
      "351       8.84\n",
      "352       3.64\n",
      "         ...  \n",
      "17088     7.60\n",
      "17089     7.64\n",
      "17090     7.60\n",
      "17091     7.42\n",
      "17092     6.48\n",
      "Name: count_original, Length: 2286, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print count predictions\n",
    "print(predictions['negative_count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
