{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count', 'year', 'month', 'day', 'hour', 'dataset', 'dayofyear', 'weekofyear', 'dayofweek']\n"
     ]
    }
   ],
   "source": [
    "print(original_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_83210/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day >= 3 and x.day <= 15)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day > 15)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day >= 5 and x.day <= 17)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day < 5)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imoprt pipeline, scaler, RFregressor, MLPregressor and metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "target_columns = ['casual', 'registered']\n",
    "\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:\n",
    "\n",
    "        # create random forest pipeline for the target and direction\n",
    "        globals()[f'{direction}_{target}_pipeline_1'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', RandomForestRegressor(n_estimators=100)),\n",
    "        ])\n",
    "        \n",
    "        \"\"\"\n",
    "        # create MLP pipeline for the target and direction\n",
    "        globals()[f'{direction}_{target}_mlp_pipeline_2'] = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', MLPRegressor(max_iter=2000, hidden_layer_sizes=(100, 100), verbose=True, n_iter_no_change=25)),\n",
    "        ])\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive casual most important features:\n",
      "                importance\n",
      "casual, +1        0.531137\n",
      "dayofweek         0.161453\n",
      "humidity          0.070568\n",
      "atemp             0.027914\n",
      "hour              0.025007\n",
      "casual, +2        0.023519\n",
      "registered, +1    0.018377\n",
      "temp              0.016690\n",
      "humidity, +1      0.015712\n",
      "humidity, +2      0.012161\n",
      "windspeed, +2     0.009579\n",
      "registered, +2    0.009482\n",
      "windspeed         0.008177\n",
      "windspeed, +1     0.007368\n",
      "atemp, +2         0.006897\n",
      "temp, +1          0.006791\n",
      "workingday        0.006438\n",
      "temp, +2          0.006303\n",
      "atemp, +1         0.005351\n",
      "weather, +1       0.004699\n",
      "dayofyear         0.004600\n",
      "holiday, +1       0.004250\n",
      "holiday, +2       0.003412\n",
      "weekofyear        0.003247\n",
      "workingday, +2    0.002641\n",
      "holiday           0.002340\n",
      "weather           0.002147\n",
      "workingday, +1    0.001839\n",
      "weather, +2       0.001248\n",
      "season            0.000652\n",
      "\n",
      "Iteration 1, loss = 1435.60512487\n",
      "Iteration 2, loss = 500.50356986\n",
      "Iteration 3, loss = 360.57507992\n",
      "Iteration 4, loss = 301.35804920\n",
      "Iteration 5, loss = 261.42217508\n",
      "Iteration 6, loss = 224.69962032\n",
      "Iteration 7, loss = 198.74261331\n",
      "Iteration 8, loss = 177.36572038\n",
      "Iteration 9, loss = 162.63566775\n",
      "Iteration 10, loss = 152.36154041\n",
      "Iteration 11, loss = 139.26807144\n",
      "Iteration 12, loss = 130.55186718\n",
      "Iteration 13, loss = 121.45177754\n",
      "Iteration 14, loss = 115.61491417\n",
      "Iteration 15, loss = 111.02231713\n",
      "Iteration 16, loss = 104.87145004\n",
      "Iteration 17, loss = 99.53092512\n",
      "Iteration 18, loss = 97.84009193\n",
      "Iteration 19, loss = 94.26377476\n",
      "Iteration 20, loss = 91.32941948\n",
      "Iteration 21, loss = 88.88238945\n",
      "Iteration 22, loss = 83.85020460\n",
      "Iteration 23, loss = 79.51358486\n",
      "Iteration 24, loss = 79.13579207\n",
      "Iteration 25, loss = 77.32891913\n",
      "Iteration 26, loss = 71.53926122\n",
      "Iteration 27, loss = 71.48631332\n",
      "Iteration 28, loss = 69.03509381\n",
      "Iteration 29, loss = 68.97966094\n",
      "Iteration 30, loss = 64.52713077\n",
      "Iteration 31, loss = 63.28008129\n",
      "Iteration 32, loss = 62.27991202\n",
      "Iteration 33, loss = 60.30430597\n",
      "Iteration 34, loss = 59.13107908\n",
      "Iteration 35, loss = 57.90374146\n",
      "Iteration 36, loss = 57.90414093\n",
      "Iteration 37, loss = 56.63909688\n",
      "Iteration 38, loss = 54.74713470\n",
      "Iteration 39, loss = 53.98983022\n",
      "Iteration 40, loss = 52.40086269\n",
      "Iteration 41, loss = 52.64102611\n",
      "Iteration 42, loss = 52.92417661\n",
      "Iteration 43, loss = 49.27529355\n",
      "Iteration 44, loss = 49.25867334\n",
      "Iteration 45, loss = 50.28107672\n",
      "Iteration 46, loss = 48.25068025\n",
      "Iteration 47, loss = 45.92087788\n",
      "Iteration 48, loss = 45.31119902\n",
      "Iteration 49, loss = 46.26630892\n",
      "Iteration 50, loss = 45.38825864\n",
      "Iteration 51, loss = 43.60496458\n",
      "Iteration 52, loss = 43.29214032\n",
      "Iteration 53, loss = 44.63987464\n",
      "Iteration 54, loss = 43.02470038\n",
      "Iteration 55, loss = 41.59282965\n",
      "Iteration 56, loss = 40.96340172\n",
      "Iteration 57, loss = 39.95741222\n",
      "Iteration 58, loss = 40.18454985\n",
      "Iteration 59, loss = 38.51383804\n",
      "Iteration 60, loss = 38.62927223\n",
      "Iteration 61, loss = 37.89115845\n",
      "Iteration 62, loss = 37.24056367\n",
      "Iteration 63, loss = 37.59530936\n",
      "Iteration 64, loss = 37.97605821\n",
      "Iteration 65, loss = 35.89807266\n",
      "Iteration 66, loss = 35.18389599\n",
      "Iteration 67, loss = 34.69075612\n",
      "Iteration 68, loss = 35.05197170\n",
      "Iteration 69, loss = 33.51866095\n",
      "Iteration 70, loss = 33.95636460\n",
      "Iteration 71, loss = 33.57292379\n",
      "Iteration 72, loss = 33.56265741\n",
      "Iteration 73, loss = 32.65745169\n",
      "Iteration 74, loss = 31.98743840\n",
      "Iteration 75, loss = 31.06940689\n",
      "Iteration 76, loss = 32.87093349\n",
      "Iteration 77, loss = 32.73676018\n",
      "Iteration 78, loss = 30.43317950\n",
      "Iteration 79, loss = 30.30167191\n",
      "Iteration 80, loss = 30.64448635\n",
      "Iteration 81, loss = 31.72270035\n",
      "Iteration 82, loss = 31.74306659\n",
      "Iteration 83, loss = 29.41840263\n",
      "Iteration 84, loss = 28.54655105\n",
      "Iteration 85, loss = 28.87542081\n",
      "Iteration 86, loss = 29.87060722\n",
      "Iteration 87, loss = 28.34777637\n",
      "Iteration 88, loss = 27.46444909\n",
      "Iteration 89, loss = 28.59678034\n",
      "Iteration 90, loss = 27.52762212\n",
      "Iteration 91, loss = 26.78968271\n",
      "Iteration 92, loss = 26.19185573\n",
      "Iteration 93, loss = 26.06289339\n",
      "Iteration 94, loss = 25.44312432\n",
      "Iteration 95, loss = 25.66760909\n",
      "Iteration 96, loss = 25.46992928\n",
      "Iteration 97, loss = 25.45402307\n",
      "Iteration 98, loss = 25.62123240\n",
      "Iteration 99, loss = 25.66826091\n",
      "Iteration 100, loss = 24.41921919\n",
      "Iteration 101, loss = 24.38049455\n",
      "Iteration 102, loss = 25.09179922\n",
      "Iteration 103, loss = 24.20681765\n",
      "Iteration 104, loss = 23.23365000\n",
      "Iteration 105, loss = 24.73763319\n",
      "Iteration 106, loss = 23.04975543\n",
      "Iteration 107, loss = 24.60936440\n",
      "Iteration 108, loss = 23.58715753\n",
      "Iteration 109, loss = 23.18233066\n",
      "Iteration 110, loss = 22.95227356\n",
      "Iteration 111, loss = 22.55838952\n",
      "Iteration 112, loss = 21.93282394\n",
      "Iteration 113, loss = 21.64930630\n",
      "Iteration 114, loss = 21.74207795\n",
      "Iteration 115, loss = 21.74333763\n",
      "Iteration 116, loss = 22.59603358\n",
      "Iteration 117, loss = 22.03205797\n",
      "Iteration 118, loss = 21.16591617\n",
      "Iteration 119, loss = 20.84588099\n",
      "Iteration 120, loss = 21.04058973\n",
      "Iteration 121, loss = 21.81509108\n",
      "Iteration 122, loss = 21.62207096\n",
      "Iteration 123, loss = 20.06152036\n",
      "Iteration 124, loss = 19.89557897\n",
      "Iteration 125, loss = 20.87625211\n",
      "Iteration 126, loss = 20.60544278\n",
      "Iteration 127, loss = 20.67408837\n",
      "Iteration 128, loss = 19.74461189\n",
      "Iteration 129, loss = 19.21955126\n",
      "Iteration 130, loss = 18.66791576\n",
      "Iteration 131, loss = 19.50599086\n",
      "Iteration 132, loss = 19.55417472\n",
      "Iteration 133, loss = 19.28357155\n",
      "Iteration 134, loss = 20.34273400\n",
      "Iteration 135, loss = 21.11230627\n",
      "Iteration 136, loss = 19.65824241\n",
      "Iteration 137, loss = 18.54485973\n",
      "Iteration 138, loss = 18.01116054\n",
      "Iteration 139, loss = 18.09345640\n",
      "Iteration 140, loss = 17.57888724\n",
      "Iteration 141, loss = 17.32193485\n",
      "Iteration 142, loss = 18.04989509\n",
      "Iteration 143, loss = 17.52210167\n",
      "Iteration 144, loss = 17.70247511\n",
      "Iteration 145, loss = 17.87065016\n",
      "Iteration 146, loss = 17.39996338\n",
      "Iteration 147, loss = 17.03598124\n",
      "Iteration 148, loss = 17.23274686\n",
      "Iteration 149, loss = 17.51024038\n",
      "Iteration 150, loss = 17.26269409\n",
      "Iteration 151, loss = 17.35179837\n",
      "Iteration 152, loss = 16.92437478\n",
      "Iteration 153, loss = 17.48084821\n",
      "Iteration 154, loss = 17.81313024\n",
      "Iteration 155, loss = 17.75758279\n",
      "Iteration 156, loss = 16.65686551\n",
      "Iteration 157, loss = 17.00367638\n",
      "Iteration 158, loss = 17.54110413\n",
      "Iteration 159, loss = 16.10397849\n",
      "Iteration 160, loss = 16.47686619\n",
      "Iteration 161, loss = 15.96294569\n",
      "Iteration 162, loss = 17.65725117\n",
      "Iteration 163, loss = 16.70037842\n",
      "Iteration 164, loss = 16.45163397\n",
      "Iteration 165, loss = 15.61582203\n",
      "Iteration 166, loss = 16.37182336\n",
      "Iteration 167, loss = 15.92011937\n",
      "Iteration 168, loss = 14.65292205\n",
      "Iteration 169, loss = 15.34111569\n",
      "Iteration 170, loss = 16.26679232\n",
      "Iteration 171, loss = 14.48432141\n",
      "Iteration 172, loss = 14.00996270\n",
      "Iteration 173, loss = 16.73773449\n",
      "Iteration 174, loss = 16.55256342\n",
      "Iteration 175, loss = 15.04014617\n",
      "Iteration 176, loss = 14.50520194\n",
      "Iteration 177, loss = 15.54970179\n",
      "Iteration 178, loss = 15.49258204\n",
      "Iteration 179, loss = 15.38311810\n",
      "Iteration 180, loss = 14.48795148\n",
      "Iteration 181, loss = 13.86359629\n",
      "Iteration 182, loss = 14.50576565\n",
      "Iteration 183, loss = 14.01779928\n",
      "Iteration 184, loss = 13.79115986\n",
      "Iteration 185, loss = 13.91646500\n",
      "Iteration 186, loss = 13.05577086\n",
      "Iteration 187, loss = 13.05828420\n",
      "Iteration 188, loss = 14.92348039\n",
      "Iteration 189, loss = 14.33733383\n",
      "Iteration 190, loss = 13.36546824\n",
      "Iteration 191, loss = 12.90251281\n",
      "Iteration 192, loss = 13.27246038\n",
      "Iteration 193, loss = 14.29601823\n",
      "Iteration 194, loss = 13.18386535\n",
      "Iteration 195, loss = 12.89396623\n",
      "Iteration 196, loss = 13.52108313\n",
      "Iteration 197, loss = 12.79819431\n",
      "Iteration 198, loss = 14.86228676\n",
      "Iteration 199, loss = 13.04778360\n",
      "Iteration 200, loss = 12.45914600\n",
      "Iteration 201, loss = 12.51757856\n",
      "Iteration 202, loss = 13.36751270\n",
      "Iteration 203, loss = 12.83710892\n",
      "Iteration 204, loss = 12.14267532\n",
      "Iteration 205, loss = 11.87526319\n",
      "Iteration 206, loss = 12.14036656\n",
      "Iteration 207, loss = 12.00252481\n",
      "Iteration 208, loss = 11.58097458\n",
      "Iteration 209, loss = 11.54008308\n",
      "Iteration 210, loss = 13.47028978\n",
      "Iteration 211, loss = 11.78898194\n",
      "Iteration 212, loss = 11.81092456\n",
      "Iteration 213, loss = 12.24906190\n",
      "Iteration 214, loss = 11.82377066\n",
      "Iteration 215, loss = 11.61993850\n",
      "Iteration 216, loss = 11.30406895\n",
      "Iteration 217, loss = 11.36302056\n",
      "Iteration 218, loss = 11.72187760\n",
      "Iteration 219, loss = 11.03344831\n",
      "Iteration 220, loss = 11.37852159\n",
      "Iteration 221, loss = 10.75590317\n",
      "Iteration 222, loss = 11.50334438\n",
      "Iteration 223, loss = 14.85497660\n",
      "Iteration 224, loss = 11.45085442\n",
      "Iteration 225, loss = 11.02591018\n",
      "Iteration 226, loss = 10.87215767\n",
      "Iteration 227, loss = 11.23761516\n",
      "Iteration 228, loss = 11.33742007\n",
      "Iteration 229, loss = 10.92231316\n",
      "Iteration 230, loss = 11.28611199\n",
      "Iteration 231, loss = 10.58371956\n",
      "Iteration 232, loss = 11.27166575\n",
      "Iteration 233, loss = 10.81911761\n",
      "Iteration 234, loss = 11.04965864\n",
      "Iteration 235, loss = 10.21052079\n",
      "Iteration 236, loss = 10.43045326\n",
      "Iteration 237, loss = 10.03113720\n",
      "Iteration 238, loss = 9.72105757\n",
      "Iteration 239, loss = 10.18998355\n",
      "Iteration 240, loss = 9.70989919\n",
      "Iteration 241, loss = 10.14442770\n",
      "Iteration 242, loss = 13.25460083\n",
      "Iteration 243, loss = 15.30991703\n",
      "Iteration 244, loss = 10.77956044\n",
      "Iteration 245, loss = 11.14909053\n",
      "Iteration 246, loss = 10.31009170\n",
      "Iteration 247, loss = 10.32087580\n",
      "Iteration 248, loss = 9.75354223\n",
      "Iteration 249, loss = 9.65926189\n",
      "Iteration 250, loss = 10.52226437\n",
      "Iteration 251, loss = 11.31557745\n",
      "Iteration 252, loss = 10.24279171\n",
      "Iteration 253, loss = 8.86338551\n",
      "Iteration 254, loss = 8.83707671\n",
      "Iteration 255, loss = 8.89798546\n",
      "Iteration 256, loss = 9.08049655\n",
      "Iteration 257, loss = 8.93937836\n",
      "Iteration 258, loss = 9.19305571\n",
      "Iteration 259, loss = 9.16807628\n",
      "Iteration 260, loss = 9.62722651\n",
      "Iteration 261, loss = 10.41533771\n",
      "Iteration 262, loss = 10.75822062\n",
      "Iteration 263, loss = 9.02422526\n",
      "Iteration 264, loss = 8.36326033\n",
      "Iteration 265, loss = 8.42637113\n",
      "Iteration 266, loss = 8.38177505\n",
      "Iteration 267, loss = 8.98726165\n",
      "Iteration 268, loss = 8.92861836\n",
      "Iteration 269, loss = 8.65392812\n",
      "Iteration 270, loss = 8.54028807\n",
      "Iteration 271, loss = 9.70659114\n",
      "Iteration 272, loss = 9.21521595\n",
      "Iteration 273, loss = 9.29689398\n",
      "Iteration 274, loss = 8.16922233\n",
      "Iteration 275, loss = 8.10923085\n",
      "Iteration 276, loss = 8.52950266\n",
      "Iteration 277, loss = 8.99534945\n",
      "Iteration 278, loss = 9.80680798\n",
      "Iteration 279, loss = 9.10372617\n",
      "Iteration 280, loss = 9.38966868\n",
      "Iteration 281, loss = 8.27132647\n",
      "Iteration 282, loss = 9.10189075\n",
      "Iteration 283, loss = 9.17119828\n",
      "Iteration 284, loss = 9.10171181\n",
      "Iteration 285, loss = 8.24576989\n",
      "Iteration 286, loss = 8.17710608\n",
      "Iteration 287, loss = 9.91579717\n",
      "Iteration 288, loss = 11.36602373\n",
      "Iteration 289, loss = 10.89509254\n",
      "Iteration 290, loss = 9.37316390\n",
      "Iteration 291, loss = 8.81939354\n",
      "Iteration 292, loss = 8.45443467\n",
      "Iteration 293, loss = 8.19466505\n",
      "Iteration 294, loss = 8.28508570\n",
      "Iteration 295, loss = 8.18553610\n",
      "Iteration 296, loss = 8.54243007\n",
      "Iteration 297, loss = 7.59490573\n",
      "Iteration 298, loss = 7.76464596\n",
      "Iteration 299, loss = 8.24911863\n",
      "Iteration 300, loss = 7.87586431\n",
      "Iteration 301, loss = 7.33756915\n",
      "Iteration 302, loss = 7.39592915\n",
      "Iteration 303, loss = 7.81946487\n",
      "Iteration 304, loss = 11.74810490\n",
      "Iteration 305, loss = 9.00739001\n",
      "Iteration 306, loss = 7.52564769\n",
      "Iteration 307, loss = 7.30360183\n",
      "Iteration 308, loss = 7.24762449\n",
      "Iteration 309, loss = 7.39445565\n",
      "Iteration 310, loss = 6.85030740\n",
      "Iteration 311, loss = 6.90727568\n",
      "Iteration 312, loss = 6.92404695\n",
      "Iteration 313, loss = 7.19364286\n",
      "Iteration 314, loss = 7.25479226\n",
      "Iteration 315, loss = 6.78554777\n",
      "Iteration 316, loss = 7.27372974\n",
      "Iteration 317, loss = 6.98963972\n",
      "Iteration 318, loss = 7.35334207\n",
      "Iteration 319, loss = 6.74296934\n",
      "Iteration 320, loss = 6.84674922\n",
      "Iteration 321, loss = 7.10264997\n",
      "Iteration 322, loss = 7.49174997\n",
      "Iteration 323, loss = 7.40265809\n",
      "Iteration 324, loss = 6.95028501\n",
      "Iteration 325, loss = 8.53188439\n",
      "Iteration 326, loss = 8.11731926\n",
      "Iteration 327, loss = 8.46292363\n",
      "Iteration 328, loss = 7.39694521\n",
      "Iteration 329, loss = 7.20973975\n",
      "Iteration 330, loss = 7.00387793\n",
      "Iteration 331, loss = 7.61426496\n",
      "Iteration 332, loss = 7.63262542\n",
      "Iteration 333, loss = 8.14660884\n",
      "Iteration 334, loss = 6.85880864\n",
      "Iteration 335, loss = 6.33849124\n",
      "Iteration 336, loss = 6.02285170\n",
      "Iteration 337, loss = 6.23938169\n",
      "Iteration 338, loss = 7.56922400\n",
      "Iteration 339, loss = 6.61928843\n",
      "Iteration 340, loss = 6.40008551\n",
      "Iteration 341, loss = 7.06282432\n",
      "Iteration 342, loss = 6.93919374\n",
      "Iteration 343, loss = 6.47028453\n",
      "Iteration 344, loss = 7.20968405\n",
      "Iteration 345, loss = 7.63855079\n",
      "Iteration 346, loss = 7.79963806\n",
      "Iteration 347, loss = 7.10590175\n",
      "Iteration 348, loss = 5.96130874\n",
      "Iteration 349, loss = 6.70238972\n",
      "Iteration 350, loss = 6.61657292\n",
      "Iteration 351, loss = 6.67354516\n",
      "Iteration 352, loss = 6.93162622\n",
      "Iteration 353, loss = 6.12929324\n",
      "Iteration 354, loss = 7.50765920\n",
      "Iteration 355, loss = 7.93975941\n",
      "Iteration 356, loss = 6.48710027\n",
      "Iteration 357, loss = 5.83378560\n",
      "Iteration 358, loss = 5.82337851\n",
      "Iteration 359, loss = 5.99267795\n",
      "Iteration 360, loss = 6.29446632\n",
      "Iteration 361, loss = 6.77537993\n",
      "Iteration 362, loss = 6.11976147\n",
      "Iteration 363, loss = 5.64767308\n",
      "Iteration 364, loss = 5.53919209\n",
      "Iteration 365, loss = 5.53181872\n",
      "Iteration 366, loss = 5.66972659\n",
      "Iteration 367, loss = 5.76930945\n",
      "Iteration 368, loss = 6.32914747\n",
      "Iteration 369, loss = 6.28232775\n",
      "Iteration 370, loss = 6.10667613\n",
      "Iteration 371, loss = 5.86690956\n",
      "Iteration 372, loss = 5.93821465\n",
      "Iteration 373, loss = 5.89198355\n",
      "Iteration 374, loss = 5.57460752\n",
      "Iteration 375, loss = 6.55269613\n",
      "Iteration 376, loss = 6.06030525\n",
      "Iteration 377, loss = 6.05035155\n",
      "Iteration 378, loss = 5.87691894\n",
      "Iteration 379, loss = 6.38058125\n",
      "Iteration 380, loss = 7.05513264\n",
      "Iteration 381, loss = 7.60783490\n",
      "Iteration 382, loss = 5.96048385\n",
      "Iteration 383, loss = 6.89652324\n",
      "Iteration 384, loss = 5.80294803\n",
      "Iteration 385, loss = 5.14119242\n",
      "Iteration 386, loss = 5.08899287\n",
      "Iteration 387, loss = 6.71663301\n",
      "Iteration 388, loss = 6.26912577\n",
      "Iteration 389, loss = 6.51470124\n",
      "Iteration 390, loss = 7.64100970\n",
      "Iteration 391, loss = 7.95238121\n",
      "Iteration 392, loss = 6.45057103\n",
      "Iteration 393, loss = 6.29437146\n",
      "Iteration 394, loss = 5.51145667\n",
      "Iteration 395, loss = 5.23876480\n",
      "Iteration 396, loss = 5.21180381\n",
      "Iteration 397, loss = 5.43579973\n",
      "Iteration 398, loss = 5.88972640\n",
      "Iteration 399, loss = 5.58130905\n",
      "Iteration 400, loss = 4.96012729\n",
      "Iteration 401, loss = 5.06717890\n",
      "Iteration 402, loss = 6.24299556\n",
      "Iteration 403, loss = 5.99013486\n",
      "Iteration 404, loss = 4.85046543\n",
      "Iteration 405, loss = 6.14476416\n",
      "Iteration 406, loss = 5.08891403\n",
      "Iteration 407, loss = 4.98331732\n",
      "Iteration 408, loss = 6.13915610\n",
      "Iteration 409, loss = 6.20653778\n",
      "Iteration 410, loss = 6.02692377\n",
      "Iteration 411, loss = 6.58415529\n",
      "Iteration 412, loss = 5.52665178\n",
      "Iteration 413, loss = 5.21977590\n",
      "Iteration 414, loss = 4.67276602\n",
      "Iteration 415, loss = 4.74153906\n",
      "Iteration 416, loss = 4.92849241\n",
      "Iteration 417, loss = 4.90012118\n",
      "Iteration 418, loss = 4.94095134\n",
      "Iteration 419, loss = 4.84971988\n",
      "Iteration 420, loss = 4.84647876\n",
      "Iteration 421, loss = 4.44647105\n",
      "Iteration 422, loss = 5.18254271\n",
      "Iteration 423, loss = 5.53597312\n",
      "Iteration 424, loss = 6.08297486\n",
      "Iteration 425, loss = 4.81502143\n",
      "Iteration 426, loss = 4.26812604\n",
      "Iteration 427, loss = 4.63328211\n",
      "Iteration 428, loss = 4.74263279\n",
      "Iteration 429, loss = 4.98254029\n",
      "Iteration 430, loss = 5.90566556\n",
      "Iteration 431, loss = 5.79819796\n",
      "Iteration 432, loss = 5.59561918\n",
      "Iteration 433, loss = 4.97541096\n",
      "Iteration 434, loss = 5.75492980\n",
      "Iteration 435, loss = 6.06791521\n",
      "Iteration 436, loss = 5.97449826\n",
      "Iteration 437, loss = 5.12805807\n",
      "Iteration 438, loss = 5.15541614\n",
      "Iteration 439, loss = 4.51343620\n",
      "Iteration 440, loss = 4.47479144\n",
      "Iteration 441, loss = 4.18239439\n",
      "Iteration 442, loss = 4.25109979\n",
      "Iteration 443, loss = 4.86230442\n",
      "Iteration 444, loss = 4.59212547\n",
      "Iteration 445, loss = 4.83922575\n",
      "Iteration 446, loss = 4.97170758\n",
      "Iteration 447, loss = 4.69338660\n",
      "Iteration 448, loss = 4.31877959\n",
      "Iteration 449, loss = 4.34383084\n",
      "Iteration 450, loss = 4.43230776\n",
      "Iteration 451, loss = 4.66418669\n",
      "Iteration 452, loss = 4.65670831\n",
      "Iteration 453, loss = 4.96570575\n",
      "Iteration 454, loss = 6.18216756\n",
      "Iteration 455, loss = 5.46501931\n",
      "Iteration 456, loss = 4.44552696\n",
      "Iteration 457, loss = 5.70335228\n",
      "Iteration 458, loss = 8.71434804\n",
      "Iteration 459, loss = 7.46536628\n",
      "Iteration 460, loss = 5.66807661\n",
      "Iteration 461, loss = 4.76002218\n",
      "Iteration 462, loss = 4.14752714\n",
      "Iteration 463, loss = 3.99024900\n",
      "Iteration 464, loss = 5.45786692\n",
      "Iteration 465, loss = 4.69249693\n",
      "Iteration 466, loss = 4.52655768\n",
      "Iteration 467, loss = 4.21854110\n",
      "Iteration 468, loss = 4.47503524\n",
      "Iteration 469, loss = 4.70803051\n",
      "Iteration 470, loss = 4.26725130\n",
      "Iteration 471, loss = 3.89245682\n",
      "Iteration 472, loss = 4.07915079\n",
      "Iteration 473, loss = 3.82212911\n",
      "Iteration 474, loss = 4.11488156\n",
      "Iteration 475, loss = 3.86158617\n",
      "Iteration 476, loss = 3.72147439\n",
      "Iteration 477, loss = 4.39400683\n",
      "Iteration 478, loss = 4.60006372\n",
      "Iteration 479, loss = 5.48987290\n",
      "Iteration 480, loss = 4.69563371\n",
      "Iteration 481, loss = 4.19661579\n",
      "Iteration 482, loss = 4.18499515\n",
      "Iteration 483, loss = 4.07969228\n",
      "Iteration 484, loss = 4.54755789\n",
      "Iteration 485, loss = 4.61765722\n",
      "Iteration 486, loss = 4.61640320\n",
      "Iteration 487, loss = 4.38169997\n",
      "Iteration 488, loss = 4.29301564\n",
      "Iteration 489, loss = 4.20974242\n",
      "Iteration 490, loss = 4.42632425\n",
      "Iteration 491, loss = 4.00046284\n",
      "Iteration 492, loss = 3.71723399\n",
      "Iteration 493, loss = 3.40438810\n",
      "Iteration 494, loss = 3.47792314\n",
      "Iteration 495, loss = 5.23522775\n",
      "Iteration 496, loss = 7.86296829\n",
      "Iteration 497, loss = 6.10887222\n",
      "Iteration 498, loss = 4.75203945\n",
      "Iteration 499, loss = 3.98683584\n",
      "Iteration 500, loss = 3.38025944\n",
      "Iteration 501, loss = 3.87800541\n",
      "Iteration 502, loss = 4.15197207\n",
      "Iteration 503, loss = 4.28318374\n",
      "Iteration 504, loss = 4.08412338\n",
      "Iteration 505, loss = 4.66510702\n",
      "Iteration 506, loss = 4.63064093\n",
      "Iteration 507, loss = 7.16759964\n",
      "Iteration 508, loss = 5.68292982\n",
      "Iteration 509, loss = 4.26282666\n",
      "Iteration 510, loss = 4.66306451\n",
      "Iteration 511, loss = 4.35958074\n",
      "Iteration 512, loss = 3.41393656\n",
      "Iteration 513, loss = 3.92418242\n",
      "Iteration 514, loss = 4.43155457\n",
      "Iteration 515, loss = 3.96557037\n",
      "Iteration 516, loss = 3.74723004\n",
      "Iteration 517, loss = 3.83603258\n",
      "Iteration 518, loss = 4.00605092\n",
      "Iteration 519, loss = 4.74770275\n",
      "Iteration 520, loss = 3.32426562\n",
      "Iteration 521, loss = 3.35179714\n",
      "Iteration 522, loss = 3.43396500\n",
      "Iteration 523, loss = 3.45929355\n",
      "Iteration 524, loss = 3.57919454\n",
      "Iteration 525, loss = 4.16918259\n",
      "Iteration 526, loss = 3.95633840\n",
      "Iteration 527, loss = 4.15693372\n",
      "Iteration 528, loss = 3.56703403\n",
      "Iteration 529, loss = 3.36810821\n",
      "Iteration 530, loss = 4.07345321\n",
      "Iteration 531, loss = 3.68564746\n",
      "Iteration 532, loss = 3.26476366\n",
      "Iteration 533, loss = 2.98171138\n",
      "Iteration 534, loss = 3.29152470\n",
      "Iteration 535, loss = 4.13995573\n",
      "Iteration 536, loss = 3.76277138\n",
      "Iteration 537, loss = 4.55460727\n",
      "Iteration 538, loss = 4.83967041\n",
      "Iteration 539, loss = 5.32759541\n",
      "Iteration 540, loss = 5.52144553\n",
      "Iteration 541, loss = 6.12007181\n",
      "Iteration 542, loss = 6.18877914\n",
      "Iteration 543, loss = 4.39695649\n",
      "Iteration 544, loss = 4.04846282\n",
      "Iteration 545, loss = 3.40253821\n",
      "Iteration 546, loss = 3.77684485\n",
      "Iteration 547, loss = 3.38190128\n",
      "Iteration 548, loss = 3.59432997\n",
      "Iteration 549, loss = 3.19420622\n",
      "Iteration 550, loss = 3.41429980\n",
      "Iteration 551, loss = 3.12541177\n",
      "Iteration 552, loss = 3.03096582\n",
      "Iteration 553, loss = 3.49376649\n",
      "Iteration 554, loss = 4.45401313\n",
      "Iteration 555, loss = 4.12462314\n",
      "Iteration 556, loss = 4.04547699\n",
      "Iteration 557, loss = 3.76629027\n",
      "Iteration 558, loss = 3.92051369\n",
      "Iteration 559, loss = 3.47261097\n",
      "Training loss did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "negative casual most important features:\n",
      "                importance\n",
      "casual, -1        0.525363\n",
      "workingday        0.115116\n",
      "humidity          0.077324\n",
      "hour              0.035872\n",
      "casual, -2        0.028438\n",
      "dayofweek         0.027130\n",
      "atemp             0.023400\n",
      "registered, -1    0.018632\n",
      "temp              0.017075\n",
      "workingday, -1    0.014714\n",
      "windspeed         0.014575\n",
      "windspeed, -1     0.011372\n",
      "humidity, -2      0.011342\n",
      "registered, -2    0.011276\n",
      "windspeed, -2     0.010551\n",
      "dayofyear         0.008540\n",
      "humidity, -1      0.008489\n",
      "atemp, -2         0.008212\n",
      "temp, -2          0.007652\n",
      "atemp, -1         0.005603\n",
      "temp, -1          0.005200\n",
      "weekofyear        0.004203\n",
      "weather, -1       0.002438\n",
      "weather           0.002336\n",
      "workingday, -2    0.001793\n",
      "weather, -2       0.001306\n",
      "holiday           0.000996\n",
      "season            0.000729\n",
      "holiday, -1       0.000290\n",
      "holiday, -2       0.000036\n",
      "\n",
      "Iteration 1, loss = 1310.79466539\n",
      "Iteration 2, loss = 481.01624251\n",
      "Iteration 3, loss = 369.93440378\n",
      "Iteration 4, loss = 326.41841971\n",
      "Iteration 5, loss = 290.38480595\n",
      "Iteration 6, loss = 268.39047817\n",
      "Iteration 7, loss = 246.54795172\n",
      "Iteration 8, loss = 230.30011867\n",
      "Iteration 9, loss = 215.11703412\n",
      "Iteration 10, loss = 199.92579613\n",
      "Iteration 11, loss = 187.16658695\n",
      "Iteration 12, loss = 173.72431183\n",
      "Iteration 13, loss = 164.97987631\n",
      "Iteration 14, loss = 157.04486082\n",
      "Iteration 15, loss = 152.33543338\n",
      "Iteration 16, loss = 143.83994712\n",
      "Iteration 17, loss = 138.01666594\n",
      "Iteration 18, loss = 132.37828725\n",
      "Iteration 19, loss = 127.52969689\n",
      "Iteration 20, loss = 122.54372632\n",
      "Iteration 21, loss = 121.23300139\n",
      "Iteration 22, loss = 117.90036706\n",
      "Iteration 23, loss = 112.80867866\n",
      "Iteration 24, loss = 110.30700603\n",
      "Iteration 25, loss = 110.36473660\n",
      "Iteration 26, loss = 101.87365389\n",
      "Iteration 27, loss = 97.78879418\n",
      "Iteration 28, loss = 96.47876954\n",
      "Iteration 29, loss = 94.28577944\n",
      "Iteration 30, loss = 91.06402696\n",
      "Iteration 31, loss = 95.31351812\n",
      "Iteration 32, loss = 92.33697933\n",
      "Iteration 33, loss = 84.73790459\n",
      "Iteration 34, loss = 82.13807595\n",
      "Iteration 35, loss = 79.10747835\n",
      "Iteration 36, loss = 78.72763078\n",
      "Iteration 37, loss = 79.40119918\n",
      "Iteration 38, loss = 72.19991626\n",
      "Iteration 39, loss = 70.93417343\n",
      "Iteration 40, loss = 68.88521385\n",
      "Iteration 41, loss = 68.72503650\n",
      "Iteration 42, loss = 68.92381810\n",
      "Iteration 43, loss = 69.28940186\n",
      "Iteration 44, loss = 62.97488291\n",
      "Iteration 45, loss = 61.90286524\n",
      "Iteration 46, loss = 61.83686556\n",
      "Iteration 47, loss = 61.43794590\n",
      "Iteration 48, loss = 61.89995203\n",
      "Iteration 49, loss = 57.07938380\n",
      "Iteration 50, loss = 56.39946412\n",
      "Iteration 51, loss = 55.13057358\n",
      "Iteration 52, loss = 52.97706861\n",
      "Iteration 53, loss = 51.58208342\n",
      "Iteration 54, loss = 51.88848800\n",
      "Iteration 55, loss = 50.67340051\n",
      "Iteration 56, loss = 50.35301315\n",
      "Iteration 57, loss = 52.05267800\n",
      "Iteration 58, loss = 48.79839321\n",
      "Iteration 59, loss = 51.39765812\n",
      "Iteration 60, loss = 46.20222574\n",
      "Iteration 61, loss = 46.83841293\n",
      "Iteration 62, loss = 44.98428179\n",
      "Iteration 63, loss = 45.14013908\n",
      "Iteration 64, loss = 43.95442522\n",
      "Iteration 65, loss = 44.12954179\n",
      "Iteration 66, loss = 45.06887643\n",
      "Iteration 67, loss = 41.51548517\n",
      "Iteration 68, loss = 40.77739912\n",
      "Iteration 69, loss = 39.94178519\n",
      "Iteration 70, loss = 39.20594331\n",
      "Iteration 71, loss = 37.74632351\n",
      "Iteration 72, loss = 38.23866274\n",
      "Iteration 73, loss = 37.68508070\n",
      "Iteration 74, loss = 37.70565578\n",
      "Iteration 75, loss = 36.72832376\n",
      "Iteration 76, loss = 37.15845162\n",
      "Iteration 77, loss = 35.69409668\n",
      "Iteration 78, loss = 36.28275262\n",
      "Iteration 79, loss = 35.20780720\n",
      "Iteration 80, loss = 34.30000912\n",
      "Iteration 81, loss = 33.95555957\n",
      "Iteration 82, loss = 33.71962503\n",
      "Iteration 83, loss = 34.05087112\n",
      "Iteration 84, loss = 33.54715111\n",
      "Iteration 85, loss = 33.07448145\n",
      "Iteration 86, loss = 32.32675071\n",
      "Iteration 87, loss = 32.10298103\n",
      "Iteration 88, loss = 33.63466469\n",
      "Iteration 89, loss = 38.26433569\n",
      "Iteration 90, loss = 31.43516525\n",
      "Iteration 91, loss = 32.62943014\n",
      "Iteration 92, loss = 31.93489162\n",
      "Iteration 93, loss = 29.80228441\n",
      "Iteration 94, loss = 32.11390529\n",
      "Iteration 95, loss = 28.82218600\n",
      "Iteration 96, loss = 28.39017075\n",
      "Iteration 97, loss = 29.12515736\n",
      "Iteration 98, loss = 34.33134896\n",
      "Iteration 99, loss = 29.16641100\n",
      "Iteration 100, loss = 27.73829270\n",
      "Iteration 101, loss = 27.23427293\n",
      "Iteration 102, loss = 27.10868562\n",
      "Iteration 103, loss = 28.99006786\n",
      "Iteration 104, loss = 27.09280501\n",
      "Iteration 105, loss = 27.03339371\n",
      "Iteration 106, loss = 28.30866088\n",
      "Iteration 107, loss = 27.52113783\n",
      "Iteration 108, loss = 34.72528696\n",
      "Iteration 109, loss = 27.60191920\n",
      "Iteration 110, loss = 28.01559804\n",
      "Iteration 111, loss = 25.52172774\n",
      "Iteration 112, loss = 25.46149824\n",
      "Iteration 113, loss = 24.97411046\n",
      "Iteration 114, loss = 24.46193881\n",
      "Iteration 115, loss = 24.87481495\n",
      "Iteration 116, loss = 23.95865778\n",
      "Iteration 117, loss = 24.93363544\n",
      "Iteration 118, loss = 24.27828579\n",
      "Iteration 119, loss = 24.77997434\n",
      "Iteration 120, loss = 23.47732340\n",
      "Iteration 121, loss = 23.17755744\n",
      "Iteration 122, loss = 23.55114824\n",
      "Iteration 123, loss = 23.20011624\n",
      "Iteration 124, loss = 23.48420829\n",
      "Iteration 125, loss = 23.35033022\n",
      "Iteration 126, loss = 22.98324316\n",
      "Iteration 127, loss = 21.72360179\n",
      "Iteration 128, loss = 21.85223815\n",
      "Iteration 129, loss = 21.65279027\n",
      "Iteration 130, loss = 23.45021998\n",
      "Iteration 131, loss = 22.53487181\n",
      "Iteration 132, loss = 21.45485788\n",
      "Iteration 133, loss = 22.71344246\n",
      "Iteration 134, loss = 21.68279709\n",
      "Iteration 135, loss = 20.45536925\n",
      "Iteration 136, loss = 21.08955307\n",
      "Iteration 137, loss = 20.82810186\n",
      "Iteration 138, loss = 19.86353950\n",
      "Iteration 139, loss = 21.83121097\n",
      "Iteration 140, loss = 20.34287085\n",
      "Iteration 141, loss = 21.78919096\n",
      "Iteration 142, loss = 21.55123636\n",
      "Iteration 143, loss = 21.12556780\n",
      "Iteration 144, loss = 19.17206777\n",
      "Iteration 145, loss = 19.39771600\n",
      "Iteration 146, loss = 20.52122628\n",
      "Iteration 147, loss = 20.46163165\n",
      "Iteration 148, loss = 20.45798053\n",
      "Iteration 149, loss = 22.69121926\n",
      "Iteration 150, loss = 19.71157139\n",
      "Iteration 151, loss = 19.25316262\n",
      "Iteration 152, loss = 18.90800185\n",
      "Iteration 153, loss = 19.21973138\n",
      "Iteration 154, loss = 20.37259524\n",
      "Iteration 155, loss = 20.35567275\n",
      "Iteration 156, loss = 18.35033632\n",
      "Iteration 157, loss = 19.34693957\n",
      "Iteration 158, loss = 21.46233496\n",
      "Iteration 159, loss = 18.01385941\n",
      "Iteration 160, loss = 17.33353426\n",
      "Iteration 161, loss = 17.40425731\n",
      "Iteration 162, loss = 18.11846982\n",
      "Iteration 163, loss = 19.59260201\n",
      "Iteration 164, loss = 18.92896127\n",
      "Iteration 165, loss = 18.21005102\n",
      "Iteration 166, loss = 17.46551238\n",
      "Iteration 167, loss = 16.73206702\n",
      "Iteration 168, loss = 16.32175898\n",
      "Iteration 169, loss = 16.65258395\n",
      "Iteration 170, loss = 16.62075323\n",
      "Iteration 171, loss = 16.33375830\n",
      "Iteration 172, loss = 16.28086765\n",
      "Iteration 173, loss = 17.04835048\n",
      "Iteration 174, loss = 16.32397910\n",
      "Iteration 175, loss = 16.60629229\n",
      "Iteration 176, loss = 16.84596244\n",
      "Iteration 177, loss = 16.45410632\n",
      "Iteration 178, loss = 17.06588774\n",
      "Iteration 179, loss = 16.49165829\n",
      "Iteration 180, loss = 15.63479509\n",
      "Iteration 181, loss = 16.24132649\n",
      "Iteration 182, loss = 15.48118278\n",
      "Iteration 183, loss = 16.29951880\n",
      "Iteration 184, loss = 16.13669544\n",
      "Iteration 185, loss = 16.27403171\n",
      "Iteration 186, loss = 15.68961390\n",
      "Iteration 187, loss = 15.75027410\n",
      "Iteration 188, loss = 16.16757321\n",
      "Iteration 189, loss = 14.89850902\n",
      "Iteration 190, loss = 14.21547464\n",
      "Iteration 191, loss = 17.02061043\n",
      "Iteration 192, loss = 15.62927989\n",
      "Iteration 193, loss = 14.98338381\n",
      "Iteration 194, loss = 15.18252874\n",
      "Iteration 195, loss = 15.10294095\n",
      "Iteration 196, loss = 15.08957088\n",
      "Iteration 197, loss = 15.51364759\n",
      "Iteration 198, loss = 20.09570252\n",
      "Iteration 199, loss = 14.17899737\n",
      "Iteration 200, loss = 14.31027984\n",
      "Iteration 201, loss = 13.90601768\n",
      "Iteration 202, loss = 13.73207847\n",
      "Iteration 203, loss = 13.70001700\n",
      "Iteration 204, loss = 13.83748979\n",
      "Iteration 205, loss = 13.82019668\n",
      "Iteration 206, loss = 14.49880207\n",
      "Iteration 207, loss = 15.03417359\n",
      "Iteration 208, loss = 13.67111621\n",
      "Iteration 209, loss = 13.89884106\n",
      "Iteration 210, loss = 13.09895392\n",
      "Iteration 211, loss = 13.20849306\n",
      "Iteration 212, loss = 13.36447898\n",
      "Iteration 213, loss = 12.71135483\n",
      "Iteration 214, loss = 12.78075674\n",
      "Iteration 215, loss = 12.89668146\n",
      "Iteration 216, loss = 14.30632331\n",
      "Iteration 217, loss = 13.19412038\n",
      "Iteration 218, loss = 13.57492356\n",
      "Iteration 219, loss = 15.17539283\n",
      "Iteration 220, loss = 13.07241459\n",
      "Iteration 221, loss = 12.93701851\n",
      "Iteration 222, loss = 14.85690964\n",
      "Iteration 223, loss = 14.84653871\n",
      "Iteration 224, loss = 14.08636207\n",
      "Iteration 225, loss = 13.06026582\n",
      "Iteration 226, loss = 12.97998951\n",
      "Iteration 227, loss = 12.54133687\n",
      "Iteration 228, loss = 13.38644852\n",
      "Iteration 229, loss = 13.22280110\n",
      "Iteration 230, loss = 13.09103782\n",
      "Iteration 231, loss = 12.12661125\n",
      "Iteration 232, loss = 12.50374829\n",
      "Iteration 233, loss = 13.02382020\n",
      "Iteration 234, loss = 11.68976412\n",
      "Iteration 235, loss = 11.08775648\n",
      "Iteration 236, loss = 11.34684054\n",
      "Iteration 237, loss = 11.78017483\n",
      "Iteration 238, loss = 13.27891270\n",
      "Iteration 239, loss = 14.72653274\n",
      "Iteration 240, loss = 12.55672025\n",
      "Iteration 241, loss = 11.49758001\n",
      "Iteration 242, loss = 10.91501357\n",
      "Iteration 243, loss = 10.71573608\n",
      "Iteration 244, loss = 11.97591243\n",
      "Iteration 245, loss = 11.74396648\n",
      "Iteration 246, loss = 12.13821378\n",
      "Iteration 247, loss = 11.52166535\n",
      "Iteration 248, loss = 14.06319107\n",
      "Iteration 249, loss = 12.66356054\n",
      "Iteration 250, loss = 11.91158217\n",
      "Iteration 251, loss = 10.85744933\n",
      "Iteration 252, loss = 10.45542647\n",
      "Iteration 253, loss = 11.34430356\n",
      "Iteration 254, loss = 12.31857005\n",
      "Iteration 255, loss = 10.30865986\n",
      "Iteration 256, loss = 11.01568841\n",
      "Iteration 257, loss = 11.97275653\n",
      "Iteration 258, loss = 10.43014940\n",
      "Iteration 259, loss = 9.97338403\n",
      "Iteration 260, loss = 10.04534283\n",
      "Iteration 261, loss = 10.74679542\n",
      "Iteration 262, loss = 11.10007081\n",
      "Iteration 263, loss = 12.18427357\n",
      "Iteration 264, loss = 10.17502646\n",
      "Iteration 265, loss = 10.80787976\n",
      "Iteration 266, loss = 10.38072671\n",
      "Iteration 267, loss = 10.77831144\n",
      "Iteration 268, loss = 10.22955975\n",
      "Iteration 269, loss = 10.78983033\n",
      "Iteration 270, loss = 10.68730965\n",
      "Iteration 271, loss = 9.83128133\n",
      "Iteration 272, loss = 9.75420152\n",
      "Iteration 273, loss = 9.37719217\n",
      "Iteration 274, loss = 11.86682652\n",
      "Iteration 275, loss = 10.71037762\n",
      "Iteration 276, loss = 10.61827641\n",
      "Iteration 277, loss = 10.09910824\n",
      "Iteration 278, loss = 9.58726956\n",
      "Iteration 279, loss = 9.56497063\n",
      "Iteration 280, loss = 10.27191489\n",
      "Iteration 281, loss = 10.23015111\n",
      "Iteration 282, loss = 9.40345782\n",
      "Iteration 283, loss = 11.64171178\n",
      "Iteration 284, loss = 10.03239363\n",
      "Iteration 285, loss = 9.09130910\n",
      "Iteration 286, loss = 9.08935063\n",
      "Iteration 287, loss = 9.79931479\n",
      "Iteration 288, loss = 9.59402672\n",
      "Iteration 289, loss = 12.51911101\n",
      "Iteration 290, loss = 12.12161817\n",
      "Iteration 291, loss = 10.72482474\n",
      "Iteration 292, loss = 9.16810257\n",
      "Iteration 293, loss = 8.81904486\n",
      "Iteration 294, loss = 8.99589299\n",
      "Iteration 295, loss = 10.57219922\n",
      "Iteration 296, loss = 10.03436591\n",
      "Iteration 297, loss = 9.07279523\n",
      "Iteration 298, loss = 9.68538842\n",
      "Iteration 299, loss = 9.24916982\n",
      "Iteration 300, loss = 9.03567622\n",
      "Iteration 301, loss = 9.81884586\n",
      "Iteration 302, loss = 9.48534525\n",
      "Iteration 303, loss = 8.91420319\n",
      "Iteration 304, loss = 9.74736703\n",
      "Iteration 305, loss = 9.79581396\n",
      "Iteration 306, loss = 10.34356860\n",
      "Iteration 307, loss = 9.44548533\n",
      "Iteration 308, loss = 9.03368708\n",
      "Iteration 309, loss = 9.19979138\n",
      "Iteration 310, loss = 8.61842781\n",
      "Iteration 311, loss = 9.93674602\n",
      "Iteration 312, loss = 8.63709678\n",
      "Iteration 313, loss = 8.74873346\n",
      "Iteration 314, loss = 8.69528811\n",
      "Iteration 315, loss = 9.17783078\n",
      "Iteration 316, loss = 8.50831668\n",
      "Iteration 317, loss = 8.33732780\n",
      "Iteration 318, loss = 7.88483473\n",
      "Iteration 319, loss = 8.46235243\n",
      "Iteration 320, loss = 8.56047130\n",
      "Iteration 321, loss = 8.10855861\n",
      "Iteration 322, loss = 8.07814622\n",
      "Iteration 323, loss = 8.55127275\n",
      "Iteration 324, loss = 7.80530814\n",
      "Iteration 325, loss = 7.76185450\n",
      "Iteration 326, loss = 8.18307467\n",
      "Iteration 327, loss = 8.81872542\n",
      "Iteration 328, loss = 8.82021649\n",
      "Iteration 329, loss = 10.36318317\n",
      "Iteration 330, loss = 13.34433060\n",
      "Iteration 331, loss = 9.00411048\n",
      "Iteration 332, loss = 8.57547282\n",
      "Iteration 333, loss = 8.01227009\n",
      "Iteration 334, loss = 7.57337437\n",
      "Iteration 335, loss = 7.40193973\n",
      "Iteration 336, loss = 7.94596585\n",
      "Iteration 337, loss = 7.36694243\n",
      "Iteration 338, loss = 6.97696505\n",
      "Iteration 339, loss = 7.14004555\n",
      "Iteration 340, loss = 9.90485059\n",
      "Iteration 341, loss = 8.23180690\n",
      "Iteration 342, loss = 7.78933700\n",
      "Iteration 343, loss = 7.63099571\n",
      "Iteration 344, loss = 7.40103677\n",
      "Iteration 345, loss = 10.76717903\n",
      "Iteration 346, loss = 8.52354129\n",
      "Iteration 347, loss = 8.27185079\n",
      "Iteration 348, loss = 6.95059996\n",
      "Iteration 349, loss = 6.93148167\n",
      "Iteration 350, loss = 6.94007181\n",
      "Iteration 351, loss = 6.80665235\n",
      "Iteration 352, loss = 6.72443497\n",
      "Iteration 353, loss = 6.76182549\n",
      "Iteration 354, loss = 7.13389363\n",
      "Iteration 355, loss = 7.16663565\n",
      "Iteration 356, loss = 7.25847901\n",
      "Iteration 357, loss = 7.28095962\n",
      "Iteration 358, loss = 7.61834264\n",
      "Iteration 359, loss = 7.47702080\n",
      "Iteration 360, loss = 7.86121596\n",
      "Iteration 361, loss = 7.10007329\n",
      "Iteration 362, loss = 7.45331115\n",
      "Iteration 363, loss = 7.11040654\n",
      "Iteration 364, loss = 6.74361940\n",
      "Iteration 365, loss = 7.36334134\n",
      "Iteration 366, loss = 7.23820395\n",
      "Iteration 367, loss = 8.77039256\n",
      "Iteration 368, loss = 7.27443529\n",
      "Iteration 369, loss = 6.66020958\n",
      "Iteration 370, loss = 6.85565669\n",
      "Iteration 371, loss = 6.43186218\n",
      "Iteration 372, loss = 6.79055743\n",
      "Iteration 373, loss = 6.67091003\n",
      "Iteration 374, loss = 8.49836547\n",
      "Iteration 375, loss = 7.08632385\n",
      "Iteration 376, loss = 6.26413322\n",
      "Iteration 377, loss = 7.18168798\n",
      "Iteration 378, loss = 6.67586663\n",
      "Iteration 379, loss = 7.05617594\n",
      "Iteration 380, loss = 6.42722382\n",
      "Iteration 381, loss = 7.27497691\n",
      "Iteration 382, loss = 7.36423836\n",
      "Iteration 383, loss = 6.88897078\n",
      "Iteration 384, loss = 6.59462929\n",
      "Iteration 385, loss = 6.51793178\n",
      "Iteration 386, loss = 6.23244400\n",
      "Iteration 387, loss = 6.22217484\n",
      "Iteration 388, loss = 6.37740547\n",
      "Iteration 389, loss = 6.95614649\n",
      "Iteration 390, loss = 7.83150860\n",
      "Iteration 391, loss = 9.18049292\n",
      "Iteration 392, loss = 7.45563828\n",
      "Iteration 393, loss = 6.55688651\n",
      "Iteration 394, loss = 6.33035411\n",
      "Iteration 395, loss = 6.51631726\n",
      "Iteration 396, loss = 6.45280247\n",
      "Iteration 397, loss = 7.14665963\n",
      "Iteration 398, loss = 6.92190206\n",
      "Iteration 399, loss = 6.29561791\n",
      "Iteration 400, loss = 6.09162119\n",
      "Iteration 401, loss = 6.03314633\n",
      "Iteration 402, loss = 6.80154459\n",
      "Iteration 403, loss = 6.41404912\n",
      "Iteration 404, loss = 6.37616769\n",
      "Iteration 405, loss = 6.04375413\n",
      "Iteration 406, loss = 6.60639492\n",
      "Iteration 407, loss = 6.87821452\n",
      "Iteration 408, loss = 6.11089523\n",
      "Iteration 409, loss = 6.74044613\n",
      "Iteration 410, loss = 6.79489778\n",
      "Iteration 411, loss = 6.78760581\n",
      "Iteration 412, loss = 6.50521471\n",
      "Iteration 413, loss = 6.43131045\n",
      "Iteration 414, loss = 5.98380908\n",
      "Iteration 415, loss = 5.80692723\n",
      "Iteration 416, loss = 6.05642816\n",
      "Iteration 417, loss = 6.09965337\n",
      "Iteration 418, loss = 5.57153522\n",
      "Iteration 419, loss = 5.14853788\n",
      "Iteration 420, loss = 6.04378683\n",
      "Iteration 421, loss = 5.61851271\n",
      "Iteration 422, loss = 5.36480925\n",
      "Iteration 423, loss = 5.58599395\n",
      "Iteration 424, loss = 6.41985011\n",
      "Iteration 425, loss = 5.65890912\n",
      "Iteration 426, loss = 5.99961115\n",
      "Iteration 427, loss = 6.28064515\n",
      "Iteration 428, loss = 5.75389726\n",
      "Iteration 429, loss = 5.65459960\n",
      "Iteration 430, loss = 5.47210200\n",
      "Iteration 431, loss = 5.49230647\n",
      "Iteration 432, loss = 6.74051242\n",
      "Iteration 433, loss = 8.67182428\n",
      "Iteration 434, loss = 6.71895641\n",
      "Iteration 435, loss = 6.57506014\n",
      "Iteration 436, loss = 6.58769581\n",
      "Iteration 437, loss = 5.91714942\n",
      "Iteration 438, loss = 6.62178919\n",
      "Iteration 439, loss = 6.31547970\n",
      "Iteration 440, loss = 5.99865086\n",
      "Iteration 441, loss = 5.09054740\n",
      "Iteration 442, loss = 4.92270182\n",
      "Iteration 443, loss = 5.13107115\n",
      "Iteration 444, loss = 4.98613888\n",
      "Iteration 445, loss = 5.52410668\n",
      "Iteration 446, loss = 6.68019274\n",
      "Iteration 447, loss = 6.84467216\n",
      "Iteration 448, loss = 6.38932530\n",
      "Iteration 449, loss = 5.28034486\n",
      "Iteration 450, loss = 4.80585799\n",
      "Iteration 451, loss = 4.79289226\n",
      "Iteration 452, loss = 4.99260728\n",
      "Iteration 453, loss = 6.92154358\n",
      "Iteration 454, loss = 6.22906865\n",
      "Iteration 455, loss = 7.48731549\n",
      "Iteration 456, loss = 7.28057471\n",
      "Iteration 457, loss = 6.30472432\n",
      "Iteration 458, loss = 5.26678226\n",
      "Iteration 459, loss = 5.73252195\n",
      "Iteration 460, loss = 7.22379265\n",
      "Iteration 461, loss = 6.20361339\n",
      "Iteration 462, loss = 5.38634161\n",
      "Iteration 463, loss = 4.47787840\n",
      "Iteration 464, loss = 4.49524865\n",
      "Iteration 465, loss = 5.01414855\n",
      "Iteration 466, loss = 4.49005652\n",
      "Iteration 467, loss = 5.07386119\n",
      "Iteration 468, loss = 6.06477013\n",
      "Iteration 469, loss = 5.28380461\n",
      "Iteration 470, loss = 4.60314081\n",
      "Iteration 471, loss = 4.31689893\n",
      "Iteration 472, loss = 4.50178836\n",
      "Iteration 473, loss = 4.39655181\n",
      "Iteration 474, loss = 4.41858314\n",
      "Iteration 475, loss = 4.47461771\n",
      "Iteration 476, loss = 5.64251997\n",
      "Iteration 477, loss = 4.72603837\n",
      "Iteration 478, loss = 4.56735414\n",
      "Iteration 479, loss = 4.79649330\n",
      "Iteration 480, loss = 5.49252433\n",
      "Iteration 481, loss = 5.22977542\n",
      "Iteration 482, loss = 5.47107394\n",
      "Iteration 483, loss = 5.67805341\n",
      "Iteration 484, loss = 5.97020446\n",
      "Iteration 485, loss = 4.98766002\n",
      "Iteration 486, loss = 4.69785986\n",
      "Iteration 487, loss = 4.36498529\n",
      "Iteration 488, loss = 5.66951504\n",
      "Iteration 489, loss = 5.34519396\n",
      "Iteration 490, loss = 4.96927029\n",
      "Iteration 491, loss = 4.37272215\n",
      "Iteration 492, loss = 4.60483463\n",
      "Iteration 493, loss = 4.94272549\n",
      "Iteration 494, loss = 6.68104905\n",
      "Iteration 495, loss = 5.23449034\n",
      "Iteration 496, loss = 6.32083924\n",
      "Iteration 497, loss = 6.20364355\n",
      "Training loss did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "positive registered most important features:\n",
      "                importance\n",
      "registered, +1    0.679378\n",
      "hour              0.082500\n",
      "dayofweek         0.060149\n",
      "humidity          0.022714\n",
      "registered, +2    0.019652\n",
      "weather           0.017930\n",
      "casual, +1        0.014769\n",
      "casual, +2        0.010845\n",
      "weather, +1       0.009345\n",
      "workingday        0.008576\n",
      "humidity, +1      0.008375\n",
      "dayofyear         0.006653\n",
      "humidity, +2      0.006206\n",
      "atemp             0.006192\n",
      "workingday, +2    0.005962\n",
      "windspeed, +1     0.005753\n",
      "windspeed, +2     0.005255\n",
      "windspeed         0.005093\n",
      "temp              0.004617\n",
      "workingday, +1    0.003170\n",
      "atemp, +2         0.003021\n",
      "atemp, +1         0.002883\n",
      "temp, +2          0.002874\n",
      "temp, +1          0.002777\n",
      "weekofyear        0.002523\n",
      "weather, +2       0.000911\n",
      "holiday           0.000863\n",
      "season            0.000445\n",
      "holiday, +2       0.000415\n",
      "holiday, +1       0.000154\n",
      "\n",
      "Iteration 1, loss = 19491.53905441\n",
      "Iteration 2, loss = 6719.51765010\n",
      "Iteration 3, loss = 3470.30248808\n",
      "Iteration 4, loss = 2765.02453771\n",
      "Iteration 5, loss = 2402.90477609\n",
      "Iteration 6, loss = 2161.19315789\n",
      "Iteration 7, loss = 1969.24805048\n",
      "Iteration 8, loss = 1828.72006703\n",
      "Iteration 9, loss = 1714.54355702\n",
      "Iteration 10, loss = 1633.15008332\n",
      "Iteration 11, loss = 1555.42673874\n",
      "Iteration 12, loss = 1491.84689106\n",
      "Iteration 13, loss = 1437.20301250\n",
      "Iteration 14, loss = 1372.83240730\n",
      "Iteration 15, loss = 1331.57433065\n",
      "Iteration 16, loss = 1284.15871655\n",
      "Iteration 17, loss = 1252.31714254\n",
      "Iteration 18, loss = 1195.15038019\n",
      "Iteration 19, loss = 1164.19552539\n",
      "Iteration 20, loss = 1122.21987027\n",
      "Iteration 21, loss = 1088.26315017\n",
      "Iteration 22, loss = 1057.21519202\n",
      "Iteration 23, loss = 1020.76829226\n",
      "Iteration 24, loss = 992.43372943\n",
      "Iteration 25, loss = 959.20863747\n",
      "Iteration 26, loss = 925.89231773\n",
      "Iteration 27, loss = 888.46317154\n",
      "Iteration 28, loss = 859.45666262\n",
      "Iteration 29, loss = 844.15028183\n",
      "Iteration 30, loss = 812.35443918\n",
      "Iteration 31, loss = 776.11030675\n",
      "Iteration 32, loss = 756.06159916\n",
      "Iteration 33, loss = 720.39407235\n",
      "Iteration 34, loss = 696.45125992\n",
      "Iteration 35, loss = 671.79870333\n",
      "Iteration 36, loss = 648.53284968\n",
      "Iteration 37, loss = 624.80867062\n",
      "Iteration 38, loss = 602.24704387\n",
      "Iteration 39, loss = 585.10740169\n",
      "Iteration 40, loss = 557.24221415\n",
      "Iteration 41, loss = 538.33638188\n",
      "Iteration 42, loss = 516.47328052\n",
      "Iteration 43, loss = 515.95510680\n",
      "Iteration 44, loss = 500.52934241\n",
      "Iteration 45, loss = 471.27686089\n",
      "Iteration 46, loss = 446.72140714\n",
      "Iteration 47, loss = 430.93240375\n",
      "Iteration 48, loss = 430.31577541\n",
      "Iteration 49, loss = 417.91133031\n",
      "Iteration 50, loss = 403.79054541\n",
      "Iteration 51, loss = 389.88298534\n",
      "Iteration 52, loss = 376.44743010\n",
      "Iteration 53, loss = 363.94255753\n",
      "Iteration 54, loss = 353.67389680\n",
      "Iteration 55, loss = 362.00892756\n",
      "Iteration 56, loss = 345.06867507\n",
      "Iteration 57, loss = 343.47536597\n",
      "Iteration 58, loss = 327.86499039\n",
      "Iteration 59, loss = 329.77640514\n",
      "Iteration 60, loss = 318.99508745\n",
      "Iteration 61, loss = 301.25547643\n",
      "Iteration 62, loss = 299.76256115\n",
      "Iteration 63, loss = 298.20632818\n",
      "Iteration 64, loss = 286.93817985\n",
      "Iteration 65, loss = 291.40086982\n",
      "Iteration 66, loss = 279.63058445\n",
      "Iteration 67, loss = 287.78157652\n",
      "Iteration 68, loss = 269.69830233\n",
      "Iteration 69, loss = 257.97722715\n",
      "Iteration 70, loss = 274.26343368\n",
      "Iteration 71, loss = 254.99540949\n",
      "Iteration 72, loss = 248.82849675\n",
      "Iteration 73, loss = 251.26816442\n",
      "Iteration 74, loss = 242.08536822\n",
      "Iteration 75, loss = 237.01957208\n",
      "Iteration 76, loss = 234.81495162\n",
      "Iteration 77, loss = 229.50974645\n",
      "Iteration 78, loss = 230.10464008\n",
      "Iteration 79, loss = 222.96456569\n",
      "Iteration 80, loss = 224.42789755\n",
      "Iteration 81, loss = 223.76328626\n",
      "Iteration 82, loss = 219.92310450\n",
      "Iteration 83, loss = 213.98636870\n",
      "Iteration 84, loss = 207.12615634\n",
      "Iteration 85, loss = 205.64647929\n",
      "Iteration 86, loss = 212.90146277\n",
      "Iteration 87, loss = 202.09534162\n",
      "Iteration 88, loss = 200.12061666\n",
      "Iteration 89, loss = 202.52620180\n",
      "Iteration 90, loss = 189.05583773\n",
      "Iteration 91, loss = 191.01568558\n",
      "Iteration 92, loss = 187.67778112\n",
      "Iteration 93, loss = 189.85135064\n",
      "Iteration 94, loss = 193.57143134\n",
      "Iteration 95, loss = 185.68132519\n",
      "Iteration 96, loss = 182.70625027\n",
      "Iteration 97, loss = 179.49602744\n",
      "Iteration 98, loss = 184.31749911\n",
      "Iteration 99, loss = 179.35471988\n",
      "Iteration 100, loss = 178.38338256\n",
      "Iteration 101, loss = 176.35551806\n",
      "Iteration 102, loss = 167.26794262\n",
      "Iteration 103, loss = 169.05175963\n",
      "Iteration 104, loss = 171.31618002\n",
      "Iteration 105, loss = 175.49352034\n",
      "Iteration 106, loss = 173.44370722\n",
      "Iteration 107, loss = 166.71730318\n",
      "Iteration 108, loss = 168.59277411\n",
      "Iteration 109, loss = 155.70980295\n",
      "Iteration 110, loss = 160.24196268\n",
      "Iteration 111, loss = 159.39078685\n",
      "Iteration 112, loss = 156.81226494\n",
      "Iteration 113, loss = 149.30604535\n",
      "Iteration 114, loss = 154.76034874\n",
      "Iteration 115, loss = 147.19805255\n",
      "Iteration 116, loss = 157.76658247\n",
      "Iteration 117, loss = 150.93362764\n",
      "Iteration 118, loss = 152.79441743\n",
      "Iteration 119, loss = 147.91845753\n",
      "Iteration 120, loss = 149.33184506\n",
      "Iteration 121, loss = 146.47839255\n",
      "Iteration 122, loss = 141.77608469\n",
      "Iteration 123, loss = 146.65447255\n",
      "Iteration 124, loss = 140.83067092\n",
      "Iteration 125, loss = 140.35441207\n",
      "Iteration 126, loss = 133.94190466\n",
      "Iteration 127, loss = 140.50520924\n",
      "Iteration 128, loss = 136.91523806\n",
      "Iteration 129, loss = 131.95740153\n",
      "Iteration 130, loss = 129.70054662\n",
      "Iteration 131, loss = 131.78178940\n",
      "Iteration 132, loss = 130.72337975\n",
      "Iteration 133, loss = 137.02498526\n",
      "Iteration 134, loss = 127.38272153\n",
      "Iteration 135, loss = 124.21664184\n",
      "Iteration 136, loss = 127.23623358\n",
      "Iteration 137, loss = 131.38405323\n",
      "Iteration 138, loss = 132.57641150\n",
      "Iteration 139, loss = 123.17599456\n",
      "Iteration 140, loss = 121.66621492\n",
      "Iteration 141, loss = 117.97845635\n",
      "Iteration 142, loss = 118.69557743\n",
      "Iteration 143, loss = 123.37497377\n",
      "Iteration 144, loss = 124.75353932\n",
      "Iteration 145, loss = 124.72062953\n",
      "Iteration 146, loss = 122.95977377\n",
      "Iteration 147, loss = 119.19643308\n",
      "Iteration 148, loss = 112.54331937\n",
      "Iteration 149, loss = 111.52685705\n",
      "Iteration 150, loss = 110.23368981\n",
      "Iteration 151, loss = 113.90764888\n",
      "Iteration 152, loss = 111.95011761\n",
      "Iteration 153, loss = 112.39509199\n",
      "Iteration 154, loss = 114.05969524\n",
      "Iteration 155, loss = 121.23313264\n",
      "Iteration 156, loss = 125.68344724\n",
      "Iteration 157, loss = 117.95902424\n",
      "Iteration 158, loss = 109.22195450\n",
      "Iteration 159, loss = 106.65269059\n",
      "Iteration 160, loss = 110.61909240\n",
      "Iteration 161, loss = 112.11897546\n",
      "Iteration 162, loss = 110.41854893\n",
      "Iteration 163, loss = 110.89974562\n",
      "Iteration 164, loss = 105.77593774\n",
      "Iteration 165, loss = 109.91251639\n",
      "Iteration 166, loss = 102.01644875\n",
      "Iteration 167, loss = 102.65436968\n",
      "Iteration 168, loss = 98.18311619\n",
      "Iteration 169, loss = 101.39421180\n",
      "Iteration 170, loss = 103.15002495\n",
      "Iteration 171, loss = 100.65933755\n",
      "Iteration 172, loss = 100.74244760\n",
      "Iteration 173, loss = 101.05998676\n",
      "Iteration 174, loss = 97.30566633\n",
      "Iteration 175, loss = 98.30334876\n",
      "Iteration 176, loss = 101.13671963\n",
      "Iteration 177, loss = 98.76577356\n",
      "Iteration 178, loss = 95.05281287\n",
      "Iteration 179, loss = 94.71884313\n",
      "Iteration 180, loss = 95.22685011\n",
      "Iteration 181, loss = 93.57594530\n",
      "Iteration 182, loss = 97.99461061\n",
      "Iteration 183, loss = 97.59962344\n",
      "Iteration 184, loss = 97.58914694\n",
      "Iteration 185, loss = 100.05612220\n",
      "Iteration 186, loss = 87.88837169\n",
      "Iteration 187, loss = 88.46227889\n",
      "Iteration 188, loss = 90.77599582\n",
      "Iteration 189, loss = 95.03815337\n",
      "Iteration 190, loss = 91.00774880\n",
      "Iteration 191, loss = 89.51065862\n",
      "Iteration 192, loss = 94.81314606\n",
      "Iteration 193, loss = 92.77157748\n",
      "Iteration 194, loss = 88.39359542\n",
      "Iteration 195, loss = 83.77799927\n",
      "Iteration 196, loss = 82.62858074\n",
      "Iteration 197, loss = 90.12294231\n",
      "Iteration 198, loss = 82.71824238\n",
      "Iteration 199, loss = 83.93735676\n",
      "Iteration 200, loss = 84.44069811\n",
      "Iteration 201, loss = 83.34799424\n",
      "Iteration 202, loss = 82.27157024\n",
      "Iteration 203, loss = 83.68830182\n",
      "Iteration 204, loss = 80.64488292\n",
      "Iteration 205, loss = 81.94697914\n",
      "Iteration 206, loss = 86.33035023\n",
      "Iteration 207, loss = 78.70956487\n",
      "Iteration 208, loss = 81.56523541\n",
      "Iteration 209, loss = 77.96546398\n",
      "Iteration 210, loss = 76.10669641\n",
      "Iteration 211, loss = 77.88440171\n",
      "Iteration 212, loss = 81.82068066\n",
      "Iteration 213, loss = 81.14473962\n",
      "Iteration 214, loss = 79.89008954\n",
      "Iteration 215, loss = 84.29427655\n",
      "Iteration 216, loss = 77.36035374\n",
      "Iteration 217, loss = 74.34745216\n",
      "Iteration 218, loss = 78.54522391\n",
      "Iteration 219, loss = 74.84868241\n",
      "Iteration 220, loss = 75.13293008\n",
      "Iteration 221, loss = 95.50897063\n",
      "Iteration 222, loss = 74.95806004\n",
      "Iteration 223, loss = 76.70290074\n",
      "Iteration 224, loss = 73.69172927\n",
      "Iteration 225, loss = 80.85813097\n",
      "Iteration 226, loss = 75.34018019\n",
      "Iteration 227, loss = 73.88036801\n",
      "Iteration 228, loss = 76.62269033\n",
      "Iteration 229, loss = 73.50096260\n",
      "Iteration 230, loss = 71.33714917\n",
      "Iteration 231, loss = 73.47698023\n",
      "Iteration 232, loss = 68.97907216\n",
      "Iteration 233, loss = 69.09627880\n",
      "Iteration 234, loss = 70.76325527\n",
      "Iteration 235, loss = 77.98558377\n",
      "Iteration 236, loss = 77.08391517\n",
      "Iteration 237, loss = 71.20735241\n",
      "Iteration 238, loss = 73.28371275\n",
      "Iteration 239, loss = 70.78255223\n",
      "Iteration 240, loss = 68.46194725\n",
      "Iteration 241, loss = 67.82068531\n",
      "Iteration 242, loss = 68.54634273\n",
      "Iteration 243, loss = 63.44386439\n",
      "Iteration 244, loss = 67.59030530\n",
      "Iteration 245, loss = 70.37178572\n",
      "Iteration 246, loss = 67.23083014\n",
      "Iteration 247, loss = 63.29698728\n",
      "Iteration 248, loss = 62.45990378\n",
      "Iteration 249, loss = 64.41897343\n",
      "Iteration 250, loss = 68.87179232\n",
      "Iteration 251, loss = 66.92687391\n",
      "Iteration 252, loss = 66.95173221\n",
      "Iteration 253, loss = 60.04794479\n",
      "Iteration 254, loss = 61.65376420\n",
      "Iteration 255, loss = 64.56888301\n",
      "Iteration 256, loss = 61.51315204\n",
      "Iteration 257, loss = 63.45237825\n",
      "Iteration 258, loss = 67.93808765\n",
      "Iteration 259, loss = 66.00605583\n",
      "Iteration 260, loss = 67.58221135\n",
      "Iteration 261, loss = 59.94177906\n",
      "Iteration 262, loss = 61.85570269\n",
      "Iteration 263, loss = 71.09404953\n",
      "Iteration 264, loss = 63.20205913\n",
      "Iteration 265, loss = 64.95778059\n",
      "Iteration 266, loss = 62.65091584\n",
      "Iteration 267, loss = 55.66419793\n",
      "Iteration 268, loss = 60.32666510\n",
      "Iteration 269, loss = 60.81060441\n",
      "Iteration 270, loss = 58.56309165\n",
      "Iteration 271, loss = 66.66438092\n",
      "Iteration 272, loss = 57.97035566\n",
      "Iteration 273, loss = 54.75393834\n",
      "Iteration 274, loss = 57.61994310\n",
      "Iteration 275, loss = 69.67830305\n",
      "Iteration 276, loss = 68.82249901\n",
      "Iteration 277, loss = 65.57284109\n",
      "Iteration 278, loss = 67.15128666\n",
      "Iteration 279, loss = 57.55117594\n",
      "Iteration 280, loss = 55.61809925\n",
      "Iteration 281, loss = 53.49618382\n",
      "Iteration 282, loss = 56.75199147\n",
      "Iteration 283, loss = 54.21492629\n",
      "Iteration 284, loss = 56.23101808\n",
      "Iteration 285, loss = 58.33333196\n",
      "Iteration 286, loss = 57.75977667\n",
      "Iteration 287, loss = 61.15373842\n",
      "Iteration 288, loss = 56.36398322\n",
      "Iteration 289, loss = 55.62993209\n",
      "Iteration 290, loss = 56.15249363\n",
      "Iteration 291, loss = 53.11273949\n",
      "Iteration 292, loss = 56.52146218\n",
      "Iteration 293, loss = 56.90856357\n",
      "Iteration 294, loss = 53.84341296\n",
      "Iteration 295, loss = 56.67793942\n",
      "Iteration 296, loss = 57.63167006\n",
      "Iteration 297, loss = 56.82153196\n",
      "Iteration 298, loss = 74.52078853\n",
      "Iteration 299, loss = 53.63344223\n",
      "Iteration 300, loss = 55.86315265\n",
      "Iteration 301, loss = 59.90707095\n",
      "Iteration 302, loss = 49.97108522\n",
      "Iteration 303, loss = 49.91561642\n",
      "Iteration 304, loss = 50.69225728\n",
      "Iteration 305, loss = 53.22513176\n",
      "Iteration 306, loss = 57.74023790\n",
      "Iteration 307, loss = 55.93021415\n",
      "Iteration 308, loss = 50.11735868\n",
      "Iteration 309, loss = 49.45809304\n",
      "Iteration 310, loss = 50.46585153\n",
      "Iteration 311, loss = 50.27525736\n",
      "Iteration 312, loss = 50.30045427\n",
      "Iteration 313, loss = 47.96285851\n",
      "Iteration 314, loss = 48.32289738\n",
      "Iteration 315, loss = 48.44606954\n",
      "Iteration 316, loss = 46.87231848\n",
      "Iteration 317, loss = 46.73742139\n",
      "Iteration 318, loss = 46.96432382\n",
      "Iteration 319, loss = 48.47028820\n",
      "Iteration 320, loss = 49.03343278\n",
      "Iteration 321, loss = 48.10666593\n",
      "Iteration 322, loss = 52.71653069\n",
      "Iteration 323, loss = 50.57667151\n",
      "Iteration 324, loss = 52.58497828\n",
      "Iteration 325, loss = 57.15298994\n",
      "Iteration 326, loss = 49.54247430\n",
      "Iteration 327, loss = 48.78698128\n",
      "Iteration 328, loss = 51.72822191\n",
      "Iteration 329, loss = 48.04374657\n",
      "Iteration 330, loss = 53.98759372\n",
      "Iteration 331, loss = 50.93944923\n",
      "Iteration 332, loss = 51.09135732\n",
      "Iteration 333, loss = 47.09005204\n",
      "Iteration 334, loss = 43.47005804\n",
      "Iteration 335, loss = 45.43115198\n",
      "Iteration 336, loss = 46.74356067\n",
      "Iteration 337, loss = 50.84192280\n",
      "Iteration 338, loss = 53.47650609\n",
      "Iteration 339, loss = 52.12149632\n",
      "Iteration 340, loss = 44.82667105\n",
      "Iteration 341, loss = 46.27065636\n",
      "Iteration 342, loss = 45.43391156\n",
      "Iteration 343, loss = 45.31569586\n",
      "Iteration 344, loss = 53.13687637\n",
      "Iteration 345, loss = 59.71208449\n",
      "Iteration 346, loss = 46.81610153\n",
      "Iteration 347, loss = 44.73721531\n",
      "Iteration 348, loss = 44.74384082\n",
      "Iteration 349, loss = 41.46963512\n",
      "Iteration 350, loss = 44.07725637\n",
      "Iteration 351, loss = 41.58758833\n",
      "Iteration 352, loss = 40.19512134\n",
      "Iteration 353, loss = 43.13481462\n",
      "Iteration 354, loss = 41.84541646\n",
      "Iteration 355, loss = 40.70165782\n",
      "Iteration 356, loss = 42.33451564\n",
      "Iteration 357, loss = 41.08473454\n",
      "Iteration 358, loss = 45.52257507\n",
      "Iteration 359, loss = 48.53424450\n",
      "Iteration 360, loss = 43.95970848\n",
      "Iteration 361, loss = 45.23442771\n",
      "Iteration 362, loss = 43.50188974\n",
      "Iteration 363, loss = 42.56439960\n",
      "Iteration 364, loss = 43.60161292\n",
      "Iteration 365, loss = 39.37580479\n",
      "Iteration 366, loss = 47.49703262\n",
      "Iteration 367, loss = 43.70580727\n",
      "Iteration 368, loss = 44.39339338\n",
      "Iteration 369, loss = 40.97866749\n",
      "Iteration 370, loss = 40.09542708\n",
      "Iteration 371, loss = 40.17716079\n",
      "Iteration 372, loss = 42.03446106\n",
      "Iteration 373, loss = 46.14772488\n",
      "Iteration 374, loss = 45.30708421\n",
      "Iteration 375, loss = 42.90827463\n",
      "Iteration 376, loss = 39.77785776\n",
      "Iteration 377, loss = 42.09228448\n",
      "Iteration 378, loss = 39.12011785\n",
      "Iteration 379, loss = 37.43474399\n",
      "Iteration 380, loss = 37.26854293\n",
      "Iteration 381, loss = 37.91905631\n",
      "Iteration 382, loss = 37.73876216\n",
      "Iteration 383, loss = 40.03015647\n",
      "Iteration 384, loss = 40.57736157\n",
      "Iteration 385, loss = 40.78064548\n",
      "Iteration 386, loss = 41.46027327\n",
      "Iteration 387, loss = 39.61158875\n",
      "Iteration 388, loss = 39.05003628\n",
      "Iteration 389, loss = 47.20499240\n",
      "Iteration 390, loss = 40.84077083\n",
      "Iteration 391, loss = 37.69505200\n",
      "Iteration 392, loss = 36.94716740\n",
      "Iteration 393, loss = 37.72718437\n",
      "Iteration 394, loss = 40.89503163\n",
      "Iteration 395, loss = 36.11376345\n",
      "Iteration 396, loss = 35.10151493\n",
      "Iteration 397, loss = 40.80594772\n",
      "Iteration 398, loss = 39.12934728\n",
      "Iteration 399, loss = 36.19584312\n",
      "Iteration 400, loss = 35.89010007\n",
      "Iteration 401, loss = 36.93446323\n",
      "Iteration 402, loss = 39.21634607\n",
      "Iteration 403, loss = 46.24748321\n",
      "Iteration 404, loss = 38.07619599\n",
      "Iteration 405, loss = 38.91391696\n",
      "Iteration 406, loss = 35.82155571\n",
      "Iteration 407, loss = 39.51703170\n",
      "Iteration 408, loss = 39.81037250\n",
      "Iteration 409, loss = 38.50679334\n",
      "Iteration 410, loss = 36.52559658\n",
      "Iteration 411, loss = 34.50702111\n",
      "Iteration 412, loss = 37.22104622\n",
      "Iteration 413, loss = 38.18782157\n",
      "Iteration 414, loss = 35.12249209\n",
      "Iteration 415, loss = 32.57238235\n",
      "Iteration 416, loss = 32.19987639\n",
      "Iteration 417, loss = 34.55782235\n",
      "Iteration 418, loss = 32.79448369\n",
      "Iteration 419, loss = 33.88413955\n",
      "Iteration 420, loss = 38.25723428\n",
      "Iteration 421, loss = 32.80584014\n",
      "Iteration 422, loss = 35.17362633\n",
      "Iteration 423, loss = 38.22153031\n",
      "Iteration 424, loss = 48.93659233\n",
      "Iteration 425, loss = 35.53116091\n",
      "Iteration 426, loss = 37.65396770\n",
      "Iteration 427, loss = 50.39051576\n",
      "Iteration 428, loss = 47.47726219\n",
      "Iteration 429, loss = 43.65687562\n",
      "Iteration 430, loss = 43.05053569\n",
      "Iteration 431, loss = 37.74884713\n",
      "Iteration 432, loss = 32.04193663\n",
      "Iteration 433, loss = 31.69663816\n",
      "Iteration 434, loss = 35.08231009\n",
      "Iteration 435, loss = 33.33725379\n",
      "Iteration 436, loss = 33.35253443\n",
      "Iteration 437, loss = 31.83434410\n",
      "Iteration 438, loss = 32.00443644\n",
      "Iteration 439, loss = 31.01161227\n",
      "Iteration 440, loss = 35.50919605\n",
      "Iteration 441, loss = 36.70148820\n",
      "Iteration 442, loss = 32.95099537\n",
      "Iteration 443, loss = 31.71494203\n",
      "Iteration 444, loss = 31.69670020\n",
      "Iteration 445, loss = 29.86440276\n",
      "Iteration 446, loss = 31.30046026\n",
      "Iteration 447, loss = 31.98971298\n",
      "Iteration 448, loss = 34.41861543\n",
      "Iteration 449, loss = 34.08201563\n",
      "Iteration 450, loss = 31.51936855\n",
      "Iteration 451, loss = 31.56263193\n",
      "Iteration 452, loss = 34.18798071\n",
      "Iteration 453, loss = 31.16022388\n",
      "Iteration 454, loss = 33.42788741\n",
      "Iteration 455, loss = 36.90660502\n",
      "Iteration 456, loss = 35.36598172\n",
      "Iteration 457, loss = 33.66085557\n",
      "Iteration 458, loss = 31.02159702\n",
      "Iteration 459, loss = 32.20800366\n",
      "Iteration 460, loss = 32.61019036\n",
      "Iteration 461, loss = 32.60602327\n",
      "Iteration 462, loss = 28.65597931\n",
      "Iteration 463, loss = 30.42892146\n",
      "Iteration 464, loss = 30.60932741\n",
      "Iteration 465, loss = 36.16642643\n",
      "Iteration 466, loss = 33.76023058\n",
      "Iteration 467, loss = 30.29045248\n",
      "Iteration 468, loss = 29.84644123\n",
      "Iteration 469, loss = 28.67951540\n",
      "Iteration 470, loss = 29.03373503\n",
      "Iteration 471, loss = 30.36847452\n",
      "Iteration 472, loss = 30.36246087\n",
      "Iteration 473, loss = 28.17581829\n",
      "Iteration 474, loss = 28.56547712\n",
      "Iteration 475, loss = 28.08019726\n",
      "Iteration 476, loss = 29.14381675\n",
      "Iteration 477, loss = 30.03963424\n",
      "Iteration 478, loss = 32.72642231\n",
      "Iteration 479, loss = 31.38618885\n",
      "Iteration 480, loss = 35.01698625\n",
      "Iteration 481, loss = 32.10410319\n",
      "Iteration 482, loss = 33.20165394\n",
      "Iteration 483, loss = 32.97931902\n",
      "Iteration 484, loss = 31.38644979\n",
      "Iteration 485, loss = 27.73000987\n",
      "Iteration 486, loss = 26.62114793\n",
      "Iteration 487, loss = 33.42819113\n",
      "Iteration 488, loss = 30.67654312\n",
      "Iteration 489, loss = 30.20533476\n",
      "Iteration 490, loss = 26.27237922\n",
      "Iteration 491, loss = 27.88706465\n",
      "Iteration 492, loss = 30.88535223\n",
      "Iteration 493, loss = 37.19943557\n",
      "Iteration 494, loss = 28.30752928\n",
      "Iteration 495, loss = 32.98987992\n",
      "Iteration 496, loss = 32.61071877\n",
      "Iteration 497, loss = 28.61189101\n",
      "Iteration 498, loss = 29.45322419\n",
      "Iteration 499, loss = 59.74203934\n",
      "Iteration 500, loss = 45.99016908\n",
      "Iteration 501, loss = 30.95788293\n",
      "Iteration 502, loss = 27.43101163\n",
      "Iteration 503, loss = 26.56565903\n",
      "Iteration 504, loss = 27.28363278\n",
      "Iteration 505, loss = 24.50394213\n",
      "Iteration 506, loss = 26.21258783\n",
      "Iteration 507, loss = 29.32829597\n",
      "Iteration 508, loss = 26.29534298\n",
      "Iteration 509, loss = 24.13081655\n",
      "Iteration 510, loss = 25.04146078\n",
      "Iteration 511, loss = 25.69479587\n",
      "Iteration 512, loss = 26.58568026\n",
      "Iteration 513, loss = 25.15036786\n",
      "Iteration 514, loss = 24.06148389\n",
      "Iteration 515, loss = 25.81733127\n",
      "Iteration 516, loss = 27.78227030\n",
      "Iteration 517, loss = 31.64917932\n",
      "Iteration 518, loss = 47.42968572\n",
      "Iteration 519, loss = 32.85758374\n",
      "Iteration 520, loss = 30.44493103\n",
      "Iteration 521, loss = 28.85629286\n",
      "Iteration 522, loss = 24.96043926\n",
      "Iteration 523, loss = 25.69727130\n",
      "Iteration 524, loss = 25.58733627\n",
      "Iteration 525, loss = 25.32630671\n",
      "Iteration 526, loss = 23.07226956\n",
      "Iteration 527, loss = 25.77605471\n",
      "Iteration 528, loss = 27.45261904\n",
      "Iteration 529, loss = 24.78528807\n",
      "Iteration 530, loss = 25.66499814\n",
      "Iteration 531, loss = 26.23390682\n",
      "Iteration 532, loss = 27.38622343\n",
      "Iteration 533, loss = 31.51634878\n",
      "Iteration 534, loss = 26.27739632\n",
      "Iteration 535, loss = 27.55240740\n",
      "Iteration 536, loss = 26.22938625\n",
      "Iteration 537, loss = 26.50176971\n",
      "Iteration 538, loss = 26.06763302\n",
      "Iteration 539, loss = 28.13137517\n",
      "Iteration 540, loss = 25.33661193\n",
      "Iteration 541, loss = 26.95814938\n",
      "Iteration 542, loss = 27.13310946\n",
      "Iteration 543, loss = 26.36896520\n",
      "Iteration 544, loss = 24.96507159\n",
      "Iteration 545, loss = 35.53008502\n",
      "Iteration 546, loss = 37.17703744\n",
      "Iteration 547, loss = 26.83912422\n",
      "Iteration 548, loss = 24.17412318\n",
      "Iteration 549, loss = 22.79799434\n",
      "Iteration 550, loss = 24.21146485\n",
      "Iteration 551, loss = 28.72627916\n",
      "Iteration 552, loss = 26.59004088\n",
      "Iteration 553, loss = 24.37996842\n",
      "Iteration 554, loss = 25.14453438\n",
      "Iteration 555, loss = 25.96379411\n",
      "Iteration 556, loss = 25.89834424\n",
      "Iteration 557, loss = 24.06629160\n",
      "Iteration 558, loss = 24.82484444\n",
      "Iteration 559, loss = 24.23724979\n",
      "Iteration 560, loss = 31.20341224\n",
      "Iteration 561, loss = 33.07699870\n",
      "Iteration 562, loss = 26.58622051\n",
      "Iteration 563, loss = 24.22227620\n",
      "Iteration 564, loss = 24.68043962\n",
      "Iteration 565, loss = 25.56346107\n",
      "Iteration 566, loss = 27.40943919\n",
      "Iteration 567, loss = 21.85906179\n",
      "Iteration 568, loss = 21.04153902\n",
      "Iteration 569, loss = 21.93239416\n",
      "Iteration 570, loss = 24.28221351\n",
      "Iteration 571, loss = 22.35914826\n",
      "Iteration 572, loss = 28.14160720\n",
      "Iteration 573, loss = 30.78978600\n",
      "Iteration 574, loss = 30.46186087\n",
      "Iteration 575, loss = 31.43970061\n",
      "Iteration 576, loss = 25.47077915\n",
      "Iteration 577, loss = 23.01938008\n",
      "Iteration 578, loss = 23.76228011\n",
      "Iteration 579, loss = 24.77714277\n",
      "Iteration 580, loss = 24.50605456\n",
      "Iteration 581, loss = 22.72340780\n",
      "Iteration 582, loss = 21.10122854\n",
      "Iteration 583, loss = 22.68385361\n",
      "Iteration 584, loss = 23.04870045\n",
      "Iteration 585, loss = 22.71964834\n",
      "Iteration 586, loss = 25.49543278\n",
      "Iteration 587, loss = 25.08708019\n",
      "Iteration 588, loss = 33.78459419\n",
      "Iteration 589, loss = 24.92814680\n",
      "Iteration 590, loss = 22.78061653\n",
      "Iteration 591, loss = 20.50942166\n",
      "Iteration 592, loss = 21.07387992\n",
      "Iteration 593, loss = 21.46622159\n",
      "Iteration 594, loss = 23.89225034\n",
      "Iteration 595, loss = 24.76210529\n",
      "Iteration 596, loss = 22.73422819\n",
      "Iteration 597, loss = 25.69921674\n",
      "Iteration 598, loss = 22.29161188\n",
      "Iteration 599, loss = 22.98571329\n",
      "Iteration 600, loss = 22.68153862\n",
      "Iteration 601, loss = 25.13926040\n",
      "Iteration 602, loss = 24.75871171\n",
      "Iteration 603, loss = 20.95677736\n",
      "Iteration 604, loss = 21.73507847\n",
      "Iteration 605, loss = 21.72474758\n",
      "Iteration 606, loss = 21.19917067\n",
      "Iteration 607, loss = 20.50438262\n",
      "Iteration 608, loss = 21.69923264\n",
      "Iteration 609, loss = 22.46163260\n",
      "Iteration 610, loss = 22.93831261\n",
      "Iteration 611, loss = 26.43511945\n",
      "Iteration 612, loss = 32.85863734\n",
      "Iteration 613, loss = 25.15370147\n",
      "Iteration 614, loss = 30.38169188\n",
      "Iteration 615, loss = 28.99893227\n",
      "Iteration 616, loss = 20.80527152\n",
      "Iteration 617, loss = 27.41694398\n",
      "Iteration 618, loss = 25.28220976\n",
      "Iteration 619, loss = 23.51834498\n",
      "Iteration 620, loss = 22.89238465\n",
      "Iteration 621, loss = 22.24097471\n",
      "Iteration 622, loss = 21.42202358\n",
      "Iteration 623, loss = 19.58222914\n",
      "Iteration 624, loss = 18.79395274\n",
      "Iteration 625, loss = 23.18286429\n",
      "Iteration 626, loss = 24.25083413\n",
      "Iteration 627, loss = 38.68033848\n",
      "Iteration 628, loss = 22.91327747\n",
      "Iteration 629, loss = 22.06826774\n",
      "Iteration 630, loss = 21.22294947\n",
      "Iteration 631, loss = 18.56230113\n",
      "Iteration 632, loss = 19.23696659\n",
      "Iteration 633, loss = 18.97228371\n",
      "Iteration 634, loss = 18.26270504\n",
      "Iteration 635, loss = 19.93474830\n",
      "Iteration 636, loss = 26.95123485\n",
      "Iteration 637, loss = 22.64253959\n",
      "Iteration 638, loss = 20.12876652\n",
      "Iteration 639, loss = 21.14469994\n",
      "Iteration 640, loss = 23.42045053\n",
      "Iteration 641, loss = 21.74651912\n",
      "Iteration 642, loss = 20.15372997\n",
      "Iteration 643, loss = 19.91965167\n",
      "Iteration 644, loss = 20.10179748\n",
      "Iteration 645, loss = 23.94734168\n",
      "Iteration 646, loss = 23.84242796\n",
      "Iteration 647, loss = 22.88897213\n",
      "Iteration 648, loss = 20.93693446\n",
      "Iteration 649, loss = 19.99456658\n",
      "Iteration 650, loss = 18.30498020\n",
      "Iteration 651, loss = 19.10845218\n",
      "Iteration 652, loss = 21.20854471\n",
      "Iteration 653, loss = 20.84441308\n",
      "Iteration 654, loss = 22.81152586\n",
      "Iteration 655, loss = 20.60215781\n",
      "Iteration 656, loss = 19.86740578\n",
      "Iteration 657, loss = 17.65185691\n",
      "Iteration 658, loss = 21.55825931\n",
      "Iteration 659, loss = 27.71281356\n",
      "Iteration 660, loss = 22.86279380\n",
      "Iteration 661, loss = 22.58589143\n",
      "Iteration 662, loss = 21.37260987\n",
      "Iteration 663, loss = 21.29565797\n",
      "Iteration 664, loss = 18.11774377\n",
      "Iteration 665, loss = 22.24900656\n",
      "Iteration 666, loss = 22.41218782\n",
      "Iteration 667, loss = 18.66715608\n",
      "Iteration 668, loss = 19.65707249\n",
      "Iteration 669, loss = 18.47275097\n",
      "Iteration 670, loss = 16.78107106\n",
      "Iteration 671, loss = 18.48827533\n",
      "Iteration 672, loss = 19.37820986\n",
      "Iteration 673, loss = 21.67579832\n",
      "Iteration 674, loss = 23.15895633\n",
      "Iteration 675, loss = 23.43402142\n",
      "Iteration 676, loss = 27.60542767\n",
      "Iteration 677, loss = 29.96311648\n",
      "Iteration 678, loss = 18.99778779\n",
      "Iteration 679, loss = 16.96095560\n",
      "Iteration 680, loss = 20.99574769\n",
      "Iteration 681, loss = 23.50017072\n",
      "Iteration 682, loss = 20.63112406\n",
      "Iteration 683, loss = 18.91187638\n",
      "Iteration 684, loss = 19.62057541\n",
      "Iteration 685, loss = 19.56076590\n",
      "Iteration 686, loss = 18.08431380\n",
      "Iteration 687, loss = 21.15351106\n",
      "Iteration 688, loss = 18.01723302\n",
      "Iteration 689, loss = 19.30120743\n",
      "Iteration 690, loss = 21.20905640\n",
      "Iteration 691, loss = 21.42457640\n",
      "Iteration 692, loss = 20.28214396\n",
      "Iteration 693, loss = 17.01442415\n",
      "Iteration 694, loss = 19.22801040\n",
      "Iteration 695, loss = 17.66042893\n",
      "Iteration 696, loss = 17.56131058\n",
      "Training loss did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n",
      "negative registered most important features:\n",
      "                importance\n",
      "registered, -1    0.663865\n",
      "hour              0.105716\n",
      "workingday        0.031747\n",
      "dayofweek         0.031146\n",
      "registered, -2    0.021554\n",
      "casual, -1        0.019425\n",
      "weather           0.016709\n",
      "humidity          0.015922\n",
      "casual, -2        0.008240\n",
      "workingday, -1    0.008086\n",
      "weather, -1       0.007677\n",
      "humidity, -1      0.006787\n",
      "dayofyear         0.006384\n",
      "windspeed, -2     0.006178\n",
      "atemp             0.006168\n",
      "humidity, -2      0.006118\n",
      "windspeed         0.005983\n",
      "windspeed, -1     0.004985\n",
      "temp              0.004737\n",
      "temp, -2          0.004472\n",
      "atemp, -2         0.004064\n",
      "temp, -1          0.002910\n",
      "atemp, -1         0.002870\n",
      "weekofyear        0.002680\n",
      "holiday, -1       0.001812\n",
      "holiday           0.001521\n",
      "weather, -2       0.001029\n",
      "workingday, -2    0.000644\n",
      "season            0.000426\n",
      "holiday, -2       0.000145\n",
      "\n",
      "Iteration 1, loss = 18670.68680748\n",
      "Iteration 2, loss = 6098.04426337\n",
      "Iteration 3, loss = 3433.94641328\n",
      "Iteration 4, loss = 2801.21537751\n",
      "Iteration 5, loss = 2503.78612770\n",
      "Iteration 6, loss = 2280.24005344\n",
      "Iteration 7, loss = 2088.84396697\n",
      "Iteration 8, loss = 1926.31893896\n",
      "Iteration 9, loss = 1810.76999752\n",
      "Iteration 10, loss = 1718.35447396\n",
      "Iteration 11, loss = 1658.28678513\n",
      "Iteration 12, loss = 1604.63664449\n",
      "Iteration 13, loss = 1559.65452794\n",
      "Iteration 14, loss = 1512.89603383\n",
      "Iteration 15, loss = 1482.09161435\n",
      "Iteration 16, loss = 1447.72712861\n",
      "Iteration 17, loss = 1423.83383826\n",
      "Iteration 18, loss = 1389.52991208\n",
      "Iteration 19, loss = 1347.04173997\n",
      "Iteration 20, loss = 1316.42352556\n",
      "Iteration 21, loss = 1279.17712196\n",
      "Iteration 22, loss = 1264.33164666\n",
      "Iteration 23, loss = 1225.53792743\n",
      "Iteration 24, loss = 1198.57206231\n",
      "Iteration 25, loss = 1169.72933594\n",
      "Iteration 26, loss = 1132.83639118\n",
      "Iteration 27, loss = 1108.04102600\n",
      "Iteration 28, loss = 1074.08637112\n",
      "Iteration 29, loss = 1050.17025535\n",
      "Iteration 30, loss = 1018.71332207\n",
      "Iteration 31, loss = 970.29752939\n",
      "Iteration 32, loss = 946.59564705\n",
      "Iteration 33, loss = 909.60918500\n",
      "Iteration 34, loss = 877.62465398\n",
      "Iteration 35, loss = 861.62074037\n",
      "Iteration 36, loss = 808.33162908\n",
      "Iteration 37, loss = 784.52727228\n",
      "Iteration 38, loss = 751.04528957\n",
      "Iteration 39, loss = 729.53524794\n",
      "Iteration 40, loss = 705.95761239\n",
      "Iteration 41, loss = 681.54661456\n",
      "Iteration 42, loss = 644.83427374\n",
      "Iteration 43, loss = 636.11193153\n",
      "Iteration 44, loss = 623.73395370\n",
      "Iteration 45, loss = 590.34914267\n",
      "Iteration 46, loss = 566.15310647\n",
      "Iteration 47, loss = 536.06755547\n",
      "Iteration 48, loss = 522.92809231\n",
      "Iteration 49, loss = 517.40433516\n",
      "Iteration 50, loss = 502.25608341\n",
      "Iteration 51, loss = 479.40141301\n",
      "Iteration 52, loss = 465.93499514\n",
      "Iteration 53, loss = 459.91313915\n",
      "Iteration 54, loss = 439.67890287\n",
      "Iteration 55, loss = 424.65209725\n",
      "Iteration 56, loss = 410.27554264\n",
      "Iteration 57, loss = 395.98790280\n",
      "Iteration 58, loss = 406.60355005\n",
      "Iteration 59, loss = 392.08909399\n",
      "Iteration 60, loss = 378.43935797\n",
      "Iteration 61, loss = 363.93271269\n",
      "Iteration 62, loss = 351.22729261\n",
      "Iteration 63, loss = 345.84963048\n",
      "Iteration 64, loss = 342.81491683\n",
      "Iteration 65, loss = 328.45190650\n",
      "Iteration 66, loss = 321.32198551\n",
      "Iteration 67, loss = 319.61668796\n",
      "Iteration 68, loss = 319.13888251\n",
      "Iteration 69, loss = 305.59583184\n",
      "Iteration 70, loss = 304.33002773\n",
      "Iteration 71, loss = 302.83794669\n",
      "Iteration 72, loss = 284.44960214\n",
      "Iteration 73, loss = 283.34107773\n",
      "Iteration 74, loss = 273.85791195\n",
      "Iteration 75, loss = 273.60678079\n",
      "Iteration 76, loss = 272.09663433\n",
      "Iteration 77, loss = 261.61032379\n",
      "Iteration 78, loss = 259.44680591\n",
      "Iteration 79, loss = 261.00205344\n",
      "Iteration 80, loss = 251.84936591\n",
      "Iteration 81, loss = 254.33763405\n",
      "Iteration 82, loss = 244.66935818\n",
      "Iteration 83, loss = 248.31175847\n",
      "Iteration 84, loss = 244.39629257\n",
      "Iteration 85, loss = 237.11349376\n",
      "Iteration 86, loss = 241.31352405\n",
      "Iteration 87, loss = 238.22693511\n",
      "Iteration 88, loss = 235.46238628\n",
      "Iteration 89, loss = 227.97019030\n",
      "Iteration 90, loss = 216.82816727\n",
      "Iteration 91, loss = 216.47147455\n",
      "Iteration 92, loss = 213.48322350\n",
      "Iteration 93, loss = 209.80445497\n",
      "Iteration 94, loss = 211.52912267\n",
      "Iteration 95, loss = 208.02495489\n",
      "Iteration 96, loss = 209.13753702\n",
      "Iteration 97, loss = 204.23470045\n",
      "Iteration 98, loss = 198.19202925\n",
      "Iteration 99, loss = 206.79317768\n",
      "Iteration 100, loss = 192.53253332\n",
      "Iteration 101, loss = 195.17885591\n",
      "Iteration 102, loss = 189.36920110\n",
      "Iteration 103, loss = 190.26324899\n",
      "Iteration 104, loss = 186.82448388\n",
      "Iteration 105, loss = 189.07754449\n",
      "Iteration 106, loss = 184.38028608\n",
      "Iteration 107, loss = 178.33234301\n",
      "Iteration 108, loss = 173.79141277\n",
      "Iteration 109, loss = 177.75467281\n",
      "Iteration 110, loss = 170.15665176\n",
      "Iteration 111, loss = 174.37235555\n",
      "Iteration 112, loss = 174.25853562\n",
      "Iteration 113, loss = 161.56317313\n",
      "Iteration 114, loss = 163.63164501\n",
      "Iteration 115, loss = 163.74941781\n",
      "Iteration 116, loss = 173.10589341\n",
      "Iteration 117, loss = 164.45779841\n",
      "Iteration 118, loss = 157.15455353\n",
      "Iteration 119, loss = 159.16115632\n",
      "Iteration 120, loss = 164.41345481\n",
      "Iteration 121, loss = 161.62647210\n",
      "Iteration 122, loss = 164.31634874\n",
      "Iteration 123, loss = 151.68891946\n",
      "Iteration 124, loss = 155.48424594\n",
      "Iteration 125, loss = 152.09100413\n",
      "Iteration 126, loss = 145.61339757\n",
      "Iteration 127, loss = 151.68938220\n",
      "Iteration 128, loss = 149.55365615\n",
      "Iteration 129, loss = 149.82907892\n",
      "Iteration 130, loss = 153.49005179\n",
      "Iteration 131, loss = 143.08631212\n",
      "Iteration 132, loss = 147.36090243\n",
      "Iteration 133, loss = 136.93425399\n",
      "Iteration 134, loss = 147.35196831\n",
      "Iteration 135, loss = 139.25969151\n",
      "Iteration 136, loss = 135.17381627\n",
      "Iteration 137, loss = 136.81813319\n",
      "Iteration 138, loss = 134.99774012\n",
      "Iteration 139, loss = 133.29527663\n",
      "Iteration 140, loss = 130.27800557\n",
      "Iteration 141, loss = 134.58442214\n",
      "Iteration 142, loss = 134.85622487\n",
      "Iteration 143, loss = 127.74764620\n",
      "Iteration 144, loss = 126.91891105\n",
      "Iteration 145, loss = 124.29849284\n",
      "Iteration 146, loss = 123.49155434\n",
      "Iteration 147, loss = 124.31020622\n",
      "Iteration 148, loss = 124.91939410\n",
      "Iteration 149, loss = 119.72448658\n",
      "Iteration 150, loss = 124.44293575\n",
      "Iteration 151, loss = 120.01922919\n",
      "Iteration 152, loss = 118.35359527\n",
      "Iteration 153, loss = 115.38675553\n",
      "Iteration 154, loss = 119.37315255\n",
      "Iteration 155, loss = 113.98394729\n",
      "Iteration 156, loss = 114.34326762\n",
      "Iteration 157, loss = 116.99880176\n",
      "Iteration 158, loss = 113.87826247\n",
      "Iteration 159, loss = 119.21704600\n",
      "Iteration 160, loss = 112.81035359\n",
      "Iteration 161, loss = 110.44310162\n",
      "Iteration 162, loss = 113.72254397\n",
      "Iteration 163, loss = 117.65373181\n",
      "Iteration 164, loss = 110.43153281\n",
      "Iteration 165, loss = 108.28409050\n",
      "Iteration 166, loss = 109.97424179\n",
      "Iteration 167, loss = 106.05777346\n",
      "Iteration 168, loss = 106.03077221\n",
      "Iteration 169, loss = 100.24565536\n",
      "Iteration 170, loss = 106.14619774\n",
      "Iteration 171, loss = 103.36833690\n",
      "Iteration 172, loss = 101.63923564\n",
      "Iteration 173, loss = 103.22918358\n",
      "Iteration 174, loss = 115.27547751\n",
      "Iteration 175, loss = 102.25053293\n",
      "Iteration 176, loss = 106.76081630\n",
      "Iteration 177, loss = 104.27736634\n",
      "Iteration 178, loss = 102.57657733\n",
      "Iteration 179, loss = 100.35256577\n",
      "Iteration 180, loss = 96.84784628\n",
      "Iteration 181, loss = 95.89440309\n",
      "Iteration 182, loss = 93.67009659\n",
      "Iteration 183, loss = 94.85381740\n",
      "Iteration 184, loss = 95.71430422\n",
      "Iteration 185, loss = 88.83485658\n",
      "Iteration 186, loss = 96.06924207\n",
      "Iteration 187, loss = 95.47681802\n",
      "Iteration 188, loss = 86.26704381\n",
      "Iteration 189, loss = 92.52787229\n",
      "Iteration 190, loss = 92.38640859\n",
      "Iteration 191, loss = 90.50856339\n",
      "Iteration 192, loss = 91.35091628\n",
      "Iteration 193, loss = 89.95630683\n",
      "Iteration 194, loss = 97.59743459\n",
      "Iteration 195, loss = 86.74793532\n",
      "Iteration 196, loss = 82.71943264\n",
      "Iteration 197, loss = 87.01827009\n",
      "Iteration 198, loss = 87.30652529\n",
      "Iteration 199, loss = 90.32047331\n",
      "Iteration 200, loss = 87.14514373\n",
      "Iteration 201, loss = 80.29066911\n",
      "Iteration 202, loss = 80.55537874\n",
      "Iteration 203, loss = 88.68118406\n",
      "Iteration 204, loss = 83.06589549\n",
      "Iteration 205, loss = 78.03938598\n",
      "Iteration 206, loss = 87.59802390\n",
      "Iteration 207, loss = 87.55430198\n",
      "Iteration 208, loss = 79.98675297\n",
      "Iteration 209, loss = 78.54139844\n",
      "Iteration 210, loss = 78.79828429\n",
      "Iteration 211, loss = 80.49780592\n",
      "Iteration 212, loss = 82.88339773\n",
      "Iteration 213, loss = 80.74413481\n",
      "Iteration 214, loss = 86.65687109\n",
      "Iteration 215, loss = 80.69369709\n",
      "Iteration 216, loss = 76.66783181\n",
      "Iteration 217, loss = 81.06062820\n",
      "Iteration 218, loss = 79.06945361\n",
      "Iteration 219, loss = 77.67958032\n",
      "Iteration 220, loss = 74.91883735\n",
      "Iteration 221, loss = 78.72544447\n",
      "Iteration 222, loss = 74.68270943\n",
      "Iteration 223, loss = 72.67631406\n",
      "Iteration 224, loss = 70.51900725\n",
      "Iteration 225, loss = 69.69954573\n",
      "Iteration 226, loss = 77.54965989\n",
      "Iteration 227, loss = 69.95749767\n",
      "Iteration 228, loss = 68.98709159\n",
      "Iteration 229, loss = 68.02922703\n",
      "Iteration 230, loss = 68.32901714\n",
      "Iteration 231, loss = 70.18505823\n",
      "Iteration 232, loss = 68.67871934\n",
      "Iteration 233, loss = 74.02686088\n",
      "Iteration 234, loss = 72.90874368\n",
      "Iteration 235, loss = 71.99828754\n",
      "Iteration 236, loss = 70.02758115\n",
      "Iteration 237, loss = 68.54386831\n",
      "Iteration 238, loss = 71.10952640\n",
      "Iteration 239, loss = 66.32371369\n",
      "Iteration 240, loss = 64.68058007\n",
      "Iteration 241, loss = 74.60501491\n",
      "Iteration 242, loss = 87.88995756\n",
      "Iteration 243, loss = 66.17476875\n",
      "Iteration 244, loss = 61.75338786\n",
      "Iteration 245, loss = 61.93889276\n",
      "Iteration 246, loss = 61.23957481\n",
      "Iteration 247, loss = 60.94161157\n",
      "Iteration 248, loss = 66.85749866\n",
      "Iteration 249, loss = 63.12570302\n",
      "Iteration 250, loss = 65.30099706\n",
      "Iteration 251, loss = 59.33086244\n",
      "Iteration 252, loss = 62.12444530\n",
      "Iteration 253, loss = 58.86103605\n",
      "Iteration 254, loss = 60.08414170\n",
      "Iteration 255, loss = 64.33427524\n",
      "Iteration 256, loss = 65.08703711\n",
      "Iteration 257, loss = 62.18729244\n",
      "Iteration 258, loss = 58.29725794\n",
      "Iteration 259, loss = 59.81991877\n",
      "Iteration 260, loss = 58.78563520\n",
      "Iteration 261, loss = 62.88457300\n",
      "Iteration 262, loss = 60.66631432\n",
      "Iteration 263, loss = 59.82868632\n",
      "Iteration 264, loss = 60.80487860\n",
      "Iteration 265, loss = 58.85999479\n",
      "Iteration 266, loss = 64.59941898\n",
      "Iteration 267, loss = 58.45106737\n",
      "Iteration 268, loss = 55.64308308\n",
      "Iteration 269, loss = 54.65319744\n",
      "Iteration 270, loss = 56.18639989\n",
      "Iteration 271, loss = 54.44403204\n",
      "Iteration 272, loss = 52.20329394\n",
      "Iteration 273, loss = 55.46084560\n",
      "Iteration 274, loss = 56.08977516\n",
      "Iteration 275, loss = 55.60348691\n",
      "Iteration 276, loss = 56.05723551\n",
      "Iteration 277, loss = 53.33386747\n",
      "Iteration 278, loss = 63.46304447\n",
      "Iteration 279, loss = 58.32738270\n",
      "Iteration 280, loss = 53.80697553\n",
      "Iteration 281, loss = 53.24909487\n",
      "Iteration 282, loss = 58.46333629\n",
      "Iteration 283, loss = 54.45166006\n",
      "Iteration 284, loss = 51.30993959\n",
      "Iteration 285, loss = 49.92632322\n",
      "Iteration 286, loss = 49.15514847\n",
      "Iteration 287, loss = 50.24790285\n",
      "Iteration 288, loss = 49.42931871\n",
      "Iteration 289, loss = 49.22903874\n",
      "Iteration 290, loss = 49.56182315\n",
      "Iteration 291, loss = 50.60388094\n",
      "Iteration 292, loss = 50.72424365\n",
      "Iteration 293, loss = 53.84526770\n",
      "Iteration 294, loss = 49.07959479\n",
      "Iteration 295, loss = 50.25738108\n",
      "Iteration 296, loss = 53.27967726\n",
      "Iteration 297, loss = 53.22554063\n",
      "Iteration 298, loss = 49.85202733\n",
      "Iteration 299, loss = 50.54547136\n",
      "Iteration 300, loss = 62.11427460\n",
      "Iteration 301, loss = 54.68379725\n",
      "Iteration 302, loss = 49.83815007\n",
      "Iteration 303, loss = 47.93983012\n",
      "Iteration 304, loss = 47.36437897\n",
      "Iteration 305, loss = 44.84369431\n",
      "Iteration 306, loss = 47.69408107\n",
      "Iteration 307, loss = 48.59585602\n",
      "Iteration 308, loss = 46.71113414\n",
      "Iteration 309, loss = 47.44174528\n",
      "Iteration 310, loss = 48.01143252\n",
      "Iteration 311, loss = 44.87955078\n",
      "Iteration 312, loss = 47.83023337\n",
      "Iteration 313, loss = 51.06504050\n",
      "Iteration 314, loss = 62.54735552\n",
      "Iteration 315, loss = 56.08998235\n",
      "Iteration 316, loss = 49.70217959\n",
      "Iteration 317, loss = 52.02233802\n",
      "Iteration 318, loss = 45.80453350\n",
      "Iteration 319, loss = 43.65315319\n",
      "Iteration 320, loss = 49.97844174\n",
      "Iteration 321, loss = 43.19215117\n",
      "Iteration 322, loss = 44.57150515\n",
      "Iteration 323, loss = 42.34133033\n",
      "Iteration 324, loss = 39.79006795\n",
      "Iteration 325, loss = 40.77555900\n",
      "Iteration 326, loss = 46.29336005\n",
      "Iteration 327, loss = 42.92763122\n",
      "Iteration 328, loss = 42.97935051\n",
      "Iteration 329, loss = 53.58047837\n",
      "Iteration 330, loss = 42.19637611\n",
      "Iteration 331, loss = 46.62866407\n",
      "Iteration 332, loss = 40.62136656\n",
      "Iteration 333, loss = 41.22030163\n",
      "Iteration 334, loss = 41.07259153\n",
      "Iteration 335, loss = 40.74888663\n",
      "Iteration 336, loss = 37.37782945\n",
      "Iteration 337, loss = 40.44712042\n",
      "Iteration 338, loss = 40.29227630\n",
      "Iteration 339, loss = 38.42214810\n",
      "Iteration 340, loss = 38.06818336\n",
      "Iteration 341, loss = 37.06731554\n",
      "Iteration 342, loss = 38.97079794\n",
      "Iteration 343, loss = 40.46539940\n",
      "Iteration 344, loss = 40.15224401\n",
      "Iteration 345, loss = 38.82273953\n",
      "Iteration 346, loss = 50.24857837\n",
      "Iteration 347, loss = 47.11564286\n",
      "Iteration 348, loss = 48.95518903\n",
      "Iteration 349, loss = 39.92993942\n",
      "Iteration 350, loss = 36.68193549\n",
      "Iteration 351, loss = 41.42996252\n",
      "Iteration 352, loss = 38.97626828\n",
      "Iteration 353, loss = 36.34813172\n",
      "Iteration 354, loss = 34.92824173\n",
      "Iteration 355, loss = 37.87246235\n",
      "Iteration 356, loss = 41.01695663\n",
      "Iteration 357, loss = 40.92179248\n",
      "Iteration 358, loss = 35.86957209\n",
      "Iteration 359, loss = 37.36608654\n",
      "Iteration 360, loss = 45.68809952\n",
      "Iteration 361, loss = 38.88766098\n",
      "Iteration 362, loss = 35.27086441\n",
      "Iteration 363, loss = 37.03108740\n",
      "Iteration 364, loss = 36.77902543\n",
      "Iteration 365, loss = 37.91212846\n",
      "Iteration 366, loss = 42.70724864\n",
      "Iteration 367, loss = 37.63551860\n",
      "Iteration 368, loss = 33.27746320\n",
      "Iteration 369, loss = 40.31168601\n",
      "Iteration 370, loss = 38.54000632\n",
      "Iteration 371, loss = 35.98394414\n",
      "Iteration 372, loss = 34.77086905\n",
      "Iteration 373, loss = 37.35453304\n",
      "Iteration 374, loss = 34.57028056\n",
      "Iteration 375, loss = 34.80272158\n",
      "Iteration 376, loss = 35.69649366\n",
      "Iteration 377, loss = 33.63484191\n",
      "Iteration 378, loss = 34.04870783\n",
      "Iteration 379, loss = 31.69789457\n",
      "Iteration 380, loss = 32.82644133\n",
      "Iteration 381, loss = 31.41283594\n",
      "Iteration 382, loss = 31.67762588\n",
      "Iteration 383, loss = 33.15724329\n",
      "Iteration 384, loss = 32.36884689\n",
      "Iteration 385, loss = 32.23162182\n",
      "Iteration 386, loss = 38.80508829\n",
      "Iteration 387, loss = 34.79058772\n",
      "Iteration 388, loss = 32.16531662\n",
      "Iteration 389, loss = 30.78529298\n",
      "Iteration 390, loss = 35.32100661\n",
      "Iteration 391, loss = 39.95054415\n",
      "Iteration 392, loss = 33.28969333\n",
      "Iteration 393, loss = 33.38630749\n",
      "Iteration 394, loss = 34.42475728\n",
      "Iteration 395, loss = 39.10839244\n",
      "Iteration 396, loss = 32.48683869\n",
      "Iteration 397, loss = 32.34040701\n",
      "Iteration 398, loss = 35.32656560\n",
      "Iteration 399, loss = 33.84802967\n",
      "Iteration 400, loss = 38.04587871\n",
      "Iteration 401, loss = 31.88010877\n",
      "Iteration 402, loss = 29.98189608\n",
      "Iteration 403, loss = 32.24888331\n",
      "Iteration 404, loss = 31.72983399\n",
      "Iteration 405, loss = 32.74305716\n",
      "Iteration 406, loss = 35.05867629\n",
      "Iteration 407, loss = 37.48287443\n",
      "Iteration 408, loss = 37.76115310\n",
      "Iteration 409, loss = 29.33469722\n",
      "Iteration 410, loss = 28.39461266\n",
      "Iteration 411, loss = 31.38740042\n",
      "Iteration 412, loss = 30.51009384\n",
      "Iteration 413, loss = 31.03958380\n",
      "Iteration 414, loss = 28.26140828\n",
      "Iteration 415, loss = 27.07650812\n",
      "Iteration 416, loss = 27.40964374\n",
      "Iteration 417, loss = 30.54835133\n",
      "Iteration 418, loss = 28.78311009\n",
      "Iteration 419, loss = 29.08103879\n",
      "Iteration 420, loss = 28.75966573\n",
      "Iteration 421, loss = 27.71263916\n",
      "Iteration 422, loss = 30.29880294\n",
      "Iteration 423, loss = 29.65644162\n",
      "Iteration 424, loss = 27.86352679\n",
      "Iteration 425, loss = 28.28311042\n",
      "Iteration 426, loss = 38.80897979\n",
      "Iteration 427, loss = 36.29396146\n",
      "Iteration 428, loss = 30.77228505\n",
      "Iteration 429, loss = 30.81613535\n",
      "Iteration 430, loss = 33.31023815\n",
      "Iteration 431, loss = 29.86768772\n",
      "Iteration 432, loss = 27.71638148\n",
      "Iteration 433, loss = 30.24701000\n",
      "Iteration 434, loss = 28.42195621\n",
      "Iteration 435, loss = 26.35283756\n",
      "Iteration 436, loss = 27.86305789\n",
      "Iteration 437, loss = 32.30240779\n",
      "Iteration 438, loss = 35.58287935\n",
      "Iteration 439, loss = 31.75314324\n",
      "Iteration 440, loss = 28.31979015\n",
      "Iteration 441, loss = 33.89966820\n",
      "Iteration 442, loss = 30.59041741\n",
      "Iteration 443, loss = 29.60037954\n",
      "Iteration 444, loss = 28.88725638\n",
      "Iteration 445, loss = 26.54524336\n",
      "Iteration 446, loss = 25.88307727\n",
      "Iteration 447, loss = 27.29946950\n",
      "Iteration 448, loss = 28.42124864\n",
      "Iteration 449, loss = 24.65502060\n",
      "Iteration 450, loss = 24.94149933\n",
      "Iteration 451, loss = 24.45244807\n",
      "Iteration 452, loss = 24.46419635\n",
      "Iteration 453, loss = 24.57871828\n",
      "Iteration 454, loss = 30.69103543\n",
      "Iteration 455, loss = 27.31242422\n",
      "Iteration 456, loss = 27.97919643\n",
      "Iteration 457, loss = 24.96864993\n",
      "Iteration 458, loss = 24.29178763\n",
      "Iteration 459, loss = 23.61259555\n",
      "Iteration 460, loss = 24.87195200\n",
      "Iteration 461, loss = 24.00890360\n",
      "Iteration 462, loss = 24.48001973\n",
      "Iteration 463, loss = 24.73241957\n",
      "Iteration 464, loss = 27.25612203\n",
      "Iteration 465, loss = 24.24884116\n",
      "Iteration 466, loss = 24.44739538\n",
      "Iteration 467, loss = 28.25018641\n",
      "Iteration 468, loss = 25.85330957\n",
      "Iteration 469, loss = 24.08219086\n",
      "Iteration 470, loss = 28.88341223\n",
      "Iteration 471, loss = 27.60973465\n",
      "Iteration 472, loss = 24.88214544\n",
      "Iteration 473, loss = 23.27813719\n",
      "Iteration 474, loss = 23.90996192\n",
      "Iteration 475, loss = 26.40934151\n",
      "Iteration 476, loss = 26.54231084\n",
      "Iteration 477, loss = 28.46454311\n",
      "Iteration 478, loss = 24.57007624\n",
      "Iteration 479, loss = 23.48208319\n",
      "Iteration 480, loss = 24.22245759\n",
      "Iteration 481, loss = 22.90029853\n",
      "Iteration 482, loss = 21.62357242\n",
      "Iteration 483, loss = 24.81498818\n",
      "Iteration 484, loss = 25.23892647\n",
      "Iteration 485, loss = 25.94341940\n",
      "Iteration 486, loss = 23.10127564\n",
      "Iteration 487, loss = 21.76559960\n",
      "Iteration 488, loss = 21.48403229\n",
      "Iteration 489, loss = 25.16101362\n",
      "Iteration 490, loss = 28.72874754\n",
      "Iteration 491, loss = 23.24691192\n",
      "Iteration 492, loss = 22.92536353\n",
      "Iteration 493, loss = 31.87389856\n",
      "Iteration 494, loss = 25.25988783\n",
      "Iteration 495, loss = 25.69863779\n",
      "Iteration 496, loss = 24.14305983\n",
      "Iteration 497, loss = 28.01882719\n",
      "Iteration 498, loss = 32.83169926\n",
      "Iteration 499, loss = 26.23844663\n",
      "Iteration 500, loss = 23.41258205\n",
      "Iteration 501, loss = 26.30131152\n",
      "Iteration 502, loss = 29.62027432\n",
      "Iteration 503, loss = 25.50313084\n",
      "Iteration 504, loss = 21.79493795\n",
      "Iteration 505, loss = 20.68473700\n",
      "Iteration 506, loss = 26.07144106\n",
      "Iteration 507, loss = 23.34017803\n",
      "Iteration 508, loss = 26.00921558\n",
      "Iteration 509, loss = 22.01206265\n",
      "Iteration 510, loss = 23.33331820\n",
      "Iteration 511, loss = 19.98762956\n",
      "Iteration 512, loss = 20.19482755\n",
      "Iteration 513, loss = 22.98989464\n",
      "Iteration 514, loss = 20.10030039\n",
      "Iteration 515, loss = 19.23168100\n",
      "Iteration 516, loss = 20.79219308\n",
      "Iteration 517, loss = 20.93396123\n",
      "Iteration 518, loss = 25.00627384\n",
      "Iteration 519, loss = 25.77144635\n",
      "Iteration 520, loss = 22.82824298\n",
      "Iteration 521, loss = 21.22277780\n",
      "Iteration 522, loss = 26.04848399\n",
      "Iteration 523, loss = 20.93252078\n",
      "Iteration 524, loss = 18.12927974\n",
      "Iteration 525, loss = 18.94330239\n",
      "Iteration 526, loss = 34.21876694\n",
      "Iteration 527, loss = 32.17732813\n",
      "Iteration 528, loss = 30.45331587\n",
      "Iteration 529, loss = 22.79632698\n",
      "Iteration 530, loss = 19.14165897\n",
      "Iteration 531, loss = 22.56299980\n",
      "Iteration 532, loss = 20.82541558\n",
      "Iteration 533, loss = 19.27584917\n",
      "Iteration 534, loss = 20.36092262\n",
      "Iteration 535, loss = 20.28433059\n",
      "Iteration 536, loss = 26.65969395\n",
      "Iteration 537, loss = 24.10818526\n",
      "Iteration 538, loss = 20.86472527\n",
      "Iteration 539, loss = 20.99357401\n",
      "Iteration 540, loss = 24.92972465\n",
      "Iteration 541, loss = 28.90271510\n",
      "Iteration 542, loss = 26.92396925\n",
      "Iteration 543, loss = 22.68837973\n",
      "Iteration 544, loss = 24.54506847\n",
      "Iteration 545, loss = 23.76689509\n",
      "Iteration 546, loss = 19.19249233\n",
      "Iteration 547, loss = 21.97286734\n",
      "Iteration 548, loss = 18.65017127\n",
      "Iteration 549, loss = 19.00531550\n",
      "Iteration 550, loss = 17.77809179\n",
      "Iteration 551, loss = 16.11830572\n",
      "Iteration 552, loss = 16.74675228\n",
      "Iteration 553, loss = 16.71699547\n",
      "Iteration 554, loss = 17.81810423\n",
      "Iteration 555, loss = 17.93690854\n",
      "Iteration 556, loss = 22.18926709\n",
      "Iteration 557, loss = 19.60431990\n",
      "Iteration 558, loss = 18.03081617\n",
      "Iteration 559, loss = 21.73435037\n",
      "Iteration 560, loss = 19.01702667\n",
      "Iteration 561, loss = 26.74776109\n",
      "Iteration 562, loss = 19.69938940\n",
      "Iteration 563, loss = 19.00014304\n",
      "Iteration 564, loss = 17.99061288\n",
      "Iteration 565, loss = 16.82528822\n",
      "Iteration 566, loss = 19.77864104\n",
      "Iteration 567, loss = 19.76255901\n",
      "Iteration 568, loss = 24.77977552\n",
      "Iteration 569, loss = 20.48942935\n",
      "Iteration 570, loss = 15.66150169\n",
      "Iteration 571, loss = 16.79527923\n",
      "Iteration 572, loss = 22.99022814\n",
      "Iteration 573, loss = 17.77775499\n",
      "Iteration 574, loss = 20.82884929\n",
      "Iteration 575, loss = 18.88911032\n",
      "Iteration 576, loss = 20.27279658\n",
      "Iteration 577, loss = 22.19954898\n",
      "Iteration 578, loss = 19.63395233\n",
      "Iteration 579, loss = 18.49703616\n",
      "Iteration 580, loss = 17.78001769\n",
      "Iteration 581, loss = 17.90424481\n",
      "Iteration 582, loss = 20.43982130\n",
      "Iteration 583, loss = 21.10746432\n",
      "Iteration 584, loss = 18.78417321\n",
      "Iteration 585, loss = 19.86250658\n",
      "Iteration 586, loss = 19.57843698\n",
      "Iteration 587, loss = 18.44306568\n",
      "Iteration 588, loss = 16.33095905\n",
      "Iteration 589, loss = 17.80500389\n",
      "Iteration 590, loss = 17.89384814\n",
      "Iteration 591, loss = 20.49638472\n",
      "Iteration 592, loss = 19.80200815\n",
      "Iteration 593, loss = 21.32473419\n",
      "Iteration 594, loss = 18.43656922\n",
      "Iteration 595, loss = 16.39784726\n",
      "Iteration 596, loss = 17.32001290\n",
      "Training loss did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# select columns not to be used in the model\n",
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year', 'casual', 'registered', 'count']\n",
    "\n",
    "# train the models\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:\n",
    "        # train the first pipeline\n",
    "        globals()[f'{direction}_{target}_pipeline_1'].fit(\n",
    "            globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1),\n",
    "            globals()[f'{direction}_train_data'][target]\n",
    "        )\n",
    "\n",
    "        # save the most important feature into a dataframe\n",
    "        feature_importances = pd.DataFrame(\n",
    "            globals()[f'{direction}_{target}_pipeline_1'].named_steps['regressor'].feature_importances_,\n",
    "            index=globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1).columns,\n",
    "            columns=['importance']\n",
    "        )\n",
    "\n",
    "        # sort the dataframe\n",
    "        feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        # print the most important features\n",
    "        print(f'{direction} {target} most important features:')\n",
    "        print(feature_importances)\n",
    "        print('')\n",
    "\n",
    "        # train the second pipeline\n",
    "        globals()[f'{direction}_{target}_mlp_pipeline_2'].fit(\n",
    "            globals()[f'{direction}_train_data'].drop(columns=columns_not_to_use, axis=1),\n",
    "            globals()[f'{direction}_train_data'][target]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive casual evaluation\n",
      "MAE RF: 13.076622299497112\n",
      "MAE MLP: 16.805391534946576\n",
      "MAE AVG: 13.48302419342169\n",
      "\n",
      "negative casual evaluation\n",
      "MAE RF: 11.77374913136866\n",
      "MAE MLP: 15.853117782956026\n",
      "MAE AVG: 12.480025738556735\n",
      "\n",
      "positive registered evaluation\n",
      "MAE RF: 29.64201440282047\n",
      "MAE MLP: 38.71121546399405\n",
      "MAE AVG: 30.3797139428525\n",
      "\n",
      "negative registered evaluation\n",
      "MAE RF: 28.529174921094917\n",
      "MAE MLP: 37.43387570976768\n",
      "MAE AVG: 29.481224579370814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models\n",
    "\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:\n",
    "\n",
    "        # print the statetment of target and direction\n",
    "        print(f'{direction} {target} evaluation')\n",
    "\n",
    "        # predict the target using the first pipeline\n",
    "        globals()[f'{direction}_{target}_predictions_1'] = globals()[f'{direction}_{target}_pipeline_1'].predict(\n",
    "            globals()[f'{direction}_test_data'].drop(columns=columns_not_to_use, axis=1)\n",
    "        )\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae_1'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions_1'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE RF: {globals()[f\"{direction}_{target}_mae_1\"]}')\n",
    "\n",
    "        # predict the target using the second pipeline\n",
    "        globals()[f'{direction}_{target}_predictions_2'] = globals()[f'{direction}_{target}_mlp_pipeline_2'].predict(\n",
    "            globals()[f'{direction}_test_data'].drop(columns=columns_not_to_use, axis=1)\n",
    "        )\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae_2'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions_2'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE MLP: {globals()[f\"{direction}_{target}_mae_2\"]}')\n",
    "\n",
    "        # take the average of the two predictions\n",
    "        globals()[f'{direction}_{target}_predictions'] = (globals()[f'{direction}_{target}_predictions_1'] + globals()[f'{direction}_{target}_predictions_2']) / 2\n",
    "\n",
    "        # calculate the mean absolute error\n",
    "        globals()[f'{direction}_{target}_mae'] = mean_absolute_error(globals()[f'{direction}_test_data'][target], globals()[f'{direction}_{target}_predictions'])\n",
    "\n",
    "        # print the results\n",
    "        print(f'MAE AVG: {globals()[f\"{direction}_{target}_mae\"]}')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3423881943.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[187], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    for direction in ['positive', 'negative']:\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models:\n",
    "\n",
    "for target in target_columns:\n",
    "    for direction in ['positive', 'negative']:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
