{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_92240/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_test_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_test_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, rf=True, mlp=False, trees=100, max_iter=2000, hidden_layer_sizes=(100,)):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes, verbose=True)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf = True, mlp = False):\n",
    "\n",
    "    print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_columns = [col for col in columns_not_to_use if col not in target_columns[:i]]\n",
    "\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "            print(f'Columns used: {df.columns}')\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \"\"\"\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \"\"\"\n",
    "\n",
    "            if mlp:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction, train_columns):\n",
    "    saved_target = target\n",
    "    print('target: ', saved_target)\n",
    "    # find the column in which the prediction is stored\n",
    "    for col in train_columns:\n",
    "        if target == col:\n",
    "            if 'original' in col:\n",
    "                target = col[:-len('_original')]\n",
    "            cols_to_insert_prediction = [target]\n",
    "        elif target in col:\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "        elif col in target:\n",
    "            target = col\n",
    "            col = saved_target\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "    \n",
    "    \n",
    "    print('train columns: ', train_columns)\n",
    "    print('cols to insert prediction: ', cols_to_insert_prediction)\n",
    "    print('new target: ', target)\n",
    "\n",
    "    prediction_array = np.array(df[saved_target])\n",
    "    #print('length of prediction array: ', len(prediction_array))\n",
    "    prediction_array[mask] = prediction\n",
    "    #print('Nans in prediction array: ', np.isnan(prediction_array).sum())\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    for col in cols_to_insert_prediction:\n",
    "        df[col] = prediction_array\n",
    "        print('Nans in column: ', col, df[col].isnull().sum())\n",
    "        \n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        #print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        #print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            #print('Inserting prediction for lag: ', lag, ' in coumns: ', cols_to_insert_prediction)\n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print(f'Predictions inserted from datetime: {lagged_datetime[lagged_mask].iloc[0]} to {lagged_datetime[lagged_mask].iloc[-1]}')\n",
    "            #print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "\n",
    "            lagged_col = f'{target}, '+sign+str(abs(lag))\n",
    "            print('lagged col: ', lagged_col)\n",
    "            df.loc[datetime_mask, lagged_col] = prediction_to_store\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "            print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            df = training_data[direction]['y'].copy()\n",
    "            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in train_columns[:i]]\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "            #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "                print('Day: ', day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                print('Number of rows: ', df_days.shape[0])\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf and mlp:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                print('Length of prediction: ', len(prediction))\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "                print('current nan values: ', df[target].isna().sum())\n",
    "                print('')\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            print('Checking for missing values in column: ', target)\n",
    "            assert not prediction.isna().values.any()\n",
    "            print('\\n\\n')\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fitting pipelines...\n",
      "Fitting pipelines for casual_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, -1', 'atemp, -1', 'humidity, -1', 'holiday, -1',\n",
      "       'workingday, -1', 'weather, -1', 'temp, -1', 'casual, -1',\n",
      "       'registered, -1', 'count, -1', 'windspeed, -2', 'atemp, -2',\n",
      "       'humidity, -2', 'holiday, -2', 'workingday, -2', 'weather, -2',\n",
      "       'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, +1', 'atemp, +1', 'humidity, +1', 'holiday, +1',\n",
      "       'workingday, +1', 'weather, +1', 'temp, +1', 'casual, +1',\n",
      "       'registered, +1', 'count, +1', 'windspeed, +2', 'atemp, +2',\n",
      "       'humidity, +2', 'holiday, +2', 'workingday, +2', 'weather, +2',\n",
      "       'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for registered_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'casual_original', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
      "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
      "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
      "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
      "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'casual_original', 'dayofyear',\n",
      "       'weekofyear', 'dayofweek', 'windspeed, +1', 'atemp, +1', 'humidity, +1',\n",
      "       'holiday, +1', 'workingday, +1', 'weather, +1', 'temp, +1',\n",
      "       'casual, +1', 'registered, +1', 'count, +1', 'windspeed, +2',\n",
      "       'atemp, +2', 'humidity, +2', 'holiday, +2', 'workingday, +2',\n",
      "       'weather, +2', 'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n",
      "Fitting pipelines for count_original\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'registered_original',\n",
      "       'casual_original', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, -1', 'atemp, -1', 'humidity, -1', 'holiday, -1',\n",
      "       'workingday, -1', 'weather, -1', 'temp, -1', 'casual, -1',\n",
      "       'registered, -1', 'count, -1', 'windspeed, -2', 'atemp, -2',\n",
      "       'humidity, -2', 'holiday, -2', 'workingday, -2', 'weather, -2',\n",
      "       'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "Columns used: Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'registered_original',\n",
      "       'casual_original', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, +1', 'atemp, +1', 'humidity, +1', 'holiday, +1',\n",
      "       'workingday, +1', 'weather, +1', 'temp, +1', 'casual, +1',\n",
      "       'registered, +1', 'count, +1', 'windspeed, +2', 'atemp, +2',\n",
      "       'humidity, +2', 'holiday, +2', 'workingday, +2', 'weather, +2',\n",
      "       'temp, +2', 'casual, +2', 'registered, +2', 'count, +2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "train_columns = original_columns\n",
    "test_columns = original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = True\n",
    "mlp = False\n",
    "trees = 25\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = (100, 100)\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             rf=rf, mlp=mlp, trees=trees, \n",
    "                             max_iter = 2000, hidden_layer_sizes = (100, 100))\n",
    "\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Direction: negative\n",
      "Day:  16\n",
      "Number of rows:  558\n",
      "Length of prediction:  558\n",
      "target:  casual_original\n",
      "train columns:  ['casual_original', 'registered_original', 'count_original']\n",
      "cols to insert prediction:  ['casual']\n",
      "new target:  casual\n",
      "Nans in column:  casual 1672\n",
      "lagged col:  casual, -1\n",
      "lagged col:  casual, -2\n",
      "current nan values:  2230\n",
      "\n",
      "Day:  17\n",
      "Number of rows:  557\n",
      "Length of prediction:  557\n",
      "target:  casual_original\n",
      "train columns:  ['casual_original', 'registered_original', 'count_original']\n",
      "cols to insert prediction:  ['casual']\n",
      "new target:  casual\n",
      "Nans in column:  casual 1673\n",
      "lagged col:  casual, -1\n",
      "lagged col:  casual, -2\n",
      "current nan values:  2230\n",
      "\n",
      "Day:  18\n",
      "Number of rows:  557\n",
      "Length of prediction:  557\n",
      "target:  casual_original\n",
      "train columns:  ['casual_original', 'registered_original', 'count_original']\n",
      "cols to insert prediction:  ['casual']\n",
      "new target:  casual\n",
      "Nans in column:  casual 1673\n",
      "lagged col:  casual, -1\n",
      "current nan values:  2230\n",
      "\n",
      "Day:  19\n",
      "Number of rows:  558\n",
      "Length of prediction:  558\n",
      "target:  casual_original\n",
      "train columns:  ['casual_original', 'registered_original', 'count_original']\n",
      "cols to insert prediction:  ['casual']\n",
      "new target:  casual\n",
      "Nans in column:  casual 1672\n",
      "current nan values:  2230\n",
      "\n",
      "Dates with missing values:  330     2011-01-16 12:00:00\n",
      "331     2011-01-16 13:00:00\n",
      "332     2011-01-16 14:00:00\n",
      "333     2011-01-16 15:00:00\n",
      "334     2011-01-16 16:00:00\n",
      "                ...        \n",
      "16593   2012-12-19 19:00:00\n",
      "16594   2012-12-19 20:00:00\n",
      "16595   2012-12-19 21:00:00\n",
      "16596   2012-12-19 22:00:00\n",
      "16597   2012-12-19 23:00:00\n",
      "Name: datetime, Length: 2230, dtype: datetime64[ns]\n",
      "Checking for missing values in column:  casual_original\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# predict the pipelines\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions, dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pipelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitted_pipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmaximum_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[178], line 90\u001b[0m, in \u001b[0;36mpredict_pipelines\u001b[0;34m(fitted_pipelines, training_data, train_columns, test_columns, columns_not_to_use, test_period, maximum_day, directions, rf, mlp)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# assert there are no missing values\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChecking for missing values in column: \u001b[39m\u001b[38;5;124m'\u001b[39m, target)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prediction\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# store the predictions\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               mse         mae\n",
      "negative_casual       13915.927272   77.633585\n",
      "positive_casual       10937.922069   69.677156\n",
      "negative_registered  151020.270429  316.296692\n",
      "positive_registered  163202.314691  325.133307\n",
      "negative_count       260403.141112  397.264568\n",
      "positive_count       242602.738488  378.685935\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pipelines\n",
    "evaluation = evaluate_pipelines(predictions, training_data, test_columns, directions)\n",
    "\n",
    "#TODO fix bugs with different train and test y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
