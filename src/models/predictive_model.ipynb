{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_87480/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_test_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_test_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, rf=True, mlp=False, trees=100, max_iter=2000, hidden_layer_sizes=(100,)):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes, verbose=True)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf = True, mlp = False):\n",
    "\n",
    "    print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_coumns = columns_not_to_use + target_columns\n",
    "            df = df.drop(drop_coumns, axis=1)\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                \n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \"\"\"\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \"\"\"\n",
    "\n",
    "            if mlp:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction):\n",
    "\n",
    "    prediction_array = np.array(df[target])\n",
    "    prediction_array[mask] = prediction\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    df[target] = prediction_array\n",
    "\n",
    "    # make the null values nan\n",
    "    df[target] = df[target].apply(lambda x: x if x != 0 else np.nan)\n",
    "\n",
    "\n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        #print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        #print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "            column_to_insert = target + ', ' + sign + str(abs(lag))\n",
    "            df.loc[datetime_mask, column_to_insert] = prediction_to_store\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                      target_columns, columns_not_to_use, \n",
    "                      test_period, lag_period, maximum_day,\n",
    "                      directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            df = training_data[direction]['y'].copy()\n",
    "\n",
    "            check_columns = ['casual', 'registered', 'count', 'casual_original', 'registered_original', 'count_original']\n",
    "            additional_columns = [col for col in check_columns if col not in columns_not_to_use]\n",
    "            drop_columns = columns_not_to_use + additional_columns\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf and mlp:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf:\n",
    "                    print('drop columns: ', drop_columns)\n",
    "                    print('columns: ', df_days.columns)\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datetime', 'dataset', 'day', 'month', 'year', 'casual_original', 'registered_original', 'count_original']\n",
      "\n",
      "\n",
      "Fitting pipelines...\n",
      "Fitting pipelines for casual\n",
      "Fitting pipelines for registered\n",
      "Fitting pipelines for count\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "drop columns:  ['datetime', 'dataset', 'day', 'month', 'year', 'casual_original', 'registered_original', 'count_original', 'casual', 'registered', 'count']\n",
      "columns:  Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, -1', 'atemp, -1', 'humidity, -1', 'holiday, -1',\n",
      "       'workingday, -1', 'weather, -1', 'temp, -1', 'casual, -1',\n",
      "       'registered, -1', 'count, -1', 'windspeed, -2', 'atemp, -2',\n",
      "       'humidity, -2', 'holiday, -2', 'workingday, -2', 'weather, -2',\n",
      "       'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
      "      dtype='object')\n",
      "drop columns:  ['datetime', 'dataset', 'day', 'month', 'year', 'casual_original', 'registered_original', 'count_original', 'casual', 'registered', 'count']\n",
      "columns:  Index(['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp',\n",
      "       'humidity', 'windspeed', 'hour', 'dayofyear', 'weekofyear', 'dayofweek',\n",
      "       'windspeed, -1', 'atemp, -1', 'humidity, -1', 'holiday, -1',\n",
      "       'workingday, -1', 'weather, -1', 'temp, -1', 'casual, -1',\n",
      "       'registered, -1', 'count, -1', 'windspeed, -2', 'atemp, -2',\n",
      "       'humidity, -2', 'holiday, -2', 'workingday, -2', 'weather, -2',\n",
      "       'temp, -2', 'casual, -2', 'registered, -2', 'count, -2',\n",
      "       'casual_original, -1', 'casual_original, -2'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- casual_original, -1\n- casual_original, -2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m fitted_pipelines \u001b[38;5;241m=\u001b[39m fit_pipelines(pipelines, training_data, \n\u001b[1;32m     33\u001b[0m                                  train_columns, columns_not_to_use,\n\u001b[1;32m     34\u001b[0m                                  directions, rf\u001b[38;5;241m=\u001b[39mrf, mlp\u001b[38;5;241m=\u001b[39mmlp)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# predict the pipelines\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m predictions, dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pipelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfitted_pipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtest_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtest_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximum_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# evaluate the pipelines\u001b[39;00m\n\u001b[1;32m     43\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m evaluate_pipelines(predictions, training_data, test_columns, directions)\n",
      "Cell \u001b[0;32mIn[108], line 62\u001b[0m, in \u001b[0;36mpredict_pipelines\u001b[0;34m(fitted_pipelines, training_data, target_columns, columns_not_to_use, test_period, lag_period, maximum_day, directions, rf, mlp)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop columns: \u001b[39m\u001b[38;5;124m'\u001b[39m, drop_columns)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns: \u001b[39m\u001b[38;5;124m'\u001b[39m, df_days\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m---> 62\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mlp:\n\u001b[1;32m     64\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m pipeline2\u001b[38;5;241m.\u001b[39mpredict(df_days)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:602\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 602\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- casual_original, -1\n- casual_original, -2\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "train_columns = smoothed_columns\n",
    "\n",
    "if 'original' in train_columns[0]:\n",
    "    additional_columns = str(col[:-len('_original')] for col in train_columns)\n",
    "else:\n",
    "    additional_columns = [str(col + '_original') for col in train_columns]\n",
    "columns_not_to_use += additional_columns\n",
    "print(columns_not_to_use)\n",
    "\n",
    "test_columns = original_columns\n",
    "directions = ['negative', 'positive']\n",
    "rf = True\n",
    "mlp = False\n",
    "trees = 15\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = (100, 100)\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             rf=rf, mlp=mlp, trees=trees, \n",
    "                             max_iter = 2000, hidden_layer_sizes = (100, 100))\n",
    "\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                test_columns, columns_not_to_use, \n",
    "                                test_period, lag_period, maximum_day,\n",
    "                                directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation = evaluate_pipelines(predictions, training_data, test_columns, directions)\n",
    "\n",
    "#TODO fix bugs with different train and test y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
