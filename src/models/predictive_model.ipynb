{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_3170/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_test_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_test_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'count_original',\n",
       "       'registered_original', 'casual_original', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, trees, hidden_layer_sizes, max_iter_no_change, rf, mlp, max_iter=2000):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf[i]:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_rf'] = globals()[f'{direction}_{target_name}_pipeline_rf']\n",
    "            \n",
    "            if mlp[i]:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target_name}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes[i], verbose=True, n_iter_no_change=max_iter_no_change[i])),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target_name}_pipeline_mlp'] = globals()[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf, mlp):\n",
    "\n",
    "    #print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for i, target in enumerate(target_columns):\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in target_columns]\n",
    "        else:\n",
    "            target_name_columns = target_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            #drop_columns = columns_not_to_use\n",
    "\n",
    "            df = df.drop(drop_columns, axis=1)\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf[i]:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                \"\"\"\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "                \"\"\"\n",
    "\n",
    "            if mlp[i]:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction, train_columns):\n",
    "    saved_target = target\n",
    "    #print('target: ', saved_target)\n",
    "    # find the column in which the prediction is stored\n",
    "    for col in train_columns:\n",
    "        if target == col:\n",
    "            if 'original' in col:\n",
    "                target = col[:-len('_original')]\n",
    "                cols_to_insert_prediction = [target, col]\n",
    "            else:\n",
    "                cols_to_insert_prediction = [target]\n",
    "        elif target in col:\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "        elif col in target:\n",
    "            target = col\n",
    "            col = saved_target\n",
    "            cols_to_insert_prediction = [col, target]\n",
    "    \n",
    "    \n",
    "    #print('train columns: ', train_columns)\n",
    "    #print('cols to insert prediction: ', cols_to_insert_prediction)\n",
    "    #print('new target: ', target)\n",
    "\n",
    "    prediction_array = np.array(df[saved_target])\n",
    "    #print('length of prediction array: ', len(prediction_array))\n",
    "    prediction_array[mask] = prediction\n",
    "    #print('Nans in prediction array: ', np.isnan(prediction_array).sum())\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    for col in cols_to_insert_prediction:\n",
    "        df[col] = prediction_array\n",
    "        #print('Nans in column: ', col, df[col].isnull().sum())\n",
    "        \n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        #print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        #print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            #print('Inserting prediction for lag: ', lag, ' in coumns: ', cols_to_insert_prediction)\n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print(f'Predictions inserted from datetime: {lagged_datetime[lagged_mask].iloc[0]} to {lagged_datetime[lagged_mask].iloc[-1]}')\n",
    "            #print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "\n",
    "            lagged_col = f'{target}, '+sign+str(abs(lag))\n",
    "            #print('lagged col: ', lagged_col)\n",
    "            df.loc[datetime_mask, lagged_col] = prediction_to_store\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                        train_columns, test_columns, \n",
    "                        columns_not_to_use, test_period, \n",
    "                        maximum_day, directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    df_positive = training_data['positive']['y'].copy()\n",
    "    df_negative = training_data['negative']['y'].copy()\n",
    "    \n",
    "\n",
    "    for i, target in enumerate(test_columns):\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "            target_name_columns = [col[:-len('_original')] for col in train_columns]\n",
    "        else:\n",
    "            target_name_columns = train_columns\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "            #print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            if direction == 'positive':\n",
    "                df = df_positive\n",
    "            elif direction == 'negative':\n",
    "                df = df_negative\n",
    "            \n",
    "            drop_columns = drop_columns = [col for col in columns_not_to_use if col not in target_name_columns[:i]]\n",
    "            #drop_columns = columns_not_to_use\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "            #print('Initial nan values: ', df[target].isna().sum())\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "                #print('Day: ', day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                #print('Number of rows: ', df_days.shape[0])\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "                \n",
    "                # get the pipeline\n",
    "                if rf[i]:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target_name}_pipeline_rf']\n",
    "                if mlp[i]:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target_name}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf[i] and mlp[i]:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf[i]:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp[i]:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                #print('Length of prediction: ', len(prediction))\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction, train_columns)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "                #print('current nan values: ', df[target].isna().sum())\n",
    "                #print('')\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "            #print('\\n\\n')\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target_name}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target_name}'] = df\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "\n",
    "    for target in target_columns:\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines2(predictions, training_data, target_columns, directions):\n",
    "\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "    \n",
    "    data = training_data.copy()\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Evaluating pipelines for {target}')\n",
    "\n",
    "        if 'original' in target:\n",
    "            target_name = target[:-len('_original')]\n",
    "        else:\n",
    "            target_name = target\n",
    "\n",
    "        for direction in directions:\n",
    "            print(f'Direction: {direction}')\n",
    "\n",
    "            # get the y data\n",
    "            y = data[direction]['y'][target].copy()\n",
    "\n",
    "            print(f'Mean of y: {y.mean()}')\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target_name}']\n",
    "            print(f'Mean of prediction: {prediction.mean()}')\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target_name}', 'mae'] = mae\n",
    "\n",
    "            # subsitute the target column with the prediction\n",
    "            data[direction]['y'][target] = prediction\n",
    "\n",
    "            # assert the mse and mae are 0\n",
    "            y = data[direction]['y'][target].copy()\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "            assert mse == 0\n",
    "            assert mae == 0\n",
    "\n",
    "            print('')\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "    return evaluation, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Evaluating pipelines for casual_original\n",
      "Direction: negative\n",
      "Mean of y: 37.272965879265094\n",
      "Mean of prediction: 25.906561679790027\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 36.62788671023965\n",
      "Mean of prediction: 22.790309368191718\n",
      "\n",
      "                         mse        mae\n",
      "negative_casual  1361.708559  17.631741\n",
      "positive_casual  1361.055647  17.716200\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['casual', 'registered', 'count']\n",
    "original_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [25, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "\n",
    "train_columns = ['casual_original']\n",
    "test_columns = ['casual_original']\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, training_data_registered = evaluate_pipelines2(predictions, training_data, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor direction in directions:\\n    for target in test_columns:\\n\\n        saved_target = target\\n        \\n        if 'original' in target:\\n            target = target[:-len('_original')]\\n        \\n        df = training_data[direction]['y'].copy()\\n\\n        #\\xa0find the key of the prediction\\n        for key in predictions.keys():\\n            if direction in key and target in key:\\n                saved_key = key\\n\\n        #\\xa0get the prediction\\n        prediction = predictions[saved_key]\\n\\n        # calculate the mse and mae\\n        mse = mean_squared_error(df[target], prediction)\\n        mae = mean_absolute_error(df[target], prediction)\\n        \\n        print(f'{direction}_{target} mse: {mse}')\\n        print(f'{direction}_{target} mae: {mae}')\\n\\n        #\\xa0store the prediction\\n        df[target] = prediction\\n\\n        #\\xa0store the dataframe\\n        training_data[direction]['y'] = df\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put the predictions into the training data\n",
    "\"\"\"\n",
    "for direction in directions:\n",
    "    for target in test_columns:\n",
    "\n",
    "        saved_target = target\n",
    "        \n",
    "        if 'original' in target:\n",
    "            target = target[:-len('_original')]\n",
    "        \n",
    "        df = training_data[direction]['y'].copy()\n",
    "\n",
    "        # find the key of the prediction\n",
    "        for key in predictions.keys():\n",
    "            if direction in key and target in key:\n",
    "                saved_key = key\n",
    "\n",
    "        # get the prediction\n",
    "        prediction = predictions[saved_key]\n",
    "\n",
    "        # calculate the mse and mae\n",
    "        mse = mean_squared_error(df[target], prediction)\n",
    "        mae = mean_absolute_error(df[target], prediction)\n",
    "        \n",
    "        print(f'{direction}_{target} mse: {mse}')\n",
    "        print(f'{direction}_{target} mae: {mae}')\n",
    "\n",
    "        # store the prediction\n",
    "        df[target] = prediction\n",
    "\n",
    "        # store the dataframe\n",
    "        training_data[direction]['y'] = df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     23\u001b[0m pipelines \u001b[38;5;241m=\u001b[39m create_pipelines(train_columns, directions, \n\u001b[1;32m     24\u001b[0m                              trees, hidden_layer_sizes,\n\u001b[1;32m     25\u001b[0m                              max_iter_no_change, \n\u001b[1;32m     26\u001b[0m                              rf\u001b[38;5;241m=\u001b[39mrf, mlp\u001b[38;5;241m=\u001b[39mmlp, max_iter\u001b[38;5;241m=\u001b[39mmax_iter)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# fit the pipelines\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m fitted_pipelines \u001b[38;5;241m=\u001b[39m \u001b[43mfit_pipelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data_registered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtrain_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_not_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdirections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# predict the pipelines\u001b[39;00m\n\u001b[1;32m     34\u001b[0m predictions, dataframes \u001b[38;5;241m=\u001b[39m predict_pipelines(fitted_pipelines, training_data_registered, \n\u001b[1;32m     35\u001b[0m                                             train_columns, test_columns, \n\u001b[1;32m     36\u001b[0m                                             columns_not_to_use, test_period, \n\u001b[1;32m     37\u001b[0m                                             maximum_day, directions, rf\u001b[38;5;241m=\u001b[39mrf, mlp\u001b[38;5;241m=\u001b[39mmlp)\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mfit_pipelines\u001b[0;34m(pipelines, train_data, target_columns, columns_not_to_use, directions, rf, mlp)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rf[i]:\n\u001b[1;32m     28\u001b[0m     pipeline_rf \u001b[38;5;241m=\u001b[39m pipelines[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pipeline_rf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mpipeline_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# save the fitted pipeline\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     fitted_pipelines[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pipeline_rf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pipeline_rf\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 475\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['registered', 'count']\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data_registered, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data_registered, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count = evaluate_pipelines2(predictions, training_data_registered, test_columns, directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Fitting pipelines for registered_original\n",
      "Fitting pipelines for count_original\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "Predicting pipelines for count_original\n",
      "Evaluating pipelines for casual_original\n",
      "Direction: negative\n",
      "Mean of y: 9.433035870516184\n",
      "Mean of prediction: 9.484663167104111\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 10.896165577342048\n",
      "Mean of prediction: 11.124331154684095\n",
      "\n",
      "Evaluating pipelines for registered_original\n",
      "Direction: negative\n",
      "Mean of y: 82.34912510936132\n",
      "Mean of prediction: 44.67061242344707\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 93.62872331154684\n",
      "Mean of prediction: 43.26994335511982\n",
      "\n",
      "Evaluating pipelines for count_original\n",
      "Direction: negative\n",
      "Mean of y: 195.501312335958\n",
      "Mean of prediction: 17.48771653543307\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 188.6575163398693\n",
      "Mean of prediction: 16.892941176470586\n",
      "\n",
      "                              mse         mae\n",
      "negative_casual          0.716489    0.373115\n",
      "positive_casual          1.126129    0.468444\n",
      "negative_registered   5534.291140   53.295713\n",
      "positive_registered   9364.616505   64.161307\n",
      "negative_count       61078.146464  178.050184\n",
      "positive_count       56973.294929  171.794065\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = ['count']\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count2 = evaluate_pipelines2(predictions, triaining_data_count, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Fitting pipelines for registered_original\n",
      "Fitting pipelines for count_original\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual_original\n",
      "Predicting pipelines for registered_original\n",
      "Predicting pipelines for count_original\n",
      "Evaluating pipelines for casual_original\n",
      "Direction: negative\n",
      "Mean of y: 9.484663167104111\n",
      "Mean of prediction: 9.560034995625546\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 11.124331154684095\n",
      "Mean of prediction: 11.113673202614377\n",
      "\n",
      "Evaluating pipelines for registered_original\n",
      "Direction: negative\n",
      "Mean of y: 44.67061242344707\n",
      "Mean of prediction: 68.79012248468943\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 43.26994335511982\n",
      "Mean of prediction: 62.652514161220054\n",
      "\n",
      "Evaluating pipelines for count_original\n",
      "Direction: negative\n",
      "Mean of y: 17.48771653543307\n",
      "Mean of prediction: 52.95359580052494\n",
      "\n",
      "Direction: positive\n",
      "Mean of y: 16.892941176470586\n",
      "Mean of prediction: 52.89325490196078\n",
      "\n",
      "                             mse        mae\n",
      "negative_casual         0.495257   0.332012\n",
      "positive_casual         0.704797   0.344148\n",
      "negative_registered  1015.871976  25.004357\n",
      "positive_registered   683.271967  20.286553\n",
      "negative_count       7318.812952  35.466089\n",
      "positive_count       7488.634953  36.000767\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [False, False, False]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count2, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count2, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count3 = evaluate_pipelines2(predictions, triaining_data_count2, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipelines for casual_original\n",
      "Iteration 1, loss = 1515.71950226\n",
      "Iteration 2, loss = 378.54602594\n",
      "Iteration 3, loss = 169.93477080\n",
      "Iteration 4, loss = 88.50403649\n",
      "Iteration 5, loss = 57.77562759\n",
      "Iteration 6, loss = 49.76716620\n",
      "Iteration 7, loss = 41.30608738\n",
      "Iteration 8, loss = 37.82606256\n",
      "Iteration 9, loss = 35.03817171\n",
      "Iteration 10, loss = 33.64426181\n",
      "Iteration 11, loss = 32.15309476\n",
      "Iteration 12, loss = 31.39154987\n",
      "Iteration 13, loss = 30.80423806\n",
      "Iteration 14, loss = 30.26691577\n",
      "Iteration 15, loss = 29.62264900\n",
      "Iteration 16, loss = 28.85791981\n",
      "Iteration 17, loss = 29.10130329\n",
      "Iteration 18, loss = 28.68707639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescobraicovich/Documents/Associazioni/BSML/bike_sharing_demand/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1408.97681063\n",
      "Iteration 2, loss = 381.90818877\n",
      "Iteration 3, loss = 185.77418445\n",
      "Iteration 4, loss = 96.52703790\n",
      "Iteration 5, loss = 59.47654574\n",
      "Iteration 6, loss = 45.82443427\n",
      "Iteration 7, loss = 39.74118483\n",
      "Iteration 8, loss = 36.11733120\n",
      "Iteration 9, loss = 34.20884750\n",
      "Iteration 10, loss = 33.83678625\n",
      "Iteration 11, loss = 31.78521374\n",
      "Iteration 12, loss = 31.28817444\n",
      "Iteration 13, loss = 30.64297642\n",
      "Iteration 14, loss = 30.02090689\n",
      "Iteration 15, loss = 29.46912304\n",
      "Iteration 16, loss = 29.09051181\n",
      "Iteration 17, loss = 28.55743750\n",
      "Iteration 18, loss = 28.66613423\n",
      "Iteration 19, loss = 28.25934011\n",
      "Iteration 20, loss = 27.74353709\n",
      "Iteration 21, loss = 28.58940348\n",
      "Iteration 22, loss = 28.18600182\n",
      "Iteration 23, loss = 27.47133133\n",
      "Iteration 24, loss = 27.63400919\n",
      "Iteration 25, loss = 27.62702478\n",
      "Iteration 26, loss = 26.89816492\n",
      "Iteration 27, loss = 27.13628337\n",
      "Iteration 28, loss = 27.09675909\n",
      "Iteration 29, loss = 27.02964138\n",
      "Iteration 30, loss = 26.99039582\n",
      "Iteration 31, loss = 26.38523853\n",
      "Iteration 32, loss = 25.87113520\n",
      "Iteration 33, loss = 26.48128646\n",
      "Iteration 34, loss = 26.29414731\n",
      "Iteration 35, loss = 26.07068983\n",
      "Iteration 36, loss = 25.76387014\n",
      "Iteration 37, loss = 26.99743752\n",
      "Iteration 38, loss = 25.85222371\n",
      "Iteration 39, loss = 25.89097797\n",
      "Iteration 40, loss = 26.41637498\n",
      "Iteration 41, loss = 25.67169833\n",
      "Iteration 42, loss = 26.39013681\n",
      "Iteration 43, loss = 25.95580241\n",
      "Iteration 44, loss = 25.15360570\n",
      "Iteration 45, loss = 26.42672233\n",
      "Iteration 46, loss = 24.97131918\n",
      "Iteration 47, loss = 24.99264827\n",
      "Iteration 48, loss = 25.20624955\n",
      "Iteration 49, loss = 25.39089742\n",
      "Iteration 50, loss = 25.35214654\n",
      "Iteration 51, loss = 25.29872093\n",
      "Iteration 52, loss = 24.71379753\n",
      "Iteration 53, loss = 25.09111079\n",
      "Iteration 54, loss = 25.31180527\n",
      "Iteration 55, loss = 24.96396843\n",
      "Iteration 56, loss = 24.94047085\n",
      "Iteration 57, loss = 25.14915550\n",
      "Iteration 58, loss = 24.36743860\n",
      "Iteration 59, loss = 24.65439374\n",
      "Iteration 60, loss = 24.80015886\n",
      "Iteration 61, loss = 25.08015997\n",
      "Iteration 62, loss = 24.46562122\n",
      "Iteration 63, loss = 24.77753215\n",
      "Iteration 64, loss = 24.78992539\n",
      "Iteration 65, loss = 24.65048459\n",
      "Iteration 66, loss = 24.37675042\n",
      "Iteration 67, loss = 24.51759662\n",
      "Iteration 68, loss = 25.38595427\n",
      "Iteration 69, loss = 25.41351642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting pipelines for registered_original\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month']\n",
    "smoothed_columns = []\n",
    "original_columns = ['registered_original', 'count_original', 'casual_original']\n",
    "\n",
    "columns_not_to_use = columns_not_to_use + smoothed_columns + original_columns\n",
    "\n",
    "directions = ['negative', 'positive']\n",
    "rf = [True, True, True]\n",
    "mlp = [True, True, True]\n",
    "trees = [50, 50, 50]\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = [(100, 150, 50), (250, 350, 350, 150), (1, 1)]\n",
    "max_iter_no_change = [10, 10, 10]\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "train_columns = ['casual_original', 'registered_original', 'count_original']\n",
    "test_columns = train_columns\n",
    "\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(train_columns, directions, \n",
    "                             trees, hidden_layer_sizes,\n",
    "                             max_iter_no_change, \n",
    "                             rf=rf, mlp=mlp, max_iter=max_iter)\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, triaining_data_count3, \n",
    "                                 train_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, triaining_data_count3, \n",
    "                                            train_columns, test_columns, \n",
    "                                            columns_not_to_use, test_period, \n",
    "                                            maximum_day, directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation, triaining_data_count4 = evaluate_pipelines2(predictions, triaining_data_count3, test_columns, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348      34.98\n",
      "349      21.86\n",
      "350      15.92\n",
      "351       8.84\n",
      "352       3.64\n",
      "         ...  \n",
      "17088     7.60\n",
      "17089     7.64\n",
      "17090     7.60\n",
      "17091     7.42\n",
      "17092     6.48\n",
      "Name: count_original, Length: 2286, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print count predictions\n",
    "print(predictions['negative_count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
