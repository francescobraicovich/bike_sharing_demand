{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final data\n",
    "data = pd.read_csv('../../data/processed/final_data.csv')\n",
    "\n",
    "# get the list of columns\n",
    "columns = data.columns\n",
    "positive_columns = [col for col in columns if '+' in col]\n",
    "negative_columns = [col for col in columns if '-' in col]\n",
    "original_columns = [col for col in columns if '+' not in col and '-' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/prwtql5x5sg0mbkxc6508fqm0000gn/T/ipykernel_86133/545332924.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])\n"
     ]
    }
   ],
   "source": [
    "# use only the training data\n",
    "model_training_data = data[data['dataset'] == 'train']\n",
    "\n",
    "# convert the datetime columns to datetime\n",
    "model_training_data['datetime'] = pd.to_datetime(model_training_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# mask the data from day 3 to 15 of each month using 'datetime' column\n",
    "negative_train_mask = model_training_data['datetime'].apply(lambda x: x.day > lag_period and x.day < maximum_day - test_period)\n",
    "negative_test_mask = model_training_data['datetime'].apply(lambda x: x.day >= maximum_day - test_period)\n",
    "\n",
    "# get the negative training data\n",
    "negative_train_data = model_training_data[negative_train_mask][original_columns + negative_columns].copy()\n",
    "negative_test_data = model_training_data[negative_test_mask][original_columns + negative_columns].copy()\n",
    "\n",
    "# maske the data from day 5 to 17 of each month using 'datetime' column\n",
    "positive_train_mask = model_training_data['datetime'].apply(lambda x: x.day > test_period and x.day < maximum_day - lag_period)\n",
    "positive_test_mask = model_training_data['datetime'].apply(lambda x: x.day <= test_period)\n",
    "\n",
    "# get the positive training data\n",
    "positive_train_data = model_training_data[positive_train_mask][original_columns + positive_columns].copy()\n",
    "positive_test_data = model_training_data[positive_test_mask][original_columns + positive_columns].copy()\n",
    "\n",
    "# save the data into a dictionary\n",
    "training_data = {\n",
    "    'positive': {\n",
    "        'X': positive_train_data,\n",
    "        'y': positive_test_data\n",
    "        },\n",
    "    'negative': {\n",
    "        'X': negative_train_data,\n",
    "        'y': negative_test_data\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp',\n",
       "       'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count',\n",
       "       'year', 'month', 'day', 'hour', 'dataset', 'dayofyear', 'weekofyear',\n",
       "       'dayofweek', 'windspeed, -1', 'atemp, -1', 'humidity, -1',\n",
       "       'holiday, -1', 'workingday, -1', 'weather, -1', 'temp, -1',\n",
       "       'casual, -1', 'registered, -1', 'count, -1', 'windspeed, -2',\n",
       "       'atemp, -2', 'humidity, -2', 'holiday, -2', 'workingday, -2',\n",
       "       'weather, -2', 'temp, -2', 'casual, -2', 'registered, -2', 'count, -2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['negative']['X'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each train or test assert there are no NaN values\n",
    "assert not negative_train_data.isnull().values.any()\n",
    "assert not negative_test_data.isnull().values.any()\n",
    "assert not positive_train_data.isnull().values.any()\n",
    "assert not positive_test_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(target_columns, directions, rf=True, mlp=False, trees=100, max_iter=2000, hidden_layer_sizes=(100,)):\n",
    "    # create a dictionary to store the pipelines\n",
    "    pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        for direction in directions:\n",
    "                \n",
    "            if rf:\n",
    "                # create random forest pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target}_pipeline_rf'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', RandomForestRegressor(n_estimators=trees)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target}_pipeline_rf'] = globals()[f'{direction}_{target}_pipeline_rf']\n",
    "            \n",
    "            if mlp:\n",
    "                # create MLP pipeline for the target and direction\n",
    "                globals()[f'{direction}_{target}_pipeline_mlp'] = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', MLPRegressor(max_iter=max_iter, hidden_layer_sizes=hidden_layer_sizes, verbose=True)),\n",
    "                ])\n",
    "\n",
    "                # save the pipeline\n",
    "                pipelines[f'{direction}_{target}_pipeline_mlp'] = globals()[f'{direction}_{target}_pipeline_mlp']\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipelines(pipelines, train_data, target_columns, columns_not_to_use, directions, rf = True, mlp = False):\n",
    "\n",
    "    print('\\n\\nFitting pipelines...')\n",
    "    # create a dictionary to store the fitted pipelines\n",
    "    fitted_pipelines = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Fitting pipelines for {target}')\n",
    "        \n",
    "        for direction in directions:\n",
    "\n",
    "            df = train_data[direction]['X'].copy()\n",
    "            target_data = df[target]\n",
    "            drop_coumns = columns_not_to_use + target_columns\n",
    "            df = df.drop(drop_coumns, axis=1)\n",
    "            \n",
    "            # get the pipelines\n",
    "            if rf:\n",
    "                pipeline_rf = pipelines[f'{direction}_{target}_pipeline_rf']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_rf.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target}_pipeline_rf'] = pipeline_rf\n",
    "\n",
    "                # create a dataframe to store the feature importances\n",
    "                feature_importances = pd.DataFrame({\n",
    "                    'feature': df.columns,\n",
    "                    'importance': pipeline_rf.named_steps['regressor'].feature_importances_\n",
    "                })\n",
    "\n",
    "                # sort the features by importance\n",
    "                feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "                # print the feature importances\n",
    "                #print(f'{direction}_{target}_pipeline_rf')\n",
    "                #print(feature_importances)\n",
    "\n",
    "            if mlp:\n",
    "                pipeline_mlp = pipelines[f'{direction}_{target}_pipeline_mlp']\n",
    "                \n",
    "                # fit the pipeline\n",
    "                pipeline_mlp.fit(df, target_data)\n",
    "\n",
    "                # save the fitted pipeline\n",
    "                fitted_pipelines[f'{direction}_{target}_pipeline_mlp'] = pipeline_mlp\n",
    "\n",
    "    return fitted_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction_with_lags(df, mask, direction, target, prediction):\n",
    "\n",
    "    prediction_array = np.array(df[target])\n",
    "    prediction_array[mask] = prediction\n",
    "\n",
    "    #print('prediction array: ', prediction_array[:5], prediction_array[-5:], '\\n')\n",
    "\n",
    "    # store the prediction array in the dataframe\n",
    "    df[target] = prediction_array\n",
    "\n",
    "    # make the null values nan\n",
    "    df[target] = df[target].apply(lambda x: x if x != 0 else np.nan)\n",
    "\n",
    "\n",
    "    lags = [1, 2]\n",
    "    sign = '-'\n",
    "\n",
    "    if direction == 'positive':\n",
    "        sign = '+'\n",
    "        lags = [-1, -2]\n",
    "\n",
    "    # get the 'datetime' column for the masked data\n",
    "    datetime = df['datetime']\n",
    "    #print('datetime: ', datetime[:5], datetime[-5:], '\\n')\n",
    "    datetime_masked = df[mask]['datetime']\n",
    "\n",
    "    for lag in lags:\n",
    "        #print('lag: ', lag)\n",
    "        # get the lagged 'datetime' column\n",
    "        lagged_datetime = datetime_masked + pd.DateOffset(days=lag)\n",
    "        #print('lagged datetime: ', lagged_datetime[:5], lagged_datetime[-5:], '\\n')\n",
    "        #print('datetime masked: ', datetime_masked[:5], datetime_masked[-5:], '\\n')\n",
    "\n",
    "        # get the mask for lagged time\n",
    "        lagged_mask = lagged_datetime.isin(datetime)\n",
    "        #print('lagged mask: ', lagged_mask[:5], lagged_mask[-5:])\n",
    "        #print('total lagged mask: ', lagged_mask.sum(), '\\n')\n",
    "\n",
    "        # get the mask for the lagged 'datetime' column\n",
    "        datetime_mask =  datetime.isin(lagged_datetime)\n",
    "        #print('datetime mask: ', datetime_mask[:5], datetime_mask[-5:])\n",
    "        #print('total datetime mask: ', datetime_mask.sum(), '\\n')\n",
    "\n",
    "        #print('subset of lagged datetime: ', lagged_datetime[lagged_mask][:5], lagged_datetime[lagged_mask][-5:], '\\n')\n",
    "        #print('subset of datetime: ', datetime[datetime_mask][:5], datetime[datetime_mask][-24:18], '\\n')\n",
    "\n",
    "        # assert the number of elements in the lagged mask is equal to the number of elements in the datetime mask\n",
    "        assert lagged_mask.sum() == datetime_mask.sum()\n",
    "\n",
    "        if lagged_mask.sum() > 0:\n",
    "            \n",
    "            prediction_to_store = prediction[lagged_mask]\n",
    "            #print('prediction to store: ', prediction_to_store[:5], prediction_to_store[-5:], '\\n')\n",
    "            # insert the prediction into the dataframe subsetted by the lagged mask\n",
    "            column_to_insert = target + f', ' + sign + str(abs(lag))\n",
    "            df.loc[datetime_mask, column_to_insert] = prediction_to_store\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipelines(fitted_pipelines, training_data, \n",
    "                      target_columns, columns_not_to_use, \n",
    "                      test_period, lag_period, maximum_day,\n",
    "                      directions, rf=True, mlp=False):\n",
    "    \n",
    "    print('\\n\\nPredicting pipelines...')\n",
    "\n",
    "    # create a dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    dataframes = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f'Predicting pipelines for {target}')\n",
    "\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            df = training_data[direction]['y'].copy()\n",
    "            drop_columns = columns_not_to_use + target_columns\n",
    "\n",
    "            # set the target column to NaN\n",
    "            df[target] = np.nan\n",
    "\n",
    "            if direction == 'negative':\n",
    "                start_day = maximum_day - test_period\n",
    "            elif direction == 'positive':\n",
    "                start_day = test_period\n",
    "\n",
    "            day = start_day\n",
    "            days_predicted = 0\n",
    "\n",
    "            while days_predicted < test_period:\n",
    "                # mask the data to select all rows corresponding to a day of the month equal to start_day\n",
    "                mask = df['datetime'].apply(lambda x: x.day == day)\n",
    "\n",
    "                # get the data\n",
    "                df_days = df[mask].copy()\n",
    "                df_days = df_days.drop(drop_columns, axis=1)\n",
    "\n",
    "                #print('direction: ', direction)\n",
    "                #print('current day: ', day)\n",
    "\n",
    "                # get the pipeline\n",
    "                if rf:\n",
    "                    pipeline1 = fitted_pipelines[f'{direction}_{target}_pipeline_rf']\n",
    "                if mlp:\n",
    "                    pipeline2 = fitted_pipelines[f'{direction}_{target}_pipeline_mlp']\n",
    "\n",
    "                # take the mean of the predictions if both pipelines are used\n",
    "                if rf and mlp:\n",
    "                    prediction1 = pipeline1.predict(df_days)\n",
    "                    prediction2 = pipeline2.predict(df_days)\n",
    "                    prediction = (prediction1 + prediction2) / 2\n",
    "                elif rf:\n",
    "                    prediction = pipeline1.predict(df_days)\n",
    "                elif mlp:\n",
    "                    prediction = pipeline2.predict(df_days)\n",
    "\n",
    "                # assert the lenght of the prediction is equal to the lenght of the mask\n",
    "                assert len(prediction) == np.sum(mask)\n",
    "\n",
    "                # store the prediction\n",
    "                df = store_prediction_with_lags(df, mask, direction, target, prediction)\n",
    "            \n",
    "                if direction == 'negative':\n",
    "                    day += 1\n",
    "\n",
    "                elif direction == 'positive':\n",
    "                    day -= 1\n",
    "\n",
    "                days_predicted += 1\n",
    "\n",
    "            prediction = df[target]\n",
    "            \n",
    "            if prediction.isna().sum() > 0:\n",
    "                # print the dates with missing values\n",
    "                print('Dates with missing values: ', df[df[target].isna()]['datetime'])\n",
    "\n",
    "            # assert there are no missing values\n",
    "            assert not prediction.isna().values.any()\n",
    "\n",
    "            # store the predictions\n",
    "            predictions[f'{direction}_{target}'] = prediction\n",
    "\n",
    "            # store the dataframe\n",
    "            dataframes[f'{direction}_{target}'] = df\n",
    "\n",
    "            print('')\n",
    "\n",
    "    return predictions, dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipelines(predictions, training_data, target_columns, directions):\n",
    "    # create a dictionary to store the evaluation metrics\n",
    "    evaluation = pd.DataFrame()\n",
    "\n",
    "    for target in target_columns:\n",
    "        for direction in directions:\n",
    "\n",
    "            # get the y data\n",
    "            y = training_data[direction]['y'][target]\n",
    "\n",
    "            # get the predictions\n",
    "            prediction = predictions[f'{direction}_{target}']\n",
    "\n",
    "            # calculate the evaluation metrics\n",
    "            mse = mean_squared_error(y, prediction)\n",
    "            mae = mean_absolute_error(y, prediction)\n",
    "\n",
    "            # save the evaluation metrics\n",
    "            evaluation.loc[f'{direction}_{target}', 'mse'] = mse\n",
    "            evaluation.loc[f'{direction}_{target}', 'mae'] = mae\n",
    "\n",
    "    # print the evaluation metrics\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fitting pipelines...\n",
      "Fitting pipelines for casual\n",
      "Iteration 1, loss = 1544.83379691\n",
      "Iteration 2, loss = 675.52834746\n",
      "Iteration 3, loss = 384.00208550\n",
      "Iteration 4, loss = 329.69408241\n",
      "Iteration 5, loss = 298.56312455\n",
      "Iteration 6, loss = 274.95873774\n",
      "Iteration 7, loss = 253.54756724\n",
      "Iteration 8, loss = 234.81933949\n",
      "Iteration 9, loss = 221.21471040\n",
      "Iteration 10, loss = 209.36150625\n",
      "Iteration 11, loss = 199.33020793\n",
      "Iteration 12, loss = 192.08694646\n",
      "Iteration 13, loss = 183.55975686\n",
      "Iteration 14, loss = 178.11026499\n",
      "Iteration 15, loss = 170.17018584\n",
      "Iteration 16, loss = 164.39749495\n",
      "Iteration 17, loss = 159.60177871\n",
      "Iteration 18, loss = 153.95642412\n",
      "Iteration 19, loss = 149.73092490\n",
      "Iteration 20, loss = 146.01457359\n",
      "Iteration 21, loss = 141.87878105\n",
      "Iteration 22, loss = 137.75503434\n",
      "Iteration 23, loss = 135.04132455\n",
      "Iteration 24, loss = 131.78471659\n",
      "Iteration 25, loss = 127.85079858\n",
      "Iteration 26, loss = 124.82721377\n",
      "Iteration 27, loss = 122.42959334\n",
      "Iteration 28, loss = 119.75745790\n",
      "Iteration 29, loss = 117.84020766\n",
      "Iteration 30, loss = 113.52734340\n",
      "Iteration 31, loss = 112.25498072\n",
      "Iteration 32, loss = 111.68941516\n",
      "Iteration 33, loss = 108.15429318\n",
      "Iteration 34, loss = 104.46872800\n",
      "Iteration 35, loss = 103.36950277\n",
      "Iteration 36, loss = 99.79297154\n",
      "Iteration 37, loss = 98.12209558\n",
      "Iteration 38, loss = 95.06550908\n",
      "Iteration 39, loss = 93.51620773\n",
      "Iteration 40, loss = 91.38682306\n",
      "Iteration 41, loss = 89.37797103\n",
      "Iteration 42, loss = 87.35681197\n",
      "Iteration 43, loss = 85.60401713\n",
      "Iteration 44, loss = 83.67683883\n",
      "Iteration 45, loss = 82.57726241\n",
      "Iteration 46, loss = 81.21106176\n",
      "Iteration 47, loss = 79.83460795\n",
      "Iteration 48, loss = 76.98685537\n",
      "Iteration 49, loss = 77.32738837\n",
      "Iteration 50, loss = 74.57300515\n",
      "Iteration 51, loss = 72.39400435\n",
      "Iteration 52, loss = 71.29342017\n",
      "Iteration 53, loss = 69.92623988\n",
      "Iteration 54, loss = 70.05823152\n",
      "Iteration 55, loss = 69.23409448\n",
      "Iteration 56, loss = 66.45300822\n",
      "Iteration 57, loss = 65.50970148\n",
      "Iteration 58, loss = 65.28739208\n",
      "Iteration 59, loss = 63.53783352\n",
      "Iteration 60, loss = 62.48075555\n",
      "Iteration 61, loss = 61.62074872\n",
      "Iteration 62, loss = 60.26065884\n",
      "Iteration 63, loss = 59.67156182\n",
      "Iteration 64, loss = 58.47396814\n",
      "Iteration 65, loss = 57.30647353\n",
      "Iteration 66, loss = 57.56692971\n",
      "Iteration 67, loss = 56.04560942\n",
      "Iteration 68, loss = 54.85506055\n",
      "Iteration 69, loss = 54.75181257\n",
      "Iteration 70, loss = 53.98427509\n",
      "Iteration 71, loss = 53.59754257\n",
      "Iteration 72, loss = 53.12016836\n",
      "Iteration 73, loss = 52.54242109\n",
      "Iteration 74, loss = 51.92522897\n",
      "Iteration 75, loss = 50.55037276\n",
      "Iteration 76, loss = 49.61386584\n",
      "Iteration 77, loss = 49.03900568\n",
      "Iteration 78, loss = 49.39123390\n",
      "Iteration 79, loss = 47.59638528\n",
      "Iteration 80, loss = 47.42074025\n",
      "Iteration 81, loss = 47.52672363\n",
      "Iteration 82, loss = 46.14480833\n",
      "Iteration 83, loss = 45.65143104\n",
      "Iteration 84, loss = 45.19747643\n",
      "Iteration 85, loss = 44.24611117\n",
      "Iteration 86, loss = 43.92257524\n",
      "Iteration 87, loss = 43.89563427\n",
      "Iteration 88, loss = 42.86874536\n",
      "Iteration 89, loss = 42.21543201\n",
      "Iteration 90, loss = 42.56865050\n",
      "Iteration 91, loss = 41.93125986\n",
      "Iteration 92, loss = 41.39086180\n",
      "Iteration 93, loss = 41.03039213\n",
      "Iteration 94, loss = 40.40392483\n",
      "Iteration 95, loss = 39.90783990\n",
      "Iteration 96, loss = 39.09082725\n",
      "Iteration 97, loss = 38.89424037\n",
      "Iteration 98, loss = 38.24266217\n",
      "Iteration 99, loss = 38.02827211\n",
      "Iteration 100, loss = 37.25733999\n",
      "Iteration 101, loss = 36.58651130\n",
      "Iteration 102, loss = 36.60555234\n",
      "Iteration 103, loss = 36.08007251\n",
      "Iteration 104, loss = 35.40079032\n",
      "Iteration 105, loss = 36.02365843\n",
      "Iteration 106, loss = 35.07617814\n",
      "Iteration 107, loss = 34.63972870\n",
      "Iteration 108, loss = 34.56301157\n",
      "Iteration 109, loss = 34.83934922\n",
      "Iteration 110, loss = 33.48965957\n",
      "Iteration 111, loss = 33.70459472\n",
      "Iteration 112, loss = 33.03244130\n",
      "Iteration 113, loss = 32.11102312\n",
      "Iteration 114, loss = 32.53106191\n",
      "Iteration 115, loss = 32.15914195\n",
      "Iteration 116, loss = 31.45180161\n",
      "Iteration 117, loss = 31.53952809\n",
      "Iteration 118, loss = 30.80486456\n",
      "Iteration 119, loss = 31.31125705\n",
      "Iteration 120, loss = 30.41775890\n",
      "Iteration 121, loss = 29.74333228\n",
      "Iteration 122, loss = 29.78780593\n",
      "Iteration 123, loss = 29.04779179\n",
      "Iteration 124, loss = 29.61057104\n",
      "Iteration 125, loss = 29.38660565\n",
      "Iteration 126, loss = 28.85443106\n",
      "Iteration 127, loss = 28.18969049\n",
      "Iteration 128, loss = 28.25170871\n",
      "Iteration 129, loss = 27.38924228\n",
      "Iteration 130, loss = 27.20187789\n",
      "Iteration 131, loss = 27.40421104\n",
      "Iteration 132, loss = 27.39287156\n",
      "Iteration 133, loss = 26.43006584\n",
      "Iteration 134, loss = 26.36986156\n",
      "Iteration 135, loss = 26.29175621\n",
      "Iteration 136, loss = 26.68794745\n",
      "Iteration 137, loss = 26.12446876\n",
      "Iteration 138, loss = 25.60521987\n",
      "Iteration 139, loss = 26.11631305\n",
      "Iteration 140, loss = 25.91134291\n",
      "Iteration 141, loss = 25.36063096\n",
      "Iteration 142, loss = 24.31605743\n",
      "Iteration 143, loss = 24.51759156\n",
      "Iteration 144, loss = 25.33443554\n",
      "Iteration 145, loss = 25.07897759\n",
      "Iteration 146, loss = 23.94327065\n",
      "Iteration 147, loss = 23.80735787\n",
      "Iteration 148, loss = 23.67583171\n",
      "Iteration 149, loss = 23.78824702\n",
      "Iteration 150, loss = 22.85131237\n",
      "Iteration 151, loss = 23.70543223\n",
      "Iteration 152, loss = 23.00242376\n",
      "Iteration 153, loss = 22.55998370\n",
      "Iteration 154, loss = 22.95568297\n",
      "Iteration 155, loss = 22.16490725\n",
      "Iteration 156, loss = 21.87369589\n",
      "Iteration 157, loss = 21.92362934\n",
      "Iteration 158, loss = 21.60186997\n",
      "Iteration 159, loss = 21.84523223\n",
      "Iteration 160, loss = 21.90584521\n",
      "Iteration 161, loss = 21.17815283\n",
      "Iteration 162, loss = 21.22401327\n",
      "Iteration 163, loss = 20.79442230\n",
      "Iteration 164, loss = 20.69357234\n",
      "Iteration 165, loss = 20.44329819\n",
      "Iteration 166, loss = 20.50858245\n",
      "Iteration 167, loss = 20.40345368\n",
      "Iteration 168, loss = 20.51099822\n",
      "Iteration 169, loss = 19.86598059\n",
      "Iteration 170, loss = 20.11710883\n",
      "Iteration 171, loss = 20.51435530\n",
      "Iteration 172, loss = 19.89685594\n",
      "Iteration 173, loss = 19.55332734\n",
      "Iteration 174, loss = 19.27638159\n",
      "Iteration 175, loss = 19.14162939\n",
      "Iteration 176, loss = 19.31961819\n",
      "Iteration 177, loss = 19.07168542\n",
      "Iteration 178, loss = 18.75649830\n",
      "Iteration 179, loss = 18.64021322\n",
      "Iteration 180, loss = 18.86404422\n",
      "Iteration 181, loss = 18.41578632\n",
      "Iteration 182, loss = 19.01158800\n",
      "Iteration 183, loss = 18.36396153\n",
      "Iteration 184, loss = 18.11068695\n",
      "Iteration 185, loss = 18.58722095\n",
      "Iteration 186, loss = 17.78190179\n",
      "Iteration 187, loss = 17.61816369\n",
      "Iteration 188, loss = 17.95848996\n",
      "Iteration 189, loss = 18.25953858\n",
      "Iteration 190, loss = 17.58757689\n",
      "Iteration 191, loss = 17.21627873\n",
      "Iteration 192, loss = 16.83979792\n",
      "Iteration 193, loss = 17.48080834\n",
      "Iteration 194, loss = 17.10199462\n",
      "Iteration 195, loss = 16.64874402\n",
      "Iteration 196, loss = 17.24640542\n",
      "Iteration 197, loss = 16.92535381\n",
      "Iteration 198, loss = 16.93918510\n",
      "Iteration 199, loss = 16.77590906\n",
      "Iteration 200, loss = 16.92129507\n",
      "Iteration 201, loss = 16.16293685\n",
      "Iteration 202, loss = 15.93089661\n",
      "Iteration 203, loss = 16.01748247\n",
      "Iteration 204, loss = 15.96950767\n",
      "Iteration 205, loss = 16.04758790\n",
      "Iteration 206, loss = 15.94260488\n",
      "Iteration 207, loss = 16.18372360\n",
      "Iteration 208, loss = 15.64487636\n",
      "Iteration 209, loss = 15.43536647\n",
      "Iteration 210, loss = 15.59271858\n",
      "Iteration 211, loss = 15.45772619\n",
      "Iteration 212, loss = 15.71658877\n",
      "Iteration 213, loss = 15.15896326\n",
      "Iteration 214, loss = 14.86407800\n",
      "Iteration 215, loss = 15.17051693\n",
      "Iteration 216, loss = 15.30064119\n",
      "Iteration 217, loss = 15.00741299\n",
      "Iteration 218, loss = 15.01504355\n",
      "Iteration 219, loss = 14.80685742\n",
      "Iteration 220, loss = 14.93095643\n",
      "Iteration 221, loss = 14.31196653\n",
      "Iteration 222, loss = 15.22934009\n",
      "Iteration 223, loss = 15.22637206\n",
      "Iteration 224, loss = 14.83821567\n",
      "Iteration 225, loss = 14.36074585\n",
      "Iteration 226, loss = 14.17269276\n",
      "Iteration 227, loss = 13.72205841\n",
      "Iteration 228, loss = 13.92189605\n",
      "Iteration 229, loss = 14.10252410\n",
      "Iteration 230, loss = 13.92221507\n",
      "Iteration 231, loss = 13.65514853\n",
      "Iteration 232, loss = 13.39537235\n",
      "Iteration 233, loss = 13.39751097\n",
      "Iteration 234, loss = 13.79615007\n",
      "Iteration 235, loss = 13.75228912\n",
      "Iteration 236, loss = 13.45976394\n",
      "Iteration 237, loss = 13.48013607\n",
      "Iteration 238, loss = 13.19493902\n",
      "Iteration 239, loss = 13.33799081\n",
      "Iteration 240, loss = 13.55639921\n",
      "Iteration 241, loss = 13.25863354\n",
      "Iteration 242, loss = 14.17545000\n",
      "Iteration 243, loss = 13.50758498\n",
      "Iteration 244, loss = 13.01729343\n",
      "Iteration 245, loss = 13.05146781\n",
      "Iteration 246, loss = 12.91614315\n",
      "Iteration 247, loss = 12.63190535\n",
      "Iteration 248, loss = 12.64531245\n",
      "Iteration 249, loss = 12.79504383\n",
      "Iteration 250, loss = 12.42456753\n",
      "Iteration 251, loss = 12.27672027\n",
      "Iteration 252, loss = 12.40040980\n",
      "Iteration 253, loss = 12.48764505\n",
      "Iteration 254, loss = 12.36723454\n",
      "Iteration 255, loss = 12.23472462\n",
      "Iteration 256, loss = 12.53247392\n",
      "Iteration 257, loss = 12.28975817\n",
      "Iteration 258, loss = 12.27195381\n",
      "Iteration 259, loss = 12.06675511\n",
      "Iteration 260, loss = 11.96302178\n",
      "Iteration 261, loss = 12.32949292\n",
      "Iteration 262, loss = 11.80102064\n",
      "Iteration 263, loss = 11.81867871\n",
      "Iteration 264, loss = 11.64017349\n",
      "Iteration 265, loss = 11.54396234\n",
      "Iteration 266, loss = 12.01432228\n",
      "Iteration 267, loss = 12.00291614\n",
      "Iteration 268, loss = 11.74725975\n",
      "Iteration 269, loss = 11.87204420\n",
      "Iteration 270, loss = 11.59929129\n",
      "Iteration 271, loss = 11.84184164\n",
      "Iteration 272, loss = 11.64535590\n",
      "Iteration 273, loss = 12.06205973\n",
      "Iteration 274, loss = 11.11533392\n",
      "Iteration 275, loss = 11.28867219\n",
      "Iteration 276, loss = 11.28693168\n",
      "Iteration 277, loss = 11.16985489\n",
      "Iteration 278, loss = 10.99705523\n",
      "Iteration 279, loss = 11.15999018\n",
      "Iteration 280, loss = 10.78248193\n",
      "Iteration 281, loss = 11.35179077\n",
      "Iteration 282, loss = 11.17468896\n",
      "Iteration 283, loss = 11.68297277\n",
      "Iteration 284, loss = 11.15308303\n",
      "Iteration 285, loss = 10.68745422\n",
      "Iteration 286, loss = 10.96325070\n",
      "Iteration 287, loss = 12.05638600\n",
      "Iteration 288, loss = 10.91336875\n",
      "Iteration 289, loss = 10.95393958\n",
      "Iteration 290, loss = 10.67129396\n",
      "Iteration 291, loss = 10.50910429\n",
      "Iteration 292, loss = 10.78002369\n",
      "Iteration 293, loss = 10.19876578\n",
      "Iteration 294, loss = 10.42242163\n",
      "Iteration 295, loss = 10.34858239\n",
      "Iteration 296, loss = 10.37548835\n",
      "Iteration 297, loss = 10.51435384\n",
      "Iteration 298, loss = 10.30594543\n",
      "Iteration 299, loss = 10.35678746\n",
      "Iteration 300, loss = 10.61792789\n",
      "Iteration 301, loss = 10.18748098\n",
      "Iteration 302, loss = 10.23423618\n",
      "Iteration 303, loss = 10.46296218\n",
      "Iteration 304, loss = 10.30899496\n",
      "Iteration 305, loss = 10.07393941\n",
      "Iteration 306, loss = 9.99567689\n",
      "Iteration 307, loss = 10.19671683\n",
      "Iteration 308, loss = 10.52746551\n",
      "Iteration 309, loss = 9.85018611\n",
      "Iteration 310, loss = 9.66380512\n",
      "Iteration 311, loss = 9.87770825\n",
      "Iteration 312, loss = 9.79333940\n",
      "Iteration 313, loss = 9.84637824\n",
      "Iteration 314, loss = 9.70374527\n",
      "Iteration 315, loss = 9.43105001\n",
      "Iteration 316, loss = 9.59048390\n",
      "Iteration 317, loss = 9.56990939\n",
      "Iteration 318, loss = 9.47356027\n",
      "Iteration 319, loss = 9.48663929\n",
      "Iteration 320, loss = 9.39462890\n",
      "Iteration 321, loss = 9.54257639\n",
      "Iteration 322, loss = 9.79675487\n",
      "Iteration 323, loss = 9.35566355\n",
      "Iteration 324, loss = 9.65390955\n",
      "Iteration 325, loss = 9.55614381\n",
      "Iteration 326, loss = 9.13393473\n",
      "Iteration 327, loss = 9.31161165\n",
      "Iteration 328, loss = 9.23505090\n",
      "Iteration 329, loss = 9.18584210\n",
      "Iteration 330, loss = 9.54444048\n",
      "Iteration 331, loss = 9.50329380\n",
      "Iteration 332, loss = 9.25859020\n",
      "Iteration 333, loss = 9.19559359\n",
      "Iteration 334, loss = 9.08346722\n",
      "Iteration 335, loss = 9.06837831\n",
      "Iteration 336, loss = 8.90584710\n",
      "Iteration 337, loss = 8.87113084\n",
      "Iteration 338, loss = 9.01535022\n",
      "Iteration 339, loss = 8.78422757\n",
      "Iteration 340, loss = 8.95491027\n",
      "Iteration 341, loss = 8.75141440\n",
      "Iteration 342, loss = 8.82265743\n",
      "Iteration 343, loss = 9.03590739\n",
      "Iteration 344, loss = 8.80447977\n",
      "Iteration 345, loss = 8.93378109\n",
      "Iteration 346, loss = 8.67879644\n",
      "Iteration 347, loss = 8.80387741\n",
      "Iteration 348, loss = 8.89757823\n",
      "Iteration 349, loss = 8.71532908\n",
      "Iteration 350, loss = 8.61103692\n",
      "Iteration 351, loss = 8.38216039\n",
      "Iteration 352, loss = 8.34787270\n",
      "Iteration 353, loss = 8.65888011\n",
      "Iteration 354, loss = 8.49483691\n",
      "Iteration 355, loss = 8.50975463\n",
      "Iteration 356, loss = 8.43714556\n",
      "Iteration 357, loss = 8.74849329\n",
      "Iteration 358, loss = 8.22442356\n",
      "Iteration 359, loss = 8.10661941\n",
      "Iteration 360, loss = 8.48352920\n",
      "Iteration 361, loss = 8.13792326\n",
      "Iteration 362, loss = 8.11792802\n",
      "Iteration 363, loss = 8.01610879\n",
      "Iteration 364, loss = 8.34166349\n",
      "Iteration 365, loss = 8.16210386\n",
      "Iteration 366, loss = 8.47511872\n",
      "Iteration 367, loss = 8.38234838\n",
      "Iteration 368, loss = 8.48420166\n",
      "Iteration 369, loss = 8.15967553\n",
      "Iteration 370, loss = 8.27616451\n",
      "Iteration 371, loss = 7.92075031\n",
      "Iteration 372, loss = 8.05633547\n",
      "Iteration 373, loss = 7.84939652\n",
      "Iteration 374, loss = 8.02179896\n",
      "Iteration 375, loss = 7.81644029\n",
      "Iteration 376, loss = 7.78547224\n",
      "Iteration 377, loss = 8.36881369\n",
      "Iteration 378, loss = 7.81169828\n",
      "Iteration 379, loss = 7.61902940\n",
      "Iteration 380, loss = 7.87441778\n",
      "Iteration 381, loss = 7.55682584\n",
      "Iteration 382, loss = 7.61385990\n",
      "Iteration 383, loss = 7.66554803\n",
      "Iteration 384, loss = 7.56209232\n",
      "Iteration 385, loss = 8.09813030\n",
      "Iteration 386, loss = 7.82462082\n",
      "Iteration 387, loss = 7.86554846\n",
      "Iteration 388, loss = 7.39595241\n",
      "Iteration 389, loss = 7.72061585\n",
      "Iteration 390, loss = 7.60863698\n",
      "Iteration 391, loss = 7.56792692\n",
      "Iteration 392, loss = 7.53414911\n",
      "Iteration 393, loss = 7.44551459\n",
      "Iteration 394, loss = 7.33639463\n",
      "Iteration 395, loss = 7.54718873\n",
      "Iteration 396, loss = 7.42109030\n",
      "Iteration 397, loss = 7.62632510\n",
      "Iteration 398, loss = 7.46397567\n",
      "Iteration 399, loss = 7.21619339\n",
      "Iteration 400, loss = 7.32097638\n",
      "Iteration 401, loss = 7.48111867\n",
      "Iteration 402, loss = 7.40788025\n",
      "Iteration 403, loss = 7.22219860\n",
      "Iteration 404, loss = 7.27096425\n",
      "Iteration 405, loss = 7.53843659\n",
      "Iteration 406, loss = 7.32490474\n",
      "Iteration 407, loss = 7.25648649\n",
      "Iteration 408, loss = 7.23201768\n",
      "Iteration 409, loss = 7.19533016\n",
      "Iteration 410, loss = 7.51814757\n",
      "Iteration 411, loss = 7.11103060\n",
      "Iteration 412, loss = 7.19261954\n",
      "Iteration 413, loss = 7.13629919\n",
      "Iteration 414, loss = 6.97341048\n",
      "Iteration 415, loss = 6.86021215\n",
      "Iteration 416, loss = 7.08393331\n",
      "Iteration 417, loss = 6.87781570\n",
      "Iteration 418, loss = 6.84821990\n",
      "Iteration 419, loss = 7.01175707\n",
      "Iteration 420, loss = 7.17137442\n",
      "Iteration 421, loss = 6.94319814\n",
      "Iteration 422, loss = 7.01320426\n",
      "Iteration 423, loss = 6.80236024\n",
      "Iteration 424, loss = 7.01135866\n",
      "Iteration 425, loss = 7.17617758\n",
      "Iteration 426, loss = 7.37713960\n",
      "Iteration 427, loss = 6.96808576\n",
      "Iteration 428, loss = 6.96889796\n",
      "Iteration 429, loss = 6.93175890\n",
      "Iteration 430, loss = 6.84229451\n",
      "Iteration 431, loss = 7.00395447\n",
      "Iteration 432, loss = 7.27871223\n",
      "Iteration 433, loss = 6.76600938\n",
      "Iteration 434, loss = 6.90637600\n",
      "Iteration 435, loss = 6.91051590\n",
      "Iteration 436, loss = 6.72905814\n",
      "Iteration 437, loss = 6.70568836\n",
      "Iteration 438, loss = 6.78539723\n",
      "Iteration 439, loss = 6.62837146\n",
      "Iteration 440, loss = 6.94836975\n",
      "Iteration 441, loss = 7.08828638\n",
      "Iteration 442, loss = 6.31186699\n",
      "Iteration 443, loss = 6.49842532\n",
      "Iteration 444, loss = 6.48894784\n",
      "Iteration 445, loss = 6.55518156\n",
      "Iteration 446, loss = 6.50845355\n",
      "Iteration 447, loss = 6.95977532\n",
      "Iteration 448, loss = 6.50638171\n",
      "Iteration 449, loss = 6.40025297\n",
      "Iteration 450, loss = 6.36989160\n",
      "Iteration 451, loss = 6.35518657\n",
      "Iteration 452, loss = 6.32047361\n",
      "Iteration 453, loss = 6.43953462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1539.68991945\n",
      "Iteration 2, loss = 699.55111490\n",
      "Iteration 3, loss = 370.37538387\n",
      "Iteration 4, loss = 308.09570639\n",
      "Iteration 5, loss = 278.14227810\n",
      "Iteration 6, loss = 255.47233180\n",
      "Iteration 7, loss = 234.74440957\n",
      "Iteration 8, loss = 217.11518581\n",
      "Iteration 9, loss = 203.18813294\n",
      "Iteration 10, loss = 190.78396404\n",
      "Iteration 11, loss = 180.21921477\n",
      "Iteration 12, loss = 170.32157829\n",
      "Iteration 13, loss = 161.76449200\n",
      "Iteration 14, loss = 154.24506438\n",
      "Iteration 15, loss = 147.35224009\n",
      "Iteration 16, loss = 140.63258835\n",
      "Iteration 17, loss = 135.91190620\n",
      "Iteration 18, loss = 130.74544718\n",
      "Iteration 19, loss = 126.86132011\n",
      "Iteration 20, loss = 124.39805349\n",
      "Iteration 21, loss = 119.96941723\n",
      "Iteration 22, loss = 116.03753311\n",
      "Iteration 23, loss = 113.53571593\n",
      "Iteration 24, loss = 110.64765476\n",
      "Iteration 25, loss = 108.98381927\n",
      "Iteration 26, loss = 106.79896298\n",
      "Iteration 27, loss = 103.40086143\n",
      "Iteration 28, loss = 101.01596748\n",
      "Iteration 29, loss = 98.95114038\n",
      "Iteration 30, loss = 96.32052680\n",
      "Iteration 31, loss = 95.04984850\n",
      "Iteration 32, loss = 92.12307036\n",
      "Iteration 33, loss = 89.77886226\n",
      "Iteration 34, loss = 87.78011948\n",
      "Iteration 35, loss = 85.77703989\n",
      "Iteration 36, loss = 84.11569295\n",
      "Iteration 37, loss = 82.13518639\n",
      "Iteration 38, loss = 80.95689355\n",
      "Iteration 39, loss = 78.60994586\n",
      "Iteration 40, loss = 77.62400476\n",
      "Iteration 41, loss = 75.33900264\n",
      "Iteration 42, loss = 73.58399885\n",
      "Iteration 43, loss = 72.28215310\n",
      "Iteration 44, loss = 70.83555594\n",
      "Iteration 45, loss = 69.00217874\n",
      "Iteration 46, loss = 67.20771256\n",
      "Iteration 47, loss = 66.11864954\n",
      "Iteration 48, loss = 65.79751548\n",
      "Iteration 49, loss = 63.82674305\n",
      "Iteration 50, loss = 62.58319589\n",
      "Iteration 51, loss = 61.47598826\n",
      "Iteration 52, loss = 60.47433975\n",
      "Iteration 53, loss = 58.68529118\n",
      "Iteration 54, loss = 57.58594650\n",
      "Iteration 55, loss = 57.22207640\n",
      "Iteration 56, loss = 55.37009053\n",
      "Iteration 57, loss = 55.00726739\n",
      "Iteration 58, loss = 53.79149867\n",
      "Iteration 59, loss = 52.94518571\n",
      "Iteration 60, loss = 51.92062188\n",
      "Iteration 61, loss = 51.85280271\n",
      "Iteration 62, loss = 50.88247945\n",
      "Iteration 63, loss = 49.64705068\n",
      "Iteration 64, loss = 49.28025528\n",
      "Iteration 65, loss = 48.36879822\n",
      "Iteration 66, loss = 48.27186988\n",
      "Iteration 67, loss = 47.41579930\n",
      "Iteration 68, loss = 46.48216566\n",
      "Iteration 69, loss = 45.07731715\n",
      "Iteration 70, loss = 45.14924016\n",
      "Iteration 71, loss = 43.83806023\n",
      "Iteration 72, loss = 44.67367122\n",
      "Iteration 73, loss = 44.06712608\n",
      "Iteration 74, loss = 42.54825721\n",
      "Iteration 75, loss = 41.82790825\n",
      "Iteration 76, loss = 41.44870777\n",
      "Iteration 77, loss = 41.01962917\n",
      "Iteration 78, loss = 40.27144036\n",
      "Iteration 79, loss = 39.61767037\n",
      "Iteration 80, loss = 39.30047082\n",
      "Iteration 81, loss = 39.04735932\n",
      "Iteration 82, loss = 38.75869372\n",
      "Iteration 83, loss = 39.73296272\n",
      "Iteration 84, loss = 37.16160719\n",
      "Iteration 85, loss = 37.25697332\n",
      "Iteration 86, loss = 36.55472886\n",
      "Iteration 87, loss = 36.07927590\n",
      "Iteration 88, loss = 35.85124290\n",
      "Iteration 89, loss = 35.50373269\n",
      "Iteration 90, loss = 34.95340836\n",
      "Iteration 91, loss = 34.58258828\n",
      "Iteration 92, loss = 34.42206405\n",
      "Iteration 93, loss = 34.43854365\n",
      "Iteration 94, loss = 34.16765288\n",
      "Iteration 95, loss = 35.32037241\n",
      "Iteration 96, loss = 33.07198611\n",
      "Iteration 97, loss = 34.25730878\n",
      "Iteration 98, loss = 33.60425542\n",
      "Iteration 99, loss = 32.17044244\n",
      "Iteration 100, loss = 32.02412078\n",
      "Iteration 101, loss = 31.43161995\n",
      "Iteration 102, loss = 31.21947012\n",
      "Iteration 103, loss = 30.74204443\n",
      "Iteration 104, loss = 30.51741664\n",
      "Iteration 105, loss = 30.03020369\n",
      "Iteration 106, loss = 30.02838155\n",
      "Iteration 107, loss = 29.97088493\n",
      "Iteration 108, loss = 29.40803309\n",
      "Iteration 109, loss = 29.33967666\n",
      "Iteration 110, loss = 28.53209603\n",
      "Iteration 111, loss = 28.66562191\n",
      "Iteration 112, loss = 28.13786257\n",
      "Iteration 113, loss = 27.74397804\n",
      "Iteration 114, loss = 28.06703361\n",
      "Iteration 115, loss = 27.99266058\n",
      "Iteration 116, loss = 26.88480957\n",
      "Iteration 117, loss = 27.08767364\n",
      "Iteration 118, loss = 27.23650352\n",
      "Iteration 119, loss = 27.38462260\n",
      "Iteration 120, loss = 27.03115599\n",
      "Iteration 121, loss = 26.90694894\n",
      "Iteration 122, loss = 25.66954183\n",
      "Iteration 123, loss = 26.10077947\n",
      "Iteration 124, loss = 25.84079287\n",
      "Iteration 125, loss = 25.18143287\n",
      "Iteration 126, loss = 25.25057053\n",
      "Iteration 127, loss = 24.71283925\n",
      "Iteration 128, loss = 24.19137601\n",
      "Iteration 129, loss = 23.83832036\n",
      "Iteration 130, loss = 24.07359965\n",
      "Iteration 131, loss = 23.87660723\n",
      "Iteration 132, loss = 23.75587727\n",
      "Iteration 133, loss = 23.63502660\n",
      "Iteration 134, loss = 23.13380579\n",
      "Iteration 135, loss = 22.69241870\n",
      "Iteration 136, loss = 22.53956759\n",
      "Iteration 137, loss = 22.47227262\n",
      "Iteration 138, loss = 22.92730204\n",
      "Iteration 139, loss = 22.09861762\n",
      "Iteration 140, loss = 22.36394771\n",
      "Iteration 141, loss = 22.21553172\n",
      "Iteration 142, loss = 21.85887048\n",
      "Iteration 143, loss = 21.39719853\n",
      "Iteration 144, loss = 21.27633889\n",
      "Iteration 145, loss = 21.24703636\n",
      "Iteration 146, loss = 21.00364206\n",
      "Iteration 147, loss = 21.08880383\n",
      "Iteration 148, loss = 21.54289546\n",
      "Iteration 149, loss = 21.65897242\n",
      "Iteration 150, loss = 20.08502617\n",
      "Iteration 151, loss = 20.40653539\n",
      "Iteration 152, loss = 20.10540702\n",
      "Iteration 153, loss = 20.04248715\n",
      "Iteration 154, loss = 19.67508998\n",
      "Iteration 155, loss = 20.08674372\n",
      "Iteration 156, loss = 19.77428344\n",
      "Iteration 157, loss = 19.97285719\n",
      "Iteration 158, loss = 19.45375271\n",
      "Iteration 159, loss = 19.68100486\n",
      "Iteration 160, loss = 18.86548615\n",
      "Iteration 161, loss = 18.58831196\n",
      "Iteration 162, loss = 19.82983119\n",
      "Iteration 163, loss = 18.57516524\n",
      "Iteration 164, loss = 18.65417700\n",
      "Iteration 165, loss = 18.49969908\n",
      "Iteration 166, loss = 18.11131125\n",
      "Iteration 167, loss = 18.14429492\n",
      "Iteration 168, loss = 18.27207903\n",
      "Iteration 169, loss = 18.63276356\n",
      "Iteration 170, loss = 17.66164084\n",
      "Iteration 171, loss = 17.91962974\n",
      "Iteration 172, loss = 17.94999217\n",
      "Iteration 173, loss = 17.75129102\n",
      "Iteration 174, loss = 17.33114894\n",
      "Iteration 175, loss = 17.35393087\n",
      "Iteration 176, loss = 16.94814018\n",
      "Iteration 177, loss = 17.30734520\n",
      "Iteration 178, loss = 16.96057226\n",
      "Iteration 179, loss = 16.65090255\n",
      "Iteration 180, loss = 16.66916172\n",
      "Iteration 181, loss = 16.53778155\n",
      "Iteration 182, loss = 16.28539517\n",
      "Iteration 183, loss = 16.70259625\n",
      "Iteration 184, loss = 16.32285342\n",
      "Iteration 185, loss = 15.97153098\n",
      "Iteration 186, loss = 16.19069051\n",
      "Iteration 187, loss = 16.64453769\n",
      "Iteration 188, loss = 15.83917804\n",
      "Iteration 189, loss = 15.70942772\n",
      "Iteration 190, loss = 15.77890721\n",
      "Iteration 191, loss = 15.74247077\n",
      "Iteration 192, loss = 15.99365040\n",
      "Iteration 193, loss = 15.65645577\n",
      "Iteration 194, loss = 15.55135045\n",
      "Iteration 195, loss = 15.36703148\n",
      "Iteration 196, loss = 15.24418146\n",
      "Iteration 197, loss = 15.20786075\n",
      "Iteration 198, loss = 14.92839999\n",
      "Iteration 199, loss = 14.68641690\n",
      "Iteration 200, loss = 14.68150823\n",
      "Iteration 201, loss = 14.60796347\n",
      "Iteration 202, loss = 14.26181887\n",
      "Iteration 203, loss = 14.66887244\n",
      "Iteration 204, loss = 14.72043121\n",
      "Iteration 205, loss = 14.36905560\n",
      "Iteration 206, loss = 14.60199839\n",
      "Iteration 207, loss = 13.99780544\n",
      "Iteration 208, loss = 14.00077267\n",
      "Iteration 209, loss = 14.13598977\n",
      "Iteration 210, loss = 13.97925283\n",
      "Iteration 211, loss = 13.79966937\n",
      "Iteration 212, loss = 13.83217563\n",
      "Iteration 213, loss = 13.74755380\n",
      "Iteration 214, loss = 13.58216197\n",
      "Iteration 215, loss = 14.08920169\n",
      "Iteration 216, loss = 13.54312028\n",
      "Iteration 217, loss = 13.51035086\n",
      "Iteration 218, loss = 13.94540503\n",
      "Iteration 219, loss = 13.50062148\n",
      "Iteration 220, loss = 13.62083926\n",
      "Iteration 221, loss = 13.15181064\n",
      "Iteration 222, loss = 13.26187352\n",
      "Iteration 223, loss = 13.11202362\n",
      "Iteration 224, loss = 13.57587767\n",
      "Iteration 225, loss = 12.74920828\n",
      "Iteration 226, loss = 12.63385478\n",
      "Iteration 227, loss = 12.77488693\n",
      "Iteration 228, loss = 12.61526811\n",
      "Iteration 229, loss = 12.77374651\n",
      "Iteration 230, loss = 13.08416498\n",
      "Iteration 231, loss = 13.01035101\n",
      "Iteration 232, loss = 12.42935457\n",
      "Iteration 233, loss = 12.48076812\n",
      "Iteration 234, loss = 12.70179420\n",
      "Iteration 235, loss = 12.61516552\n",
      "Iteration 236, loss = 12.26956656\n",
      "Iteration 237, loss = 12.42558116\n",
      "Iteration 238, loss = 12.55102549\n",
      "Iteration 239, loss = 12.05806565\n",
      "Iteration 240, loss = 12.11452422\n",
      "Iteration 241, loss = 11.85839103\n",
      "Iteration 242, loss = 12.11845244\n",
      "Iteration 243, loss = 11.97072878\n",
      "Iteration 244, loss = 12.09732033\n",
      "Iteration 245, loss = 11.89182605\n",
      "Iteration 246, loss = 11.65457087\n",
      "Iteration 247, loss = 11.65661252\n",
      "Iteration 248, loss = 11.97388417\n",
      "Iteration 249, loss = 11.60655832\n",
      "Iteration 250, loss = 11.54076848\n",
      "Iteration 251, loss = 11.32313833\n",
      "Iteration 252, loss = 11.77324250\n",
      "Iteration 253, loss = 11.69733645\n",
      "Iteration 254, loss = 11.73838218\n",
      "Iteration 255, loss = 11.28681497\n",
      "Iteration 256, loss = 11.39711301\n",
      "Iteration 257, loss = 11.02236737\n",
      "Iteration 258, loss = 10.94721848\n",
      "Iteration 259, loss = 11.02713700\n",
      "Iteration 260, loss = 10.78429631\n",
      "Iteration 261, loss = 10.75903774\n",
      "Iteration 262, loss = 11.08335367\n",
      "Iteration 263, loss = 10.97160260\n",
      "Iteration 264, loss = 10.87094244\n",
      "Iteration 265, loss = 10.96001170\n",
      "Iteration 266, loss = 10.95869081\n",
      "Iteration 267, loss = 11.11218006\n",
      "Iteration 268, loss = 10.59741420\n",
      "Iteration 269, loss = 10.37857985\n",
      "Iteration 270, loss = 10.62564289\n",
      "Iteration 271, loss = 10.78326939\n",
      "Iteration 272, loss = 11.06970203\n",
      "Iteration 273, loss = 10.24215553\n",
      "Iteration 274, loss = 10.38100507\n",
      "Iteration 275, loss = 10.30843141\n",
      "Iteration 276, loss = 10.48813182\n",
      "Iteration 277, loss = 10.59459070\n",
      "Iteration 278, loss = 10.58189607\n",
      "Iteration 279, loss = 10.43571033\n",
      "Iteration 280, loss = 10.30236147\n",
      "Iteration 281, loss = 10.36834579\n",
      "Iteration 282, loss = 10.15910551\n",
      "Iteration 283, loss = 9.99673636\n",
      "Iteration 284, loss = 10.11483944\n",
      "Iteration 285, loss = 10.67177742\n",
      "Iteration 286, loss = 9.73729044\n",
      "Iteration 287, loss = 10.01089425\n",
      "Iteration 288, loss = 9.79783120\n",
      "Iteration 289, loss = 9.86588011\n",
      "Iteration 290, loss = 9.88519593\n",
      "Iteration 291, loss = 9.46747360\n",
      "Iteration 292, loss = 9.81504654\n",
      "Iteration 293, loss = 9.57332674\n",
      "Iteration 294, loss = 9.55158903\n",
      "Iteration 295, loss = 9.62172637\n",
      "Iteration 296, loss = 9.84732453\n",
      "Iteration 297, loss = 9.37558905\n",
      "Iteration 298, loss = 9.78706632\n",
      "Iteration 299, loss = 9.38656005\n",
      "Iteration 300, loss = 9.26267613\n",
      "Iteration 301, loss = 9.19178671\n",
      "Iteration 302, loss = 9.23672459\n",
      "Iteration 303, loss = 9.38677328\n",
      "Iteration 304, loss = 9.42700443\n",
      "Iteration 305, loss = 8.99265801\n",
      "Iteration 306, loss = 10.11687979\n",
      "Iteration 307, loss = 9.14543092\n",
      "Iteration 308, loss = 9.46455368\n",
      "Iteration 309, loss = 8.79257046\n",
      "Iteration 310, loss = 9.06285782\n",
      "Iteration 311, loss = 9.25900957\n",
      "Iteration 312, loss = 8.99180722\n",
      "Iteration 313, loss = 8.79301414\n",
      "Iteration 314, loss = 9.04535034\n",
      "Iteration 315, loss = 9.01151544\n",
      "Iteration 316, loss = 8.92365994\n",
      "Iteration 317, loss = 8.56441330\n",
      "Iteration 318, loss = 8.67322348\n",
      "Iteration 319, loss = 8.65534679\n",
      "Iteration 320, loss = 8.54089481\n",
      "Iteration 321, loss = 8.77342874\n",
      "Iteration 322, loss = 8.73094325\n",
      "Iteration 323, loss = 8.59227376\n",
      "Iteration 324, loss = 8.76458430\n",
      "Iteration 325, loss = 8.89253527\n",
      "Iteration 326, loss = 8.47123797\n",
      "Iteration 327, loss = 8.67965395\n",
      "Iteration 328, loss = 8.54146638\n",
      "Iteration 329, loss = 8.16787608\n",
      "Iteration 330, loss = 8.35184408\n",
      "Iteration 331, loss = 8.35794650\n",
      "Iteration 332, loss = 8.36144046\n",
      "Iteration 333, loss = 8.12073492\n",
      "Iteration 334, loss = 8.40269701\n",
      "Iteration 335, loss = 8.30524015\n",
      "Iteration 336, loss = 8.15425428\n",
      "Iteration 337, loss = 8.28243378\n",
      "Iteration 338, loss = 8.09214451\n",
      "Iteration 339, loss = 8.27754338\n",
      "Iteration 340, loss = 7.94610946\n",
      "Iteration 341, loss = 8.15878956\n",
      "Iteration 342, loss = 7.96153052\n",
      "Iteration 343, loss = 8.29374424\n",
      "Iteration 344, loss = 7.92727345\n",
      "Iteration 345, loss = 8.03665582\n",
      "Iteration 346, loss = 8.02840350\n",
      "Iteration 347, loss = 8.14549503\n",
      "Iteration 348, loss = 7.98754851\n",
      "Iteration 349, loss = 7.99652714\n",
      "Iteration 350, loss = 8.09051944\n",
      "Iteration 351, loss = 7.98646836\n",
      "Iteration 352, loss = 7.67686911\n",
      "Iteration 353, loss = 8.52605155\n",
      "Iteration 354, loss = 7.92059778\n",
      "Iteration 355, loss = 7.67644717\n",
      "Iteration 356, loss = 7.61636357\n",
      "Iteration 357, loss = 7.45818980\n",
      "Iteration 358, loss = 8.02420707\n",
      "Iteration 359, loss = 7.59640751\n",
      "Iteration 360, loss = 7.55698688\n",
      "Iteration 361, loss = 7.47006125\n",
      "Iteration 362, loss = 7.54512860\n",
      "Iteration 363, loss = 7.75374878\n",
      "Iteration 364, loss = 7.66319502\n",
      "Iteration 365, loss = 8.27222752\n",
      "Iteration 366, loss = 7.48056170\n",
      "Iteration 367, loss = 7.38883882\n",
      "Iteration 368, loss = 7.62645903\n",
      "Iteration 369, loss = 7.79941615\n",
      "Iteration 370, loss = 7.52228914\n",
      "Iteration 371, loss = 7.73954438\n",
      "Iteration 372, loss = 7.17326162\n",
      "Iteration 373, loss = 7.25298619\n",
      "Iteration 374, loss = 7.37365923\n",
      "Iteration 375, loss = 7.40789237\n",
      "Iteration 376, loss = 7.39581559\n",
      "Iteration 377, loss = 7.28079119\n",
      "Iteration 378, loss = 7.33586747\n",
      "Iteration 379, loss = 7.13378442\n",
      "Iteration 380, loss = 6.99679888\n",
      "Iteration 381, loss = 7.31993918\n",
      "Iteration 382, loss = 7.08789770\n",
      "Iteration 383, loss = 7.57954249\n",
      "Iteration 384, loss = 7.41908859\n",
      "Iteration 385, loss = 7.39664343\n",
      "Iteration 386, loss = 7.19611564\n",
      "Iteration 387, loss = 6.99096184\n",
      "Iteration 388, loss = 6.84343282\n",
      "Iteration 389, loss = 7.12544031\n",
      "Iteration 390, loss = 6.83448110\n",
      "Iteration 391, loss = 7.03847512\n",
      "Iteration 392, loss = 7.02597665\n",
      "Iteration 393, loss = 7.16308175\n",
      "Iteration 394, loss = 7.24751535\n",
      "Iteration 395, loss = 7.50367380\n",
      "Iteration 396, loss = 6.89457839\n",
      "Iteration 397, loss = 6.80930593\n",
      "Iteration 398, loss = 6.71170904\n",
      "Iteration 399, loss = 6.88227331\n",
      "Iteration 400, loss = 7.10431259\n",
      "Iteration 401, loss = 6.85478606\n",
      "Iteration 402, loss = 6.77948571\n",
      "Iteration 403, loss = 6.94066834\n",
      "Iteration 404, loss = 6.93206880\n",
      "Iteration 405, loss = 6.68912254\n",
      "Iteration 406, loss = 6.46365464\n",
      "Iteration 407, loss = 6.64681200\n",
      "Iteration 408, loss = 6.70082204\n",
      "Iteration 409, loss = 6.58734652\n",
      "Iteration 410, loss = 6.67841806\n",
      "Iteration 411, loss = 6.51526467\n",
      "Iteration 412, loss = 7.12367268\n",
      "Iteration 413, loss = 6.98251106\n",
      "Iteration 414, loss = 7.35785730\n",
      "Iteration 415, loss = 7.12875212\n",
      "Iteration 416, loss = 6.59613298\n",
      "Iteration 417, loss = 6.56955073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting pipelines for registered\n",
      "Iteration 1, loss = 20153.26151494\n",
      "Iteration 2, loss = 15258.99815219\n",
      "Iteration 3, loss = 6900.46956913\n",
      "Iteration 4, loss = 4102.55751328\n",
      "Iteration 5, loss = 3339.79805350\n",
      "Iteration 6, loss = 2992.71726844\n",
      "Iteration 7, loss = 2767.11409727\n",
      "Iteration 8, loss = 2605.20160697\n",
      "Iteration 9, loss = 2467.94078355\n",
      "Iteration 10, loss = 2364.83954107\n",
      "Iteration 11, loss = 2266.41542398\n",
      "Iteration 12, loss = 2184.32272548\n",
      "Iteration 13, loss = 2108.37379423\n",
      "Iteration 14, loss = 2038.37688512\n",
      "Iteration 15, loss = 1972.95342455\n",
      "Iteration 16, loss = 1913.02436663\n",
      "Iteration 17, loss = 1863.24842985\n",
      "Iteration 18, loss = 1813.06688704\n",
      "Iteration 19, loss = 1770.77961868\n",
      "Iteration 20, loss = 1734.50556729\n",
      "Iteration 21, loss = 1699.49573378\n",
      "Iteration 22, loss = 1667.87872438\n",
      "Iteration 23, loss = 1640.97811508\n",
      "Iteration 24, loss = 1619.46339156\n",
      "Iteration 25, loss = 1597.56219708\n",
      "Iteration 26, loss = 1575.80858749\n",
      "Iteration 27, loss = 1553.93735491\n",
      "Iteration 28, loss = 1537.97704827\n",
      "Iteration 29, loss = 1524.14657293\n",
      "Iteration 30, loss = 1506.64899316\n",
      "Iteration 31, loss = 1485.68570233\n",
      "Iteration 32, loss = 1482.34502986\n",
      "Iteration 33, loss = 1458.24558511\n",
      "Iteration 34, loss = 1445.88955799\n",
      "Iteration 35, loss = 1432.44779778\n",
      "Iteration 36, loss = 1419.61755603\n",
      "Iteration 37, loss = 1412.48604392\n",
      "Iteration 38, loss = 1397.55974325\n",
      "Iteration 39, loss = 1389.03386008\n",
      "Iteration 40, loss = 1374.15351552\n",
      "Iteration 41, loss = 1366.18288821\n",
      "Iteration 42, loss = 1352.62822903\n",
      "Iteration 43, loss = 1340.70680337\n",
      "Iteration 44, loss = 1328.40801667\n",
      "Iteration 45, loss = 1315.02183212\n",
      "Iteration 46, loss = 1305.07167981\n",
      "Iteration 47, loss = 1293.66677773\n",
      "Iteration 48, loss = 1284.39021615\n",
      "Iteration 49, loss = 1276.63400491\n",
      "Iteration 50, loss = 1267.53858502\n",
      "Iteration 51, loss = 1248.19722184\n",
      "Iteration 52, loss = 1237.51895214\n",
      "Iteration 53, loss = 1225.12045420\n",
      "Iteration 54, loss = 1212.53230468\n",
      "Iteration 55, loss = 1200.69407673\n",
      "Iteration 56, loss = 1186.56576813\n",
      "Iteration 57, loss = 1178.64123597\n",
      "Iteration 58, loss = 1169.18536266\n",
      "Iteration 59, loss = 1155.85492594\n",
      "Iteration 60, loss = 1138.04786866\n",
      "Iteration 61, loss = 1128.61897187\n",
      "Iteration 62, loss = 1119.50435447\n",
      "Iteration 63, loss = 1105.15653552\n",
      "Iteration 64, loss = 1098.10263342\n",
      "Iteration 65, loss = 1079.87459382\n",
      "Iteration 66, loss = 1069.86128933\n",
      "Iteration 67, loss = 1054.62139188\n",
      "Iteration 68, loss = 1042.58766041\n",
      "Iteration 69, loss = 1030.87110287\n",
      "Iteration 70, loss = 1015.18031824\n",
      "Iteration 71, loss = 1005.30140723\n",
      "Iteration 72, loss = 992.90340190\n",
      "Iteration 73, loss = 980.60530370\n",
      "Iteration 74, loss = 968.86627645\n",
      "Iteration 75, loss = 954.07894894\n",
      "Iteration 76, loss = 949.85704829\n",
      "Iteration 77, loss = 939.16721961\n",
      "Iteration 78, loss = 928.23284369\n",
      "Iteration 79, loss = 911.06030755\n",
      "Iteration 80, loss = 904.56129921\n",
      "Iteration 81, loss = 895.02134893\n",
      "Iteration 82, loss = 880.60912662\n",
      "Iteration 83, loss = 867.81359167\n",
      "Iteration 84, loss = 856.28288008\n",
      "Iteration 85, loss = 840.76651275\n",
      "Iteration 86, loss = 830.63189929\n",
      "Iteration 87, loss = 814.95127764\n",
      "Iteration 88, loss = 807.91726818\n",
      "Iteration 89, loss = 795.25280018\n",
      "Iteration 90, loss = 789.27929312\n",
      "Iteration 91, loss = 774.90606687\n",
      "Iteration 92, loss = 759.71288044\n",
      "Iteration 93, loss = 747.69047788\n",
      "Iteration 94, loss = 737.25337642\n",
      "Iteration 95, loss = 732.81084725\n",
      "Iteration 96, loss = 718.01862986\n",
      "Iteration 97, loss = 703.61224156\n",
      "Iteration 98, loss = 697.98427324\n",
      "Iteration 99, loss = 688.95707702\n",
      "Iteration 100, loss = 678.30815246\n",
      "Iteration 101, loss = 668.38354493\n",
      "Iteration 102, loss = 661.35359375\n",
      "Iteration 103, loss = 650.66538036\n",
      "Iteration 104, loss = 641.52501009\n",
      "Iteration 105, loss = 628.51628708\n",
      "Iteration 106, loss = 626.69609108\n",
      "Iteration 107, loss = 616.02275160\n",
      "Iteration 108, loss = 607.43860306\n",
      "Iteration 109, loss = 593.32559450\n",
      "Iteration 110, loss = 586.13410691\n",
      "Iteration 111, loss = 585.65526531\n",
      "Iteration 112, loss = 577.50088769\n",
      "Iteration 113, loss = 568.28823421\n",
      "Iteration 114, loss = 562.97281976\n",
      "Iteration 115, loss = 557.06490632\n",
      "Iteration 116, loss = 543.15822535\n",
      "Iteration 117, loss = 544.47037803\n",
      "Iteration 118, loss = 536.94636521\n",
      "Iteration 119, loss = 527.58202148\n",
      "Iteration 120, loss = 517.50942736\n",
      "Iteration 121, loss = 509.36814429\n",
      "Iteration 122, loss = 506.88844654\n",
      "Iteration 123, loss = 502.06001505\n",
      "Iteration 124, loss = 496.54675443\n",
      "Iteration 125, loss = 496.74213509\n",
      "Iteration 126, loss = 480.96059295\n",
      "Iteration 127, loss = 481.44630751\n",
      "Iteration 128, loss = 472.76565172\n",
      "Iteration 129, loss = 467.64383604\n",
      "Iteration 130, loss = 463.06868211\n",
      "Iteration 131, loss = 457.62389933\n",
      "Iteration 132, loss = 452.64059833\n",
      "Iteration 133, loss = 446.77842742\n",
      "Iteration 134, loss = 444.62197414\n",
      "Iteration 135, loss = 440.56868874\n",
      "Iteration 136, loss = 434.05900374\n",
      "Iteration 137, loss = 429.35315749\n",
      "Iteration 138, loss = 427.37436677\n",
      "Iteration 139, loss = 419.79004256\n",
      "Iteration 140, loss = 416.71262399\n",
      "Iteration 141, loss = 414.47877261\n",
      "Iteration 142, loss = 409.65687113\n",
      "Iteration 143, loss = 415.36199008\n",
      "Iteration 144, loss = 407.83118645\n",
      "Iteration 145, loss = 401.87399578\n",
      "Iteration 146, loss = 397.21047618\n",
      "Iteration 147, loss = 393.79182758\n",
      "Iteration 148, loss = 388.69379600\n",
      "Iteration 149, loss = 388.69041772\n",
      "Iteration 150, loss = 388.03181404\n",
      "Iteration 151, loss = 379.75425023\n",
      "Iteration 152, loss = 377.18475203\n",
      "Iteration 153, loss = 373.55459922\n",
      "Iteration 154, loss = 372.78020264\n",
      "Iteration 155, loss = 372.34540170\n",
      "Iteration 156, loss = 368.39537090\n",
      "Iteration 157, loss = 364.29722742\n",
      "Iteration 158, loss = 359.93076517\n",
      "Iteration 159, loss = 355.94785345\n",
      "Iteration 160, loss = 353.26816759\n",
      "Iteration 161, loss = 353.25911330\n",
      "Iteration 162, loss = 350.41520724\n",
      "Iteration 163, loss = 350.58894279\n",
      "Iteration 164, loss = 343.84588476\n",
      "Iteration 165, loss = 341.74546518\n",
      "Iteration 166, loss = 338.73472851\n",
      "Iteration 167, loss = 338.88667569\n",
      "Iteration 168, loss = 334.33172743\n",
      "Iteration 169, loss = 331.63584701\n",
      "Iteration 170, loss = 332.73774548\n",
      "Iteration 171, loss = 329.08393114\n",
      "Iteration 172, loss = 325.17395746\n",
      "Iteration 173, loss = 324.19140510\n",
      "Iteration 174, loss = 324.69367416\n",
      "Iteration 175, loss = 321.18153376\n",
      "Iteration 176, loss = 315.21044791\n",
      "Iteration 177, loss = 318.48617224\n",
      "Iteration 178, loss = 315.20020853\n",
      "Iteration 179, loss = 307.44310597\n",
      "Iteration 180, loss = 308.47557658\n",
      "Iteration 181, loss = 311.28330038\n",
      "Iteration 182, loss = 307.17541278\n",
      "Iteration 183, loss = 305.12301362\n",
      "Iteration 184, loss = 302.86722189\n",
      "Iteration 185, loss = 299.12494818\n",
      "Iteration 186, loss = 298.56743958\n",
      "Iteration 187, loss = 298.89725656\n",
      "Iteration 188, loss = 292.77079944\n",
      "Iteration 189, loss = 293.96765191\n",
      "Iteration 190, loss = 292.53798237\n",
      "Iteration 191, loss = 291.82482810\n",
      "Iteration 192, loss = 287.11638219\n",
      "Iteration 193, loss = 286.40739155\n",
      "Iteration 194, loss = 294.01080614\n",
      "Iteration 195, loss = 288.07746930\n",
      "Iteration 196, loss = 284.12057920\n",
      "Iteration 197, loss = 286.29647834\n",
      "Iteration 198, loss = 279.19980650\n",
      "Iteration 199, loss = 282.21561139\n",
      "Iteration 200, loss = 280.49749509\n",
      "Iteration 201, loss = 272.33424248\n",
      "Iteration 202, loss = 276.00265395\n",
      "Iteration 203, loss = 276.95389162\n",
      "Iteration 204, loss = 270.80294444\n",
      "Iteration 205, loss = 268.84996550\n",
      "Iteration 206, loss = 268.96466025\n",
      "Iteration 207, loss = 268.91647546\n",
      "Iteration 208, loss = 265.09663721\n",
      "Iteration 209, loss = 266.10298488\n",
      "Iteration 210, loss = 261.33937971\n",
      "Iteration 211, loss = 261.64325800\n",
      "Iteration 212, loss = 263.10815558\n",
      "Iteration 213, loss = 259.83596381\n",
      "Iteration 214, loss = 256.46381419\n",
      "Iteration 215, loss = 257.36067850\n",
      "Iteration 216, loss = 255.83219157\n",
      "Iteration 217, loss = 254.14438156\n",
      "Iteration 218, loss = 252.99137012\n",
      "Iteration 219, loss = 254.96510391\n",
      "Iteration 220, loss = 251.17957818\n",
      "Iteration 221, loss = 250.31229912\n",
      "Iteration 222, loss = 250.25256142\n",
      "Iteration 223, loss = 247.38103039\n",
      "Iteration 224, loss = 243.29753001\n",
      "Iteration 225, loss = 246.51401812\n",
      "Iteration 226, loss = 243.61755064\n",
      "Iteration 227, loss = 243.23053962\n",
      "Iteration 228, loss = 242.54729273\n",
      "Iteration 229, loss = 244.99442116\n",
      "Iteration 230, loss = 241.40318515\n",
      "Iteration 231, loss = 237.40385075\n",
      "Iteration 232, loss = 238.41867162\n",
      "Iteration 233, loss = 236.38519932\n",
      "Iteration 234, loss = 239.76361573\n",
      "Iteration 235, loss = 234.14482562\n",
      "Iteration 236, loss = 232.52846947\n",
      "Iteration 237, loss = 232.75498094\n",
      "Iteration 238, loss = 230.23599839\n",
      "Iteration 239, loss = 239.74491266\n",
      "Iteration 240, loss = 228.72229502\n",
      "Iteration 241, loss = 228.14573828\n",
      "Iteration 242, loss = 231.42172643\n",
      "Iteration 243, loss = 227.48604253\n",
      "Iteration 244, loss = 228.42564956\n",
      "Iteration 245, loss = 227.04322398\n",
      "Iteration 246, loss = 222.85743676\n",
      "Iteration 247, loss = 225.23475199\n",
      "Iteration 248, loss = 225.24050686\n",
      "Iteration 249, loss = 226.47524647\n",
      "Iteration 250, loss = 223.09031347\n",
      "Iteration 251, loss = 221.76321617\n",
      "Iteration 252, loss = 221.99526038\n",
      "Iteration 253, loss = 220.11190571\n",
      "Iteration 254, loss = 218.89570918\n",
      "Iteration 255, loss = 214.78251514\n",
      "Iteration 256, loss = 215.89240156\n",
      "Iteration 257, loss = 218.28239717\n",
      "Iteration 258, loss = 216.48968796\n",
      "Iteration 259, loss = 212.67781941\n",
      "Iteration 260, loss = 212.49427307\n",
      "Iteration 261, loss = 209.82243773\n",
      "Iteration 262, loss = 211.49542294\n",
      "Iteration 263, loss = 216.08726254\n",
      "Iteration 264, loss = 214.54053507\n",
      "Iteration 265, loss = 208.20974786\n",
      "Iteration 266, loss = 208.17832457\n",
      "Iteration 267, loss = 207.55287613\n",
      "Iteration 268, loss = 206.92895097\n",
      "Iteration 269, loss = 205.71841166\n",
      "Iteration 270, loss = 206.42063339\n",
      "Iteration 271, loss = 206.47831521\n",
      "Iteration 272, loss = 206.98898935\n",
      "Iteration 273, loss = 203.71716774\n",
      "Iteration 274, loss = 207.56910884\n",
      "Iteration 275, loss = 203.12327435\n",
      "Iteration 276, loss = 204.97147851\n",
      "Iteration 277, loss = 201.64680646\n",
      "Iteration 278, loss = 202.76304983\n",
      "Iteration 279, loss = 198.48263031\n",
      "Iteration 280, loss = 199.91828107\n",
      "Iteration 281, loss = 197.00611176\n",
      "Iteration 282, loss = 196.38987081\n",
      "Iteration 283, loss = 197.73806084\n",
      "Iteration 284, loss = 195.14136436\n",
      "Iteration 285, loss = 195.77018906\n",
      "Iteration 286, loss = 195.49602491\n",
      "Iteration 287, loss = 196.20942766\n",
      "Iteration 288, loss = 193.90378132\n",
      "Iteration 289, loss = 192.18019930\n",
      "Iteration 290, loss = 193.65475622\n",
      "Iteration 291, loss = 192.88754948\n",
      "Iteration 292, loss = 192.80477360\n",
      "Iteration 293, loss = 188.76617894\n",
      "Iteration 294, loss = 191.52703187\n",
      "Iteration 295, loss = 191.32757603\n",
      "Iteration 296, loss = 187.15360377\n",
      "Iteration 297, loss = 191.81241992\n",
      "Iteration 298, loss = 188.15168788\n",
      "Iteration 299, loss = 185.14535623\n",
      "Iteration 300, loss = 188.35904685\n",
      "Iteration 301, loss = 187.19925508\n",
      "Iteration 302, loss = 183.60710277\n",
      "Iteration 303, loss = 184.97289199\n",
      "Iteration 304, loss = 183.40895520\n",
      "Iteration 305, loss = 188.74845409\n",
      "Iteration 306, loss = 183.09474368\n",
      "Iteration 307, loss = 182.06805896\n",
      "Iteration 308, loss = 180.49515375\n",
      "Iteration 309, loss = 184.29712384\n",
      "Iteration 310, loss = 180.89269669\n",
      "Iteration 311, loss = 180.23524953\n",
      "Iteration 312, loss = 179.14127954\n",
      "Iteration 313, loss = 178.25982138\n",
      "Iteration 314, loss = 179.46792636\n",
      "Iteration 315, loss = 178.20517927\n",
      "Iteration 316, loss = 177.11483629\n",
      "Iteration 317, loss = 176.84321839\n",
      "Iteration 318, loss = 179.37259698\n",
      "Iteration 319, loss = 180.52046727\n",
      "Iteration 320, loss = 173.98553498\n",
      "Iteration 321, loss = 175.21741795\n",
      "Iteration 322, loss = 174.32302170\n",
      "Iteration 323, loss = 176.88551409\n",
      "Iteration 324, loss = 176.07167147\n",
      "Iteration 325, loss = 171.85191293\n",
      "Iteration 326, loss = 170.13354360\n",
      "Iteration 327, loss = 173.11066062\n",
      "Iteration 328, loss = 170.78772727\n",
      "Iteration 329, loss = 174.39527850\n",
      "Iteration 330, loss = 168.75343501\n",
      "Iteration 331, loss = 170.74884336\n",
      "Iteration 332, loss = 168.65511872\n",
      "Iteration 333, loss = 169.21817706\n",
      "Iteration 334, loss = 169.97685963\n",
      "Iteration 335, loss = 172.16890337\n",
      "Iteration 336, loss = 167.11992624\n",
      "Iteration 337, loss = 165.49725185\n",
      "Iteration 338, loss = 168.09571200\n",
      "Iteration 339, loss = 166.15577251\n",
      "Iteration 340, loss = 163.06719423\n",
      "Iteration 341, loss = 162.63883896\n",
      "Iteration 342, loss = 164.35746298\n",
      "Iteration 343, loss = 164.28644303\n",
      "Iteration 344, loss = 167.40541797\n",
      "Iteration 345, loss = 166.73921094\n",
      "Iteration 346, loss = 161.32168187\n",
      "Iteration 347, loss = 160.58143728\n",
      "Iteration 348, loss = 170.24848460\n",
      "Iteration 349, loss = 161.83833447\n",
      "Iteration 350, loss = 159.91606269\n",
      "Iteration 351, loss = 162.95364193\n",
      "Iteration 352, loss = 163.19855589\n",
      "Iteration 353, loss = 161.54541767\n",
      "Iteration 354, loss = 158.70177140\n",
      "Iteration 355, loss = 158.50574324\n",
      "Iteration 356, loss = 161.43934835\n",
      "Iteration 357, loss = 158.85417306\n",
      "Iteration 358, loss = 156.65832102\n",
      "Iteration 359, loss = 158.70282282\n",
      "Iteration 360, loss = 159.26997952\n",
      "Iteration 361, loss = 158.13212918\n",
      "Iteration 362, loss = 156.08986019\n",
      "Iteration 363, loss = 154.49113062\n",
      "Iteration 364, loss = 156.76025947\n",
      "Iteration 365, loss = 155.66054281\n",
      "Iteration 366, loss = 158.92093021\n",
      "Iteration 367, loss = 153.90162033\n",
      "Iteration 368, loss = 154.81775642\n",
      "Iteration 369, loss = 152.86804366\n",
      "Iteration 370, loss = 151.61308740\n",
      "Iteration 371, loss = 155.21149566\n",
      "Iteration 372, loss = 153.50357938\n",
      "Iteration 373, loss = 154.72006401\n",
      "Iteration 374, loss = 150.20400009\n",
      "Iteration 375, loss = 151.60684661\n",
      "Iteration 376, loss = 150.00956306\n",
      "Iteration 377, loss = 153.92668797\n",
      "Iteration 378, loss = 151.43383089\n",
      "Iteration 379, loss = 154.13976531\n",
      "Iteration 380, loss = 149.40144440\n",
      "Iteration 381, loss = 145.51179755\n",
      "Iteration 382, loss = 146.42820244\n",
      "Iteration 383, loss = 146.54679108\n",
      "Iteration 384, loss = 148.31168458\n",
      "Iteration 385, loss = 149.25582309\n",
      "Iteration 386, loss = 146.58324318\n",
      "Iteration 387, loss = 144.56302488\n",
      "Iteration 388, loss = 144.77788559\n",
      "Iteration 389, loss = 147.38458211\n",
      "Iteration 390, loss = 145.02596601\n",
      "Iteration 391, loss = 141.88580746\n",
      "Iteration 392, loss = 146.53541914\n",
      "Iteration 393, loss = 147.88331506\n",
      "Iteration 394, loss = 142.95755379\n",
      "Iteration 395, loss = 143.92092497\n",
      "Iteration 396, loss = 142.94127567\n",
      "Iteration 397, loss = 145.39962705\n",
      "Iteration 398, loss = 143.29036855\n",
      "Iteration 399, loss = 144.79266528\n",
      "Iteration 400, loss = 145.27434631\n",
      "Iteration 401, loss = 144.72434755\n",
      "Iteration 402, loss = 143.30976079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20524.84420956\n",
      "Iteration 2, loss = 15120.56441379\n",
      "Iteration 3, loss = 6531.17126786\n",
      "Iteration 4, loss = 3898.91767900\n",
      "Iteration 5, loss = 3181.21513846\n",
      "Iteration 6, loss = 2807.67619473\n",
      "Iteration 7, loss = 2575.59326096\n",
      "Iteration 8, loss = 2407.82378381\n",
      "Iteration 9, loss = 2280.50779239\n",
      "Iteration 10, loss = 2178.06761908\n",
      "Iteration 11, loss = 2092.57846444\n",
      "Iteration 12, loss = 2017.15907590\n",
      "Iteration 13, loss = 1956.25507243\n",
      "Iteration 14, loss = 1901.72147516\n",
      "Iteration 15, loss = 1853.71462128\n",
      "Iteration 16, loss = 1811.26493442\n",
      "Iteration 17, loss = 1768.40459575\n",
      "Iteration 18, loss = 1727.24415473\n",
      "Iteration 19, loss = 1686.84827046\n",
      "Iteration 20, loss = 1651.68058397\n",
      "Iteration 21, loss = 1611.23191657\n",
      "Iteration 22, loss = 1573.49670233\n",
      "Iteration 23, loss = 1543.38255806\n",
      "Iteration 24, loss = 1506.73850978\n",
      "Iteration 25, loss = 1478.16056990\n",
      "Iteration 26, loss = 1445.85143188\n",
      "Iteration 27, loss = 1418.96058174\n",
      "Iteration 28, loss = 1395.43737219\n",
      "Iteration 29, loss = 1372.45153717\n",
      "Iteration 30, loss = 1347.75606803\n",
      "Iteration 31, loss = 1325.98948848\n",
      "Iteration 32, loss = 1301.90876135\n",
      "Iteration 33, loss = 1285.61234081\n",
      "Iteration 34, loss = 1266.84851246\n",
      "Iteration 35, loss = 1248.89415076\n",
      "Iteration 36, loss = 1231.86584352\n",
      "Iteration 37, loss = 1215.30764223\n",
      "Iteration 38, loss = 1197.75155039\n",
      "Iteration 39, loss = 1182.52095140\n",
      "Iteration 40, loss = 1167.95593874\n",
      "Iteration 41, loss = 1152.29355358\n",
      "Iteration 42, loss = 1136.39529920\n",
      "Iteration 43, loss = 1120.89548016\n",
      "Iteration 44, loss = 1106.86749195\n",
      "Iteration 45, loss = 1092.94814462\n",
      "Iteration 46, loss = 1080.20040045\n",
      "Iteration 47, loss = 1067.37574778\n",
      "Iteration 48, loss = 1047.60668675\n",
      "Iteration 49, loss = 1035.66621170\n",
      "Iteration 50, loss = 1021.99041678\n",
      "Iteration 51, loss = 1006.72310558\n",
      "Iteration 52, loss = 990.54644387\n",
      "Iteration 53, loss = 972.83801312\n",
      "Iteration 54, loss = 958.51225502\n",
      "Iteration 55, loss = 947.32386515\n",
      "Iteration 56, loss = 929.38387738\n",
      "Iteration 57, loss = 922.78687232\n",
      "Iteration 58, loss = 906.70040327\n",
      "Iteration 59, loss = 896.43998582\n",
      "Iteration 60, loss = 875.56779720\n",
      "Iteration 61, loss = 865.69845540\n",
      "Iteration 62, loss = 852.02484720\n",
      "Iteration 63, loss = 846.59969881\n",
      "Iteration 64, loss = 832.48736139\n",
      "Iteration 65, loss = 824.34388258\n",
      "Iteration 66, loss = 807.94675721\n",
      "Iteration 67, loss = 795.98080378\n",
      "Iteration 68, loss = 782.87658558\n",
      "Iteration 69, loss = 772.22258255\n",
      "Iteration 70, loss = 758.91767291\n",
      "Iteration 71, loss = 750.82462082\n",
      "Iteration 72, loss = 740.09530294\n",
      "Iteration 73, loss = 729.73348359\n",
      "Iteration 74, loss = 717.61189269\n",
      "Iteration 75, loss = 713.68629763\n",
      "Iteration 76, loss = 700.96342531\n",
      "Iteration 77, loss = 690.17729908\n",
      "Iteration 78, loss = 682.32808421\n",
      "Iteration 79, loss = 668.85413889\n",
      "Iteration 80, loss = 657.55174518\n",
      "Iteration 81, loss = 659.59775458\n",
      "Iteration 82, loss = 643.05581968\n",
      "Iteration 83, loss = 633.87905028\n",
      "Iteration 84, loss = 624.91289460\n",
      "Iteration 85, loss = 617.45190850\n",
      "Iteration 86, loss = 606.19280252\n",
      "Iteration 87, loss = 603.99593892\n",
      "Iteration 88, loss = 593.28857545\n",
      "Iteration 89, loss = 584.47825276\n",
      "Iteration 90, loss = 582.02601910\n",
      "Iteration 91, loss = 569.44261236\n",
      "Iteration 92, loss = 564.49922314\n",
      "Iteration 93, loss = 552.76218940\n",
      "Iteration 94, loss = 550.74544111\n",
      "Iteration 95, loss = 542.19292446\n",
      "Iteration 96, loss = 541.55297385\n",
      "Iteration 97, loss = 529.69101042\n",
      "Iteration 98, loss = 523.56563808\n",
      "Iteration 99, loss = 514.15538428\n",
      "Iteration 100, loss = 506.60158205\n",
      "Iteration 101, loss = 504.41582504\n",
      "Iteration 102, loss = 500.67328332\n",
      "Iteration 103, loss = 493.95289856\n",
      "Iteration 104, loss = 491.97187836\n",
      "Iteration 105, loss = 486.13400084\n",
      "Iteration 106, loss = 490.46935686\n",
      "Iteration 107, loss = 473.59556121\n",
      "Iteration 108, loss = 474.12111557\n",
      "Iteration 109, loss = 467.42954328\n",
      "Iteration 110, loss = 459.58724192\n",
      "Iteration 111, loss = 456.17233448\n",
      "Iteration 112, loss = 452.75914435\n",
      "Iteration 113, loss = 450.63863738\n",
      "Iteration 114, loss = 442.21523455\n",
      "Iteration 115, loss = 436.97318179\n",
      "Iteration 116, loss = 434.24263777\n",
      "Iteration 117, loss = 431.15200246\n",
      "Iteration 118, loss = 429.18357785\n",
      "Iteration 119, loss = 422.13202240\n",
      "Iteration 120, loss = 425.65242048\n",
      "Iteration 121, loss = 418.58007408\n",
      "Iteration 122, loss = 411.74415683\n",
      "Iteration 123, loss = 402.02884034\n",
      "Iteration 124, loss = 408.42455060\n",
      "Iteration 125, loss = 397.99132149\n",
      "Iteration 126, loss = 398.91778536\n",
      "Iteration 127, loss = 392.85251888\n",
      "Iteration 128, loss = 388.95098879\n",
      "Iteration 129, loss = 387.95974438\n",
      "Iteration 130, loss = 386.27587884\n",
      "Iteration 131, loss = 380.83544452\n",
      "Iteration 132, loss = 375.87874636\n",
      "Iteration 133, loss = 373.02792353\n",
      "Iteration 134, loss = 370.49002316\n",
      "Iteration 135, loss = 370.48352679\n",
      "Iteration 136, loss = 366.11133739\n",
      "Iteration 137, loss = 363.65053825\n",
      "Iteration 138, loss = 359.00471870\n",
      "Iteration 139, loss = 355.31058942\n",
      "Iteration 140, loss = 353.39322744\n",
      "Iteration 141, loss = 349.89419010\n",
      "Iteration 142, loss = 352.83025839\n",
      "Iteration 143, loss = 346.66776349\n",
      "Iteration 144, loss = 342.32553729\n",
      "Iteration 145, loss = 338.76398889\n",
      "Iteration 146, loss = 344.50558398\n",
      "Iteration 147, loss = 336.40565439\n",
      "Iteration 148, loss = 331.39678538\n",
      "Iteration 149, loss = 327.63788121\n",
      "Iteration 150, loss = 329.73017354\n",
      "Iteration 151, loss = 324.55971820\n",
      "Iteration 152, loss = 321.22462014\n",
      "Iteration 153, loss = 318.48170483\n",
      "Iteration 154, loss = 321.36347113\n",
      "Iteration 155, loss = 320.69804180\n",
      "Iteration 156, loss = 318.30613740\n",
      "Iteration 157, loss = 314.29321437\n",
      "Iteration 158, loss = 308.88416682\n",
      "Iteration 159, loss = 309.84682256\n",
      "Iteration 160, loss = 306.52024329\n",
      "Iteration 161, loss = 303.26307110\n",
      "Iteration 162, loss = 305.47325451\n",
      "Iteration 163, loss = 297.97218298\n",
      "Iteration 164, loss = 297.76817362\n",
      "Iteration 165, loss = 295.07714489\n",
      "Iteration 166, loss = 292.17839138\n",
      "Iteration 167, loss = 288.29027246\n",
      "Iteration 168, loss = 288.31480400\n",
      "Iteration 169, loss = 284.80153092\n",
      "Iteration 170, loss = 287.16544226\n",
      "Iteration 171, loss = 282.58502686\n",
      "Iteration 172, loss = 284.68023617\n",
      "Iteration 173, loss = 279.46458264\n",
      "Iteration 174, loss = 279.32268586\n",
      "Iteration 175, loss = 274.54618405\n",
      "Iteration 176, loss = 273.68531074\n",
      "Iteration 177, loss = 272.70728050\n",
      "Iteration 178, loss = 275.88250126\n",
      "Iteration 179, loss = 271.00974908\n",
      "Iteration 180, loss = 270.84359548\n",
      "Iteration 181, loss = 266.73190713\n",
      "Iteration 182, loss = 266.32208097\n",
      "Iteration 183, loss = 262.90978923\n",
      "Iteration 184, loss = 266.88507540\n",
      "Iteration 185, loss = 262.36684418\n",
      "Iteration 186, loss = 259.29162669\n",
      "Iteration 187, loss = 263.81682154\n",
      "Iteration 188, loss = 261.31478501\n",
      "Iteration 189, loss = 253.15713023\n",
      "Iteration 190, loss = 255.80319184\n",
      "Iteration 191, loss = 252.32853634\n",
      "Iteration 192, loss = 251.38846631\n",
      "Iteration 193, loss = 257.51512667\n",
      "Iteration 194, loss = 250.52727290\n",
      "Iteration 195, loss = 248.62743219\n",
      "Iteration 196, loss = 245.43730674\n",
      "Iteration 197, loss = 242.43590205\n",
      "Iteration 198, loss = 243.03914838\n",
      "Iteration 199, loss = 241.75099941\n",
      "Iteration 200, loss = 240.53816507\n",
      "Iteration 201, loss = 240.00262180\n",
      "Iteration 202, loss = 240.00792558\n",
      "Iteration 203, loss = 239.23216134\n",
      "Iteration 204, loss = 234.65738189\n",
      "Iteration 205, loss = 234.65410830\n",
      "Iteration 206, loss = 234.72664481\n",
      "Iteration 207, loss = 238.89358609\n",
      "Iteration 208, loss = 236.20047808\n",
      "Iteration 209, loss = 231.81827648\n",
      "Iteration 210, loss = 226.17024403\n",
      "Iteration 211, loss = 228.68742614\n",
      "Iteration 212, loss = 226.36513060\n",
      "Iteration 213, loss = 227.61105085\n",
      "Iteration 214, loss = 224.62901301\n",
      "Iteration 215, loss = 221.37905647\n",
      "Iteration 216, loss = 221.54422944\n",
      "Iteration 217, loss = 220.62876228\n",
      "Iteration 218, loss = 221.23955073\n",
      "Iteration 219, loss = 222.42198033\n",
      "Iteration 220, loss = 222.89308346\n",
      "Iteration 221, loss = 218.81935099\n",
      "Iteration 222, loss = 215.74376484\n",
      "Iteration 223, loss = 217.76679814\n",
      "Iteration 224, loss = 215.95950182\n",
      "Iteration 225, loss = 216.26650032\n",
      "Iteration 226, loss = 213.01162865\n",
      "Iteration 227, loss = 213.74811287\n",
      "Iteration 228, loss = 207.75573820\n",
      "Iteration 229, loss = 209.90987475\n",
      "Iteration 230, loss = 210.45916846\n",
      "Iteration 231, loss = 210.10403545\n",
      "Iteration 232, loss = 209.79271988\n",
      "Iteration 233, loss = 205.18729216\n",
      "Iteration 234, loss = 207.46968134\n",
      "Iteration 235, loss = 202.20568462\n",
      "Iteration 236, loss = 209.10173077\n",
      "Iteration 237, loss = 207.34299218\n",
      "Iteration 238, loss = 204.18725639\n",
      "Iteration 239, loss = 203.41408007\n",
      "Iteration 240, loss = 201.70236501\n",
      "Iteration 241, loss = 200.18076060\n",
      "Iteration 242, loss = 200.89025731\n",
      "Iteration 243, loss = 198.56099193\n",
      "Iteration 244, loss = 200.73607162\n",
      "Iteration 245, loss = 197.61251202\n",
      "Iteration 246, loss = 198.32842449\n",
      "Iteration 247, loss = 197.43296407\n",
      "Iteration 248, loss = 194.42268525\n",
      "Iteration 249, loss = 193.72934275\n",
      "Iteration 250, loss = 190.95876210\n",
      "Iteration 251, loss = 190.61497381\n",
      "Iteration 252, loss = 190.55595194\n",
      "Iteration 253, loss = 188.56428258\n",
      "Iteration 254, loss = 192.95575822\n",
      "Iteration 255, loss = 189.10806309\n",
      "Iteration 256, loss = 187.07081810\n",
      "Iteration 257, loss = 187.09442076\n",
      "Iteration 258, loss = 187.36356922\n",
      "Iteration 259, loss = 186.98825180\n",
      "Iteration 260, loss = 184.41185096\n",
      "Iteration 261, loss = 183.76480499\n",
      "Iteration 262, loss = 184.39763483\n",
      "Iteration 263, loss = 183.40393421\n",
      "Iteration 264, loss = 187.73938273\n",
      "Iteration 265, loss = 183.09456667\n",
      "Iteration 266, loss = 180.75223525\n",
      "Iteration 267, loss = 184.23475777\n",
      "Iteration 268, loss = 181.38957276\n",
      "Iteration 269, loss = 178.82758970\n",
      "Iteration 270, loss = 178.26102019\n",
      "Iteration 271, loss = 178.55519656\n",
      "Iteration 272, loss = 179.45298530\n",
      "Iteration 273, loss = 179.60886287\n",
      "Iteration 274, loss = 178.96356382\n",
      "Iteration 275, loss = 179.41606951\n",
      "Iteration 276, loss = 175.63985728\n",
      "Iteration 277, loss = 178.46087595\n",
      "Iteration 278, loss = 175.08613008\n",
      "Iteration 279, loss = 174.45305816\n",
      "Iteration 280, loss = 173.53729302\n",
      "Iteration 281, loss = 172.33492274\n",
      "Iteration 282, loss = 178.31517971\n",
      "Iteration 283, loss = 172.38510402\n",
      "Iteration 284, loss = 173.20368134\n",
      "Iteration 285, loss = 172.89503152\n",
      "Iteration 286, loss = 170.09060410\n",
      "Iteration 287, loss = 170.91211276\n",
      "Iteration 288, loss = 169.04079842\n",
      "Iteration 289, loss = 167.43501417\n",
      "Iteration 290, loss = 166.33419038\n",
      "Iteration 291, loss = 168.54652395\n",
      "Iteration 292, loss = 171.42325623\n",
      "Iteration 293, loss = 168.47257790\n",
      "Iteration 294, loss = 165.41191252\n",
      "Iteration 295, loss = 165.96046736\n",
      "Iteration 296, loss = 162.20971633\n",
      "Iteration 297, loss = 166.06262049\n",
      "Iteration 298, loss = 163.67285720\n",
      "Iteration 299, loss = 165.71181717\n",
      "Iteration 300, loss = 163.44630009\n",
      "Iteration 301, loss = 162.26654761\n",
      "Iteration 302, loss = 160.23180207\n",
      "Iteration 303, loss = 161.74674114\n",
      "Iteration 304, loss = 160.62174445\n",
      "Iteration 305, loss = 165.96527951\n",
      "Iteration 306, loss = 159.43114868\n",
      "Iteration 307, loss = 158.16479567\n",
      "Iteration 308, loss = 160.31689899\n",
      "Iteration 309, loss = 163.25844809\n",
      "Iteration 310, loss = 157.08013673\n",
      "Iteration 311, loss = 158.07376609\n",
      "Iteration 312, loss = 156.73598677\n",
      "Iteration 313, loss = 158.05284453\n",
      "Iteration 314, loss = 153.38639381\n",
      "Iteration 315, loss = 153.91464215\n",
      "Iteration 316, loss = 153.84098198\n",
      "Iteration 317, loss = 155.75570101\n",
      "Iteration 318, loss = 155.11968534\n",
      "Iteration 319, loss = 156.32306337\n",
      "Iteration 320, loss = 152.18485140\n",
      "Iteration 321, loss = 155.75936488\n",
      "Iteration 322, loss = 152.38613025\n",
      "Iteration 323, loss = 152.64737205\n",
      "Iteration 324, loss = 151.64863473\n",
      "Iteration 325, loss = 156.85036157\n",
      "Iteration 326, loss = 151.91737458\n",
      "Iteration 327, loss = 150.30464197\n",
      "Iteration 328, loss = 158.56194123\n",
      "Iteration 329, loss = 149.38964690\n",
      "Iteration 330, loss = 147.89202267\n",
      "Iteration 331, loss = 150.39412408\n",
      "Iteration 332, loss = 149.81463331\n",
      "Iteration 333, loss = 150.69250578\n",
      "Iteration 334, loss = 148.09108703\n",
      "Iteration 335, loss = 149.68077968\n",
      "Iteration 336, loss = 147.14545599\n",
      "Iteration 337, loss = 144.54293626\n",
      "Iteration 338, loss = 143.95723232\n",
      "Iteration 339, loss = 143.32059302\n",
      "Iteration 340, loss = 145.70315549\n",
      "Iteration 341, loss = 144.49742043\n",
      "Iteration 342, loss = 144.70805108\n",
      "Iteration 343, loss = 142.90153090\n",
      "Iteration 344, loss = 143.68815627\n",
      "Iteration 345, loss = 143.14994739\n",
      "Iteration 346, loss = 142.72384094\n",
      "Iteration 347, loss = 143.39680295\n",
      "Iteration 348, loss = 143.05446166\n",
      "Iteration 349, loss = 141.18311775\n",
      "Iteration 350, loss = 140.36088639\n",
      "Iteration 351, loss = 141.15190045\n",
      "Iteration 352, loss = 141.19661772\n",
      "Iteration 353, loss = 138.83523304\n",
      "Iteration 354, loss = 140.01907560\n",
      "Iteration 355, loss = 140.43648395\n",
      "Iteration 356, loss = 148.22425952\n",
      "Iteration 357, loss = 139.96442585\n",
      "Iteration 358, loss = 137.59540907\n",
      "Iteration 359, loss = 140.73085613\n",
      "Iteration 360, loss = 135.74009763\n",
      "Iteration 361, loss = 138.66560065\n",
      "Iteration 362, loss = 136.91197263\n",
      "Iteration 363, loss = 138.70292529\n",
      "Iteration 364, loss = 134.61300644\n",
      "Iteration 365, loss = 138.55084803\n",
      "Iteration 366, loss = 136.06895342\n",
      "Iteration 367, loss = 134.57469737\n",
      "Iteration 368, loss = 134.35872858\n",
      "Iteration 369, loss = 134.22576182\n",
      "Iteration 370, loss = 137.21839334\n",
      "Iteration 371, loss = 130.83656621\n",
      "Iteration 372, loss = 131.98508302\n",
      "Iteration 373, loss = 133.42473045\n",
      "Iteration 374, loss = 133.39886356\n",
      "Iteration 375, loss = 134.39839494\n",
      "Iteration 376, loss = 131.43992726\n",
      "Iteration 377, loss = 134.54703173\n",
      "Iteration 378, loss = 130.85243684\n",
      "Iteration 379, loss = 129.33595729\n",
      "Iteration 380, loss = 128.69334717\n",
      "Iteration 381, loss = 132.20719539\n",
      "Iteration 382, loss = 133.85933768\n",
      "Iteration 383, loss = 133.02605912\n",
      "Iteration 384, loss = 132.09918837\n",
      "Iteration 385, loss = 130.35826526\n",
      "Iteration 386, loss = 128.51997351\n",
      "Iteration 387, loss = 129.71693907\n",
      "Iteration 388, loss = 128.90519323\n",
      "Iteration 389, loss = 128.68603364\n",
      "Iteration 390, loss = 130.03422970\n",
      "Iteration 391, loss = 127.92454646\n",
      "Iteration 392, loss = 127.19401968\n",
      "Iteration 393, loss = 125.12519641\n",
      "Iteration 394, loss = 129.22710410\n",
      "Iteration 395, loss = 126.84749574\n",
      "Iteration 396, loss = 126.83237799\n",
      "Iteration 397, loss = 125.56630380\n",
      "Iteration 398, loss = 126.67279141\n",
      "Iteration 399, loss = 130.79126088\n",
      "Iteration 400, loss = 130.96369672\n",
      "Iteration 401, loss = 124.81023603\n",
      "Iteration 402, loss = 124.82943603\n",
      "Iteration 403, loss = 124.68229749\n",
      "Iteration 404, loss = 122.62689495\n",
      "Iteration 405, loss = 124.91179290\n",
      "Iteration 406, loss = 124.83335769\n",
      "Iteration 407, loss = 122.68646039\n",
      "Iteration 408, loss = 122.59352078\n",
      "Iteration 409, loss = 123.53026256\n",
      "Iteration 410, loss = 122.89459558\n",
      "Iteration 411, loss = 121.69940714\n",
      "Iteration 412, loss = 122.93100478\n",
      "Iteration 413, loss = 123.68437334\n",
      "Iteration 414, loss = 121.09821244\n",
      "Iteration 415, loss = 121.19997830\n",
      "Iteration 416, loss = 122.09537021\n",
      "Iteration 417, loss = 120.42425092\n",
      "Iteration 418, loss = 121.05976438\n",
      "Iteration 419, loss = 120.87801178\n",
      "Iteration 420, loss = 120.08435841\n",
      "Iteration 421, loss = 121.69966204\n",
      "Iteration 422, loss = 121.98462494\n",
      "Iteration 423, loss = 117.38227785\n",
      "Iteration 424, loss = 122.65979076\n",
      "Iteration 425, loss = 116.97121815\n",
      "Iteration 426, loss = 119.36929188\n",
      "Iteration 427, loss = 116.79401655\n",
      "Iteration 428, loss = 117.41788561\n",
      "Iteration 429, loss = 115.63498516\n",
      "Iteration 430, loss = 116.92568604\n",
      "Iteration 431, loss = 117.25165313\n",
      "Iteration 432, loss = 118.55350586\n",
      "Iteration 433, loss = 117.21142456\n",
      "Iteration 434, loss = 116.75805646\n",
      "Iteration 435, loss = 115.14955522\n",
      "Iteration 436, loss = 116.76958494\n",
      "Iteration 437, loss = 117.97747965\n",
      "Iteration 438, loss = 114.89394297\n",
      "Iteration 439, loss = 116.48714454\n",
      "Iteration 440, loss = 115.72592440\n",
      "Iteration 441, loss = 118.81227579\n",
      "Iteration 442, loss = 113.60525457\n",
      "Iteration 443, loss = 115.89537035\n",
      "Iteration 444, loss = 114.90170797\n",
      "Iteration 445, loss = 114.75526746\n",
      "Iteration 446, loss = 114.00809036\n",
      "Iteration 447, loss = 112.94553025\n",
      "Iteration 448, loss = 114.66723056\n",
      "Iteration 449, loss = 115.43463854\n",
      "Iteration 450, loss = 111.84262253\n",
      "Iteration 451, loss = 114.37091222\n",
      "Iteration 452, loss = 113.93474777\n",
      "Iteration 453, loss = 111.69861158\n",
      "Iteration 454, loss = 112.09076298\n",
      "Iteration 455, loss = 114.60308296\n",
      "Iteration 456, loss = 120.02467861\n",
      "Iteration 457, loss = 112.65506055\n",
      "Iteration 458, loss = 111.34940951\n",
      "Iteration 459, loss = 109.81099783\n",
      "Iteration 460, loss = 112.33538785\n",
      "Iteration 461, loss = 111.75607347\n",
      "Iteration 462, loss = 112.40106777\n",
      "Iteration 463, loss = 112.21944558\n",
      "Iteration 464, loss = 113.20884915\n",
      "Iteration 465, loss = 115.09222890\n",
      "Iteration 466, loss = 112.36603462\n",
      "Iteration 467, loss = 109.58936098\n",
      "Iteration 468, loss = 108.25655178\n",
      "Iteration 469, loss = 109.64717521\n",
      "Iteration 470, loss = 109.92509199\n",
      "Iteration 471, loss = 110.09140181\n",
      "Iteration 472, loss = 107.92465090\n",
      "Iteration 473, loss = 108.73316356\n",
      "Iteration 474, loss = 110.98966990\n",
      "Iteration 475, loss = 110.19470631\n",
      "Iteration 476, loss = 112.00799947\n",
      "Iteration 477, loss = 107.77447373\n",
      "Iteration 478, loss = 109.46456001\n",
      "Iteration 479, loss = 109.88963259\n",
      "Iteration 480, loss = 108.17085615\n",
      "Iteration 481, loss = 108.82650058\n",
      "Iteration 482, loss = 107.32383514\n",
      "Iteration 483, loss = 106.07383147\n",
      "Iteration 484, loss = 108.25850634\n",
      "Iteration 485, loss = 111.06171072\n",
      "Iteration 486, loss = 106.55887299\n",
      "Iteration 487, loss = 106.93444807\n",
      "Iteration 488, loss = 105.48638021\n",
      "Iteration 489, loss = 107.15743847\n",
      "Iteration 490, loss = 106.26407697\n",
      "Iteration 491, loss = 103.87879462\n",
      "Iteration 492, loss = 104.19420580\n",
      "Iteration 493, loss = 105.51766197\n",
      "Iteration 494, loss = 103.98058844\n",
      "Iteration 495, loss = 105.19293843\n",
      "Iteration 496, loss = 107.97163371\n",
      "Iteration 497, loss = 103.63803764\n",
      "Iteration 498, loss = 106.41074432\n",
      "Iteration 499, loss = 104.95951469\n",
      "Iteration 500, loss = 104.28787542\n",
      "Iteration 501, loss = 103.10816581\n",
      "Iteration 502, loss = 103.51399823\n",
      "Iteration 503, loss = 103.08553858\n",
      "Iteration 504, loss = 102.33979272\n",
      "Iteration 505, loss = 101.78254619\n",
      "Iteration 506, loss = 102.60514320\n",
      "Iteration 507, loss = 102.45235548\n",
      "Iteration 508, loss = 103.41405696\n",
      "Iteration 509, loss = 104.70988371\n",
      "Iteration 510, loss = 101.73144346\n",
      "Iteration 511, loss = 104.50313938\n",
      "Iteration 512, loss = 102.27240669\n",
      "Iteration 513, loss = 104.27354401\n",
      "Iteration 514, loss = 100.71730143\n",
      "Iteration 515, loss = 101.92769832\n",
      "Iteration 516, loss = 100.42671766\n",
      "Iteration 517, loss = 100.08407911\n",
      "Iteration 518, loss = 102.28922973\n",
      "Iteration 519, loss = 105.93842377\n",
      "Iteration 520, loss = 107.96627262\n",
      "Iteration 521, loss = 102.64066116\n",
      "Iteration 522, loss = 99.89488788\n",
      "Iteration 523, loss = 100.59543149\n",
      "Iteration 524, loss = 98.89081100\n",
      "Iteration 525, loss = 98.27524832\n",
      "Iteration 526, loss = 99.24145365\n",
      "Iteration 527, loss = 104.60917523\n",
      "Iteration 528, loss = 106.20139083\n",
      "Iteration 529, loss = 99.98035657\n",
      "Iteration 530, loss = 98.91751533\n",
      "Iteration 531, loss = 99.96936844\n",
      "Iteration 532, loss = 97.03225922\n",
      "Iteration 533, loss = 99.92110169\n",
      "Iteration 534, loss = 101.18198477\n",
      "Iteration 535, loss = 99.17740049\n",
      "Iteration 536, loss = 97.87344902\n",
      "Iteration 537, loss = 96.76117916\n",
      "Iteration 538, loss = 99.28693903\n",
      "Iteration 539, loss = 100.68872021\n",
      "Iteration 540, loss = 99.17396728\n",
      "Iteration 541, loss = 96.51155196\n",
      "Iteration 542, loss = 96.83007237\n",
      "Iteration 543, loss = 95.13081416\n",
      "Iteration 544, loss = 98.13547715\n",
      "Iteration 545, loss = 99.07967915\n",
      "Iteration 546, loss = 97.98245443\n",
      "Iteration 547, loss = 96.26592887\n",
      "Iteration 548, loss = 95.07258189\n",
      "Iteration 549, loss = 96.31135129\n",
      "Iteration 550, loss = 93.27818912\n",
      "Iteration 551, loss = 94.84025795\n",
      "Iteration 552, loss = 94.52505771\n",
      "Iteration 553, loss = 97.45931315\n",
      "Iteration 554, loss = 96.29952603\n",
      "Iteration 555, loss = 95.28878020\n",
      "Iteration 556, loss = 95.46803590\n",
      "Iteration 557, loss = 95.43889535\n",
      "Iteration 558, loss = 95.08465327\n",
      "Iteration 559, loss = 97.77055074\n",
      "Iteration 560, loss = 98.31119264\n",
      "Iteration 561, loss = 96.58254722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Fitting pipelines for count\n",
      "Iteration 1, loss = 31633.63409428\n",
      "Iteration 2, loss = 26082.74768820\n",
      "Iteration 3, loss = 13328.71908047\n",
      "Iteration 4, loss = 6500.93018104\n",
      "Iteration 5, loss = 5150.23534695\n",
      "Iteration 6, loss = 4555.30883482\n",
      "Iteration 7, loss = 4200.17081941\n",
      "Iteration 8, loss = 3929.53912385\n",
      "Iteration 9, loss = 3719.44472410\n",
      "Iteration 10, loss = 3553.81330790\n",
      "Iteration 11, loss = 3414.24892394\n",
      "Iteration 12, loss = 3297.31243100\n",
      "Iteration 13, loss = 3203.05928516\n",
      "Iteration 14, loss = 3119.06517863\n",
      "Iteration 15, loss = 3047.03482104\n",
      "Iteration 16, loss = 2980.71252268\n",
      "Iteration 17, loss = 2913.56871980\n",
      "Iteration 18, loss = 2857.12322034\n",
      "Iteration 19, loss = 2798.68769250\n",
      "Iteration 20, loss = 2738.34798394\n",
      "Iteration 21, loss = 2682.48578234\n",
      "Iteration 22, loss = 2627.71331146\n",
      "Iteration 23, loss = 2565.30061360\n",
      "Iteration 24, loss = 2515.25441447\n",
      "Iteration 25, loss = 2473.57021686\n",
      "Iteration 26, loss = 2426.71448432\n",
      "Iteration 27, loss = 2384.66557202\n",
      "Iteration 28, loss = 2345.94333700\n",
      "Iteration 29, loss = 2314.26557687\n",
      "Iteration 30, loss = 2289.24454615\n",
      "Iteration 31, loss = 2259.07908430\n",
      "Iteration 32, loss = 2228.69885441\n",
      "Iteration 33, loss = 2200.76403666\n",
      "Iteration 34, loss = 2174.24845970\n",
      "Iteration 35, loss = 2149.06784999\n",
      "Iteration 36, loss = 2125.91781378\n",
      "Iteration 37, loss = 2093.05262976\n",
      "Iteration 38, loss = 2066.63929229\n",
      "Iteration 39, loss = 2042.28564211\n",
      "Iteration 40, loss = 2013.97243746\n",
      "Iteration 41, loss = 1988.35898531\n",
      "Iteration 42, loss = 1960.78489847\n",
      "Iteration 43, loss = 1934.00673710\n",
      "Iteration 44, loss = 1904.48776869\n",
      "Iteration 45, loss = 1881.90106800\n",
      "Iteration 46, loss = 1851.52583656\n",
      "Iteration 47, loss = 1818.43684259\n",
      "Iteration 48, loss = 1797.39187900\n",
      "Iteration 49, loss = 1759.22135878\n",
      "Iteration 50, loss = 1735.74377557\n",
      "Iteration 51, loss = 1706.19430223\n",
      "Iteration 52, loss = 1681.41194184\n",
      "Iteration 53, loss = 1656.15933949\n",
      "Iteration 54, loss = 1630.61840656\n",
      "Iteration 55, loss = 1603.38286677\n",
      "Iteration 56, loss = 1583.59327871\n",
      "Iteration 57, loss = 1554.57630601\n",
      "Iteration 58, loss = 1524.19323367\n",
      "Iteration 59, loss = 1506.27945943\n",
      "Iteration 60, loss = 1478.27338242\n",
      "Iteration 61, loss = 1457.43531270\n",
      "Iteration 62, loss = 1431.86834705\n",
      "Iteration 63, loss = 1403.27157745\n",
      "Iteration 64, loss = 1385.30732661\n",
      "Iteration 65, loss = 1364.42634488\n",
      "Iteration 66, loss = 1345.43877060\n",
      "Iteration 67, loss = 1314.99542916\n",
      "Iteration 68, loss = 1296.57899124\n",
      "Iteration 69, loss = 1278.40527764\n",
      "Iteration 70, loss = 1254.77081081\n",
      "Iteration 71, loss = 1234.87962440\n",
      "Iteration 72, loss = 1215.96956017\n",
      "Iteration 73, loss = 1196.26394958\n",
      "Iteration 74, loss = 1175.14036160\n",
      "Iteration 75, loss = 1162.73779526\n",
      "Iteration 76, loss = 1140.73404753\n",
      "Iteration 77, loss = 1120.83959038\n",
      "Iteration 78, loss = 1100.34272102\n",
      "Iteration 79, loss = 1085.80561117\n",
      "Iteration 80, loss = 1070.01257369\n",
      "Iteration 81, loss = 1050.83530629\n",
      "Iteration 82, loss = 1037.81905579\n",
      "Iteration 83, loss = 1025.84793180\n",
      "Iteration 84, loss = 1007.77355078\n",
      "Iteration 85, loss = 993.44473114\n",
      "Iteration 86, loss = 974.97823931\n",
      "Iteration 87, loss = 965.71727682\n",
      "Iteration 88, loss = 951.81855079\n",
      "Iteration 89, loss = 937.68678914\n",
      "Iteration 90, loss = 923.53912941\n",
      "Iteration 91, loss = 912.50794137\n",
      "Iteration 92, loss = 897.36457437\n",
      "Iteration 93, loss = 889.48878271\n",
      "Iteration 94, loss = 872.52471604\n",
      "Iteration 95, loss = 864.02467954\n",
      "Iteration 96, loss = 852.19037170\n",
      "Iteration 97, loss = 848.30369564\n",
      "Iteration 98, loss = 833.19158588\n",
      "Iteration 99, loss = 822.88437979\n",
      "Iteration 100, loss = 814.64499794\n",
      "Iteration 101, loss = 805.21705518\n",
      "Iteration 102, loss = 792.49357158\n",
      "Iteration 103, loss = 779.70822509\n",
      "Iteration 104, loss = 777.08967149\n",
      "Iteration 105, loss = 765.06809081\n",
      "Iteration 106, loss = 758.88933265\n",
      "Iteration 107, loss = 749.16291180\n",
      "Iteration 108, loss = 738.02375724\n",
      "Iteration 109, loss = 730.36284020\n",
      "Iteration 110, loss = 731.55621918\n",
      "Iteration 111, loss = 716.05313717\n",
      "Iteration 112, loss = 707.13057183\n",
      "Iteration 113, loss = 699.95284396\n",
      "Iteration 114, loss = 694.25443917\n",
      "Iteration 115, loss = 686.00952072\n",
      "Iteration 116, loss = 681.55476137\n",
      "Iteration 117, loss = 672.12484551\n",
      "Iteration 118, loss = 668.73968126\n",
      "Iteration 119, loss = 652.09484557\n",
      "Iteration 120, loss = 649.25722744\n",
      "Iteration 121, loss = 643.27033132\n",
      "Iteration 122, loss = 637.73551767\n",
      "Iteration 123, loss = 635.18881668\n",
      "Iteration 124, loss = 637.70349959\n",
      "Iteration 125, loss = 615.51667609\n",
      "Iteration 126, loss = 609.82064138\n",
      "Iteration 127, loss = 605.09188639\n",
      "Iteration 128, loss = 603.01240856\n",
      "Iteration 129, loss = 597.99488829\n",
      "Iteration 130, loss = 599.85367403\n",
      "Iteration 131, loss = 590.30451284\n",
      "Iteration 132, loss = 581.84058144\n",
      "Iteration 133, loss = 581.30740508\n",
      "Iteration 134, loss = 572.79482830\n",
      "Iteration 135, loss = 565.38615540\n",
      "Iteration 136, loss = 562.88670645\n",
      "Iteration 137, loss = 558.39321272\n",
      "Iteration 138, loss = 558.46142040\n",
      "Iteration 139, loss = 547.21403125\n",
      "Iteration 140, loss = 547.07678164\n",
      "Iteration 141, loss = 544.83287031\n",
      "Iteration 142, loss = 530.25758947\n",
      "Iteration 143, loss = 531.46856310\n",
      "Iteration 144, loss = 524.88653525\n",
      "Iteration 145, loss = 520.56448298\n",
      "Iteration 146, loss = 515.36970753\n",
      "Iteration 147, loss = 511.78718525\n",
      "Iteration 148, loss = 508.15693367\n",
      "Iteration 149, loss = 503.11141221\n",
      "Iteration 150, loss = 502.56915502\n",
      "Iteration 151, loss = 498.85222047\n",
      "Iteration 152, loss = 497.73898705\n",
      "Iteration 153, loss = 490.24012005\n",
      "Iteration 154, loss = 483.31530200\n",
      "Iteration 155, loss = 486.74387723\n",
      "Iteration 156, loss = 483.51837517\n",
      "Iteration 157, loss = 478.39472857\n",
      "Iteration 158, loss = 476.94711550\n",
      "Iteration 159, loss = 472.46809127\n",
      "Iteration 160, loss = 462.97804072\n",
      "Iteration 161, loss = 461.49830082\n",
      "Iteration 162, loss = 460.71844768\n",
      "Iteration 163, loss = 451.31219267\n",
      "Iteration 164, loss = 450.62097590\n",
      "Iteration 165, loss = 446.88368024\n",
      "Iteration 166, loss = 449.88597896\n",
      "Iteration 167, loss = 446.22693180\n",
      "Iteration 168, loss = 438.81074382\n",
      "Iteration 169, loss = 441.91023782\n",
      "Iteration 170, loss = 437.58318764\n",
      "Iteration 171, loss = 431.71296810\n",
      "Iteration 172, loss = 426.13010915\n",
      "Iteration 173, loss = 426.09097262\n",
      "Iteration 174, loss = 423.37598569\n",
      "Iteration 175, loss = 419.22506587\n",
      "Iteration 176, loss = 413.60320506\n",
      "Iteration 177, loss = 415.54791247\n",
      "Iteration 178, loss = 411.67690910\n",
      "Iteration 179, loss = 410.92164343\n",
      "Iteration 180, loss = 408.34581671\n",
      "Iteration 181, loss = 409.01792446\n",
      "Iteration 182, loss = 399.78216372\n",
      "Iteration 183, loss = 398.64762638\n",
      "Iteration 184, loss = 398.01649502\n",
      "Iteration 185, loss = 392.29733020\n",
      "Iteration 186, loss = 392.61709814\n",
      "Iteration 187, loss = 386.86076850\n",
      "Iteration 188, loss = 386.99658331\n",
      "Iteration 189, loss = 387.18560798\n",
      "Iteration 190, loss = 382.34183441\n",
      "Iteration 191, loss = 379.72464719\n",
      "Iteration 192, loss = 375.95046537\n",
      "Iteration 193, loss = 376.28541106\n",
      "Iteration 194, loss = 369.09124295\n",
      "Iteration 195, loss = 366.90119451\n",
      "Iteration 196, loss = 367.53735308\n",
      "Iteration 197, loss = 370.25086325\n",
      "Iteration 198, loss = 367.27160497\n",
      "Iteration 199, loss = 359.92923904\n",
      "Iteration 200, loss = 357.25320896\n",
      "Iteration 201, loss = 360.43513370\n",
      "Iteration 202, loss = 357.18780755\n",
      "Iteration 203, loss = 351.23837083\n",
      "Iteration 204, loss = 350.95666414\n",
      "Iteration 205, loss = 350.84026748\n",
      "Iteration 206, loss = 346.31162529\n",
      "Iteration 207, loss = 343.73092204\n",
      "Iteration 208, loss = 340.71248841\n",
      "Iteration 209, loss = 340.82587034\n",
      "Iteration 210, loss = 339.38861125\n",
      "Iteration 211, loss = 334.00860899\n",
      "Iteration 212, loss = 334.56251351\n",
      "Iteration 213, loss = 332.49974053\n",
      "Iteration 214, loss = 332.63242760\n",
      "Iteration 215, loss = 330.39356635\n",
      "Iteration 216, loss = 328.59186284\n",
      "Iteration 217, loss = 324.32656554\n",
      "Iteration 218, loss = 321.78427293\n",
      "Iteration 219, loss = 320.20486103\n",
      "Iteration 220, loss = 325.08746310\n",
      "Iteration 221, loss = 317.76975653\n",
      "Iteration 222, loss = 315.93327431\n",
      "Iteration 223, loss = 314.88281320\n",
      "Iteration 224, loss = 311.95142749\n",
      "Iteration 225, loss = 315.62205979\n",
      "Iteration 226, loss = 314.62565782\n",
      "Iteration 227, loss = 308.41180089\n",
      "Iteration 228, loss = 309.29352359\n",
      "Iteration 229, loss = 311.70636307\n",
      "Iteration 230, loss = 306.27065056\n",
      "Iteration 231, loss = 304.95610425\n",
      "Iteration 232, loss = 304.53662459\n",
      "Iteration 233, loss = 298.00721888\n",
      "Iteration 234, loss = 298.54835199\n",
      "Iteration 235, loss = 296.54007949\n",
      "Iteration 236, loss = 295.92297135\n",
      "Iteration 237, loss = 292.59586404\n",
      "Iteration 238, loss = 290.35388509\n",
      "Iteration 239, loss = 293.47480694\n",
      "Iteration 240, loss = 291.46201856\n",
      "Iteration 241, loss = 287.00142280\n",
      "Iteration 242, loss = 285.39325129\n",
      "Iteration 243, loss = 287.50920267\n",
      "Iteration 244, loss = 292.52018696\n",
      "Iteration 245, loss = 286.90075963\n",
      "Iteration 246, loss = 282.41028297\n",
      "Iteration 247, loss = 281.01958643\n",
      "Iteration 248, loss = 277.06346650\n",
      "Iteration 249, loss = 274.47850154\n",
      "Iteration 250, loss = 277.84664816\n",
      "Iteration 251, loss = 272.52712628\n",
      "Iteration 252, loss = 272.80529800\n",
      "Iteration 253, loss = 270.47666626\n",
      "Iteration 254, loss = 268.05803809\n",
      "Iteration 255, loss = 268.75110835\n",
      "Iteration 256, loss = 270.95404484\n",
      "Iteration 257, loss = 268.67971952\n",
      "Iteration 258, loss = 260.42514897\n",
      "Iteration 259, loss = 265.41406745\n",
      "Iteration 260, loss = 272.38039520\n",
      "Iteration 261, loss = 260.30213846\n",
      "Iteration 262, loss = 261.34833790\n",
      "Iteration 263, loss = 259.91046768\n",
      "Iteration 264, loss = 260.63588526\n",
      "Iteration 265, loss = 256.11095942\n",
      "Iteration 266, loss = 254.72930799\n",
      "Iteration 267, loss = 253.51919257\n",
      "Iteration 268, loss = 251.05735619\n",
      "Iteration 269, loss = 250.34457443\n",
      "Iteration 270, loss = 248.08161375\n",
      "Iteration 271, loss = 251.40430196\n",
      "Iteration 272, loss = 243.80372831\n",
      "Iteration 273, loss = 246.77193971\n",
      "Iteration 274, loss = 251.05102150\n",
      "Iteration 275, loss = 247.47886135\n",
      "Iteration 276, loss = 247.62821503\n",
      "Iteration 277, loss = 247.49978471\n",
      "Iteration 278, loss = 243.00567407\n",
      "Iteration 279, loss = 236.16030983\n",
      "Iteration 280, loss = 237.37155027\n",
      "Iteration 281, loss = 237.32046088\n",
      "Iteration 282, loss = 236.84288178\n",
      "Iteration 283, loss = 239.70904034\n",
      "Iteration 284, loss = 235.97929191\n",
      "Iteration 285, loss = 231.86525505\n",
      "Iteration 286, loss = 234.02079097\n",
      "Iteration 287, loss = 229.26654198\n",
      "Iteration 288, loss = 232.89074484\n",
      "Iteration 289, loss = 227.06678998\n",
      "Iteration 290, loss = 225.63299551\n",
      "Iteration 291, loss = 231.01165649\n",
      "Iteration 292, loss = 224.45354513\n",
      "Iteration 293, loss = 228.40338699\n",
      "Iteration 294, loss = 223.89868211\n",
      "Iteration 295, loss = 222.43614194\n",
      "Iteration 296, loss = 223.29791109\n",
      "Iteration 297, loss = 222.42508104\n",
      "Iteration 298, loss = 220.19031026\n",
      "Iteration 299, loss = 222.27855013\n",
      "Iteration 300, loss = 219.41088247\n",
      "Iteration 301, loss = 215.44005427\n",
      "Iteration 302, loss = 215.69509826\n",
      "Iteration 303, loss = 218.08461801\n",
      "Iteration 304, loss = 215.93986095\n",
      "Iteration 305, loss = 212.18859013\n",
      "Iteration 306, loss = 215.50919153\n",
      "Iteration 307, loss = 213.34015756\n",
      "Iteration 308, loss = 211.26830489\n",
      "Iteration 309, loss = 210.39236704\n",
      "Iteration 310, loss = 208.92980190\n",
      "Iteration 311, loss = 207.48722969\n",
      "Iteration 312, loss = 206.81410021\n",
      "Iteration 313, loss = 205.55184364\n",
      "Iteration 314, loss = 204.50188026\n",
      "Iteration 315, loss = 206.71808153\n",
      "Iteration 316, loss = 204.93490497\n",
      "Iteration 317, loss = 207.05466554\n",
      "Iteration 318, loss = 202.52104650\n",
      "Iteration 319, loss = 205.16955286\n",
      "Iteration 320, loss = 199.98968752\n",
      "Iteration 321, loss = 198.74406105\n",
      "Iteration 322, loss = 196.97014388\n",
      "Iteration 323, loss = 202.41770868\n",
      "Iteration 324, loss = 197.44459326\n",
      "Iteration 325, loss = 197.30594292\n",
      "Iteration 326, loss = 195.61483656\n",
      "Iteration 327, loss = 200.04651426\n",
      "Iteration 328, loss = 194.00037035\n",
      "Iteration 329, loss = 194.96820982\n",
      "Iteration 330, loss = 193.99019278\n",
      "Iteration 331, loss = 191.47537557\n",
      "Iteration 332, loss = 197.09992990\n",
      "Iteration 333, loss = 191.33927929\n",
      "Iteration 334, loss = 192.97458263\n",
      "Iteration 335, loss = 186.93658441\n",
      "Iteration 336, loss = 188.72983063\n",
      "Iteration 337, loss = 189.66177926\n",
      "Iteration 338, loss = 189.37637624\n",
      "Iteration 339, loss = 189.86764823\n",
      "Iteration 340, loss = 185.95230528\n",
      "Iteration 341, loss = 185.98489225\n",
      "Iteration 342, loss = 184.74435761\n",
      "Iteration 343, loss = 189.27524706\n",
      "Iteration 344, loss = 183.31080690\n",
      "Iteration 345, loss = 185.56534527\n",
      "Iteration 346, loss = 187.03497522\n",
      "Iteration 347, loss = 183.52611369\n",
      "Iteration 348, loss = 182.38843208\n",
      "Iteration 349, loss = 182.64074032\n",
      "Iteration 350, loss = 178.81748448\n",
      "Iteration 351, loss = 184.38311920\n",
      "Iteration 352, loss = 178.71090082\n",
      "Iteration 353, loss = 180.97379689\n",
      "Iteration 354, loss = 178.83297711\n",
      "Iteration 355, loss = 174.25573015\n",
      "Iteration 356, loss = 175.12547869\n",
      "Iteration 357, loss = 176.54107639\n",
      "Iteration 358, loss = 175.91828724\n",
      "Iteration 359, loss = 176.70105335\n",
      "Iteration 360, loss = 178.25633465\n",
      "Iteration 361, loss = 171.00233260\n",
      "Iteration 362, loss = 173.11415387\n",
      "Iteration 363, loss = 171.84577003\n",
      "Iteration 364, loss = 170.77886734\n",
      "Iteration 365, loss = 173.12835095\n",
      "Iteration 366, loss = 170.99873741\n",
      "Iteration 367, loss = 169.03406177\n",
      "Iteration 368, loss = 170.31305557\n",
      "Iteration 369, loss = 166.87938352\n",
      "Iteration 370, loss = 172.09113767\n",
      "Iteration 371, loss = 170.87856227\n",
      "Iteration 372, loss = 165.09896036\n",
      "Iteration 373, loss = 167.90888784\n",
      "Iteration 374, loss = 165.03838189\n",
      "Iteration 375, loss = 164.58509682\n",
      "Iteration 376, loss = 163.75143727\n",
      "Iteration 377, loss = 164.74800744\n",
      "Iteration 378, loss = 162.76931585\n",
      "Iteration 379, loss = 164.17652987\n",
      "Iteration 380, loss = 162.56268710\n",
      "Iteration 381, loss = 164.47282443\n",
      "Iteration 382, loss = 168.05768438\n",
      "Iteration 383, loss = 162.42607278\n",
      "Iteration 384, loss = 159.92345584\n",
      "Iteration 385, loss = 161.24711132\n",
      "Iteration 386, loss = 161.63826498\n",
      "Iteration 387, loss = 159.48472835\n",
      "Iteration 388, loss = 163.39120059\n",
      "Iteration 389, loss = 159.81603649\n",
      "Iteration 390, loss = 156.55463216\n",
      "Iteration 391, loss = 156.50468581\n",
      "Iteration 392, loss = 157.53641825\n",
      "Iteration 393, loss = 157.03113869\n",
      "Iteration 394, loss = 160.01992085\n",
      "Iteration 395, loss = 157.71594502\n",
      "Iteration 396, loss = 157.16524878\n",
      "Iteration 397, loss = 153.17633108\n",
      "Iteration 398, loss = 154.50121582\n",
      "Iteration 399, loss = 153.86816754\n",
      "Iteration 400, loss = 151.17189649\n",
      "Iteration 401, loss = 152.02384297\n",
      "Iteration 402, loss = 154.98895752\n",
      "Iteration 403, loss = 149.93080264\n",
      "Iteration 404, loss = 148.65796076\n",
      "Iteration 405, loss = 147.92205241\n",
      "Iteration 406, loss = 149.18285175\n",
      "Iteration 407, loss = 149.85480315\n",
      "Iteration 408, loss = 150.04082223\n",
      "Iteration 409, loss = 148.18070562\n",
      "Iteration 410, loss = 148.36592435\n",
      "Iteration 411, loss = 147.20203477\n",
      "Iteration 412, loss = 149.53196995\n",
      "Iteration 413, loss = 148.07796588\n",
      "Iteration 414, loss = 145.08573988\n",
      "Iteration 415, loss = 151.61008419\n",
      "Iteration 416, loss = 148.90732208\n",
      "Iteration 417, loss = 145.43659104\n",
      "Iteration 418, loss = 147.98704179\n",
      "Iteration 419, loss = 145.54485894\n",
      "Iteration 420, loss = 145.94452084\n",
      "Iteration 421, loss = 143.12137989\n",
      "Iteration 422, loss = 142.24248089\n",
      "Iteration 423, loss = 142.39584029\n",
      "Iteration 424, loss = 142.95769618\n",
      "Iteration 425, loss = 143.62673495\n",
      "Iteration 426, loss = 141.80186628\n",
      "Iteration 427, loss = 142.16986791\n",
      "Iteration 428, loss = 142.44005488\n",
      "Iteration 429, loss = 141.81352978\n",
      "Iteration 430, loss = 143.94061866\n",
      "Iteration 431, loss = 139.20333612\n",
      "Iteration 432, loss = 139.55551185\n",
      "Iteration 433, loss = 141.39554884\n",
      "Iteration 434, loss = 141.30245435\n",
      "Iteration 435, loss = 139.13893171\n",
      "Iteration 436, loss = 138.08613994\n",
      "Iteration 437, loss = 138.11510109\n",
      "Iteration 438, loss = 137.24219987\n",
      "Iteration 439, loss = 136.38699000\n",
      "Iteration 440, loss = 135.64505786\n",
      "Iteration 441, loss = 135.64969126\n",
      "Iteration 442, loss = 132.47281142\n",
      "Iteration 443, loss = 133.61920417\n",
      "Iteration 444, loss = 133.12018907\n",
      "Iteration 445, loss = 132.66110379\n",
      "Iteration 446, loss = 134.01098071\n",
      "Iteration 447, loss = 133.68015287\n",
      "Iteration 448, loss = 132.37175193\n",
      "Iteration 449, loss = 131.97296673\n",
      "Iteration 450, loss = 133.64371283\n",
      "Iteration 451, loss = 137.00350190\n",
      "Iteration 452, loss = 133.81729877\n",
      "Iteration 453, loss = 131.23559165\n",
      "Iteration 454, loss = 131.60833508\n",
      "Iteration 455, loss = 131.63195093\n",
      "Iteration 456, loss = 132.95481295\n",
      "Iteration 457, loss = 131.15013830\n",
      "Iteration 458, loss = 131.11705253\n",
      "Iteration 459, loss = 130.64921064\n",
      "Iteration 460, loss = 128.99143198\n",
      "Iteration 461, loss = 129.99726612\n",
      "Iteration 462, loss = 130.62139981\n",
      "Iteration 463, loss = 128.85287786\n",
      "Iteration 464, loss = 129.25111816\n",
      "Iteration 465, loss = 127.07498056\n",
      "Iteration 466, loss = 127.77276895\n",
      "Iteration 467, loss = 127.29622433\n",
      "Iteration 468, loss = 125.84716390\n",
      "Iteration 469, loss = 127.56794589\n",
      "Iteration 470, loss = 126.61593717\n",
      "Iteration 471, loss = 128.02600099\n",
      "Iteration 472, loss = 127.87569281\n",
      "Iteration 473, loss = 126.34441585\n",
      "Iteration 474, loss = 125.31122880\n",
      "Iteration 475, loss = 124.92502007\n",
      "Iteration 476, loss = 131.32998151\n",
      "Iteration 477, loss = 124.87109978\n",
      "Iteration 478, loss = 123.59674056\n",
      "Iteration 479, loss = 123.30799744\n",
      "Iteration 480, loss = 124.35008597\n",
      "Iteration 481, loss = 121.99885491\n",
      "Iteration 482, loss = 121.46401529\n",
      "Iteration 483, loss = 125.58522194\n",
      "Iteration 484, loss = 123.41153278\n",
      "Iteration 485, loss = 121.51664542\n",
      "Iteration 486, loss = 122.10193012\n",
      "Iteration 487, loss = 122.37450371\n",
      "Iteration 488, loss = 121.91712617\n",
      "Iteration 489, loss = 122.61269225\n",
      "Iteration 490, loss = 118.70726747\n",
      "Iteration 491, loss = 121.11907966\n",
      "Iteration 492, loss = 120.89626398\n",
      "Iteration 493, loss = 118.49162769\n",
      "Iteration 494, loss = 117.66289364\n",
      "Iteration 495, loss = 119.79233365\n",
      "Iteration 496, loss = 118.70049927\n",
      "Iteration 497, loss = 121.07296673\n",
      "Iteration 498, loss = 118.24431227\n",
      "Iteration 499, loss = 115.68601118\n",
      "Iteration 500, loss = 117.07126903\n",
      "Iteration 501, loss = 116.77276718\n",
      "Iteration 502, loss = 114.44282681\n",
      "Iteration 503, loss = 116.02137198\n",
      "Iteration 504, loss = 116.78572033\n",
      "Iteration 505, loss = 119.85519237\n",
      "Iteration 506, loss = 115.49605758\n",
      "Iteration 507, loss = 115.64254221\n",
      "Iteration 508, loss = 112.54206830\n",
      "Iteration 509, loss = 115.69094295\n",
      "Iteration 510, loss = 117.02634523\n",
      "Iteration 511, loss = 115.14852003\n",
      "Iteration 512, loss = 111.54017880\n",
      "Iteration 513, loss = 111.89620347\n",
      "Iteration 514, loss = 111.83786935\n",
      "Iteration 515, loss = 115.72202255\n",
      "Iteration 516, loss = 113.23855199\n",
      "Iteration 517, loss = 113.72241646\n",
      "Iteration 518, loss = 112.06682661\n",
      "Iteration 519, loss = 111.38856304\n",
      "Iteration 520, loss = 111.94368513\n",
      "Iteration 521, loss = 112.35979401\n",
      "Iteration 522, loss = 112.32507572\n",
      "Iteration 523, loss = 111.59993463\n",
      "Iteration 524, loss = 110.45946403\n",
      "Iteration 525, loss = 111.32343482\n",
      "Iteration 526, loss = 110.91140360\n",
      "Iteration 527, loss = 111.65338926\n",
      "Iteration 528, loss = 108.95246043\n",
      "Iteration 529, loss = 108.66718138\n",
      "Iteration 530, loss = 111.88749913\n",
      "Iteration 531, loss = 108.92237342\n",
      "Iteration 532, loss = 108.88588484\n",
      "Iteration 533, loss = 108.16489561\n",
      "Iteration 534, loss = 109.51828802\n",
      "Iteration 535, loss = 106.76537322\n",
      "Iteration 536, loss = 109.72434967\n",
      "Iteration 537, loss = 107.52975206\n",
      "Iteration 538, loss = 108.90490525\n",
      "Iteration 539, loss = 108.36587941\n",
      "Iteration 540, loss = 107.71295792\n",
      "Iteration 541, loss = 108.13226555\n",
      "Iteration 542, loss = 108.40535378\n",
      "Iteration 543, loss = 107.46822791\n",
      "Iteration 544, loss = 108.55790863\n",
      "Iteration 545, loss = 108.49438534\n",
      "Iteration 546, loss = 106.58525951\n",
      "Iteration 547, loss = 106.10024518\n",
      "Iteration 548, loss = 106.30896236\n",
      "Iteration 549, loss = 105.48868214\n",
      "Iteration 550, loss = 105.72892158\n",
      "Iteration 551, loss = 106.62529149\n",
      "Iteration 552, loss = 106.94389172\n",
      "Iteration 553, loss = 104.81561223\n",
      "Iteration 554, loss = 106.51620583\n",
      "Iteration 555, loss = 105.79824120\n",
      "Iteration 556, loss = 105.66975247\n",
      "Iteration 557, loss = 103.85864354\n",
      "Iteration 558, loss = 102.95503954\n",
      "Iteration 559, loss = 106.33515949\n",
      "Iteration 560, loss = 104.40452091\n",
      "Iteration 561, loss = 103.56422424\n",
      "Iteration 562, loss = 108.41869328\n",
      "Iteration 563, loss = 106.16640523\n",
      "Iteration 564, loss = 104.97021212\n",
      "Iteration 565, loss = 100.65318825\n",
      "Iteration 566, loss = 101.97911786\n",
      "Iteration 567, loss = 102.62877033\n",
      "Iteration 568, loss = 100.76004384\n",
      "Iteration 569, loss = 100.99072458\n",
      "Iteration 570, loss = 102.10022964\n",
      "Iteration 571, loss = 101.81599212\n",
      "Iteration 572, loss = 100.20738575\n",
      "Iteration 573, loss = 100.57343510\n",
      "Iteration 574, loss = 101.83886632\n",
      "Iteration 575, loss = 100.51556141\n",
      "Iteration 576, loss = 102.31561325\n",
      "Iteration 577, loss = 101.01959351\n",
      "Iteration 578, loss = 100.55220357\n",
      "Iteration 579, loss = 101.05967761\n",
      "Iteration 580, loss = 98.98834081\n",
      "Iteration 581, loss = 99.93740945\n",
      "Iteration 582, loss = 99.80383021\n",
      "Iteration 583, loss = 98.73058063\n",
      "Iteration 584, loss = 101.10476472\n",
      "Iteration 585, loss = 100.02303010\n",
      "Iteration 586, loss = 98.18110674\n",
      "Iteration 587, loss = 99.30987194\n",
      "Iteration 588, loss = 104.61925204\n",
      "Iteration 589, loss = 98.91101200\n",
      "Iteration 590, loss = 99.63208770\n",
      "Iteration 591, loss = 98.01163860\n",
      "Iteration 592, loss = 96.19705733\n",
      "Iteration 593, loss = 96.00029701\n",
      "Iteration 594, loss = 96.55966198\n",
      "Iteration 595, loss = 95.74522073\n",
      "Iteration 596, loss = 97.21521375\n",
      "Iteration 597, loss = 95.71021015\n",
      "Iteration 598, loss = 99.08692839\n",
      "Iteration 599, loss = 96.11598334\n",
      "Iteration 600, loss = 96.82847750\n",
      "Iteration 601, loss = 95.51385974\n",
      "Iteration 602, loss = 95.01600605\n",
      "Iteration 603, loss = 95.27455203\n",
      "Iteration 604, loss = 95.40377304\n",
      "Iteration 605, loss = 96.28102691\n",
      "Iteration 606, loss = 94.74382947\n",
      "Iteration 607, loss = 94.99502360\n",
      "Iteration 608, loss = 95.78881224\n",
      "Iteration 609, loss = 94.24355936\n",
      "Iteration 610, loss = 96.11775569\n",
      "Iteration 611, loss = 94.75026414\n",
      "Iteration 612, loss = 94.88206973\n",
      "Iteration 613, loss = 92.11222680\n",
      "Iteration 614, loss = 92.42098649\n",
      "Iteration 615, loss = 92.41229230\n",
      "Iteration 616, loss = 94.92665278\n",
      "Iteration 617, loss = 93.36667473\n",
      "Iteration 618, loss = 93.11522481\n",
      "Iteration 619, loss = 93.29189314\n",
      "Iteration 620, loss = 94.02173521\n",
      "Iteration 621, loss = 91.44060995\n",
      "Iteration 622, loss = 93.65096175\n",
      "Iteration 623, loss = 96.24100078\n",
      "Iteration 624, loss = 92.74685289\n",
      "Iteration 625, loss = 92.30134615\n",
      "Iteration 626, loss = 90.81019503\n",
      "Iteration 627, loss = 90.47652018\n",
      "Iteration 628, loss = 91.53750997\n",
      "Iteration 629, loss = 89.17374946\n",
      "Iteration 630, loss = 91.26996440\n",
      "Iteration 631, loss = 92.31529045\n",
      "Iteration 632, loss = 94.13011441\n",
      "Iteration 633, loss = 95.96740533\n",
      "Iteration 634, loss = 91.57937617\n",
      "Iteration 635, loss = 91.10395531\n",
      "Iteration 636, loss = 91.58897973\n",
      "Iteration 637, loss = 89.45474893\n",
      "Iteration 638, loss = 91.67071315\n",
      "Iteration 639, loss = 91.74501365\n",
      "Iteration 640, loss = 89.44654980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 31544.44896635\n",
      "Iteration 2, loss = 24298.89240952\n",
      "Iteration 3, loss = 10633.16236579\n",
      "Iteration 4, loss = 5958.37027243\n",
      "Iteration 5, loss = 4823.52109914\n",
      "Iteration 6, loss = 4269.55190139\n",
      "Iteration 7, loss = 3918.87940452\n",
      "Iteration 8, loss = 3669.49386019\n",
      "Iteration 9, loss = 3469.74861531\n",
      "Iteration 10, loss = 3312.39088612\n",
      "Iteration 11, loss = 3186.20610331\n",
      "Iteration 12, loss = 3068.12376329\n",
      "Iteration 13, loss = 2970.31699531\n",
      "Iteration 14, loss = 2886.13623040\n",
      "Iteration 15, loss = 2808.40867894\n",
      "Iteration 16, loss = 2730.17886068\n",
      "Iteration 17, loss = 2660.45459252\n",
      "Iteration 18, loss = 2588.82185238\n",
      "Iteration 19, loss = 2516.31130642\n",
      "Iteration 20, loss = 2442.61356011\n",
      "Iteration 21, loss = 2378.97847840\n",
      "Iteration 22, loss = 2313.65016323\n",
      "Iteration 23, loss = 2260.88947070\n",
      "Iteration 24, loss = 2209.94553814\n",
      "Iteration 25, loss = 2163.26578943\n",
      "Iteration 26, loss = 2118.99491018\n",
      "Iteration 27, loss = 2079.36675516\n",
      "Iteration 28, loss = 2040.25707362\n",
      "Iteration 29, loss = 2006.09600385\n",
      "Iteration 30, loss = 1973.09592633\n",
      "Iteration 31, loss = 1946.38654104\n",
      "Iteration 32, loss = 1909.98832231\n",
      "Iteration 33, loss = 1882.23004967\n",
      "Iteration 34, loss = 1855.01957990\n",
      "Iteration 35, loss = 1828.52811293\n",
      "Iteration 36, loss = 1800.93595310\n",
      "Iteration 37, loss = 1776.11064256\n",
      "Iteration 38, loss = 1750.21624899\n",
      "Iteration 39, loss = 1730.78857709\n",
      "Iteration 40, loss = 1706.39792970\n",
      "Iteration 41, loss = 1688.52921605\n",
      "Iteration 42, loss = 1657.00575055\n",
      "Iteration 43, loss = 1634.80863753\n",
      "Iteration 44, loss = 1610.58462393\n",
      "Iteration 45, loss = 1585.36051647\n",
      "Iteration 46, loss = 1561.54262331\n",
      "Iteration 47, loss = 1541.16343747\n",
      "Iteration 48, loss = 1521.67519495\n",
      "Iteration 49, loss = 1493.34062379\n",
      "Iteration 50, loss = 1473.21499573\n",
      "Iteration 51, loss = 1451.07936583\n",
      "Iteration 52, loss = 1429.28128791\n",
      "Iteration 53, loss = 1404.94313757\n",
      "Iteration 54, loss = 1388.69247359\n",
      "Iteration 55, loss = 1363.66520927\n",
      "Iteration 56, loss = 1339.36169664\n",
      "Iteration 57, loss = 1321.12414002\n",
      "Iteration 58, loss = 1301.07003564\n",
      "Iteration 59, loss = 1278.75281707\n",
      "Iteration 60, loss = 1259.93125420\n",
      "Iteration 61, loss = 1241.64242145\n",
      "Iteration 62, loss = 1218.87768650\n",
      "Iteration 63, loss = 1206.47474768\n",
      "Iteration 64, loss = 1188.28768165\n",
      "Iteration 65, loss = 1171.28083319\n",
      "Iteration 66, loss = 1152.16932054\n",
      "Iteration 67, loss = 1127.37488036\n",
      "Iteration 68, loss = 1111.58205305\n",
      "Iteration 69, loss = 1100.97685569\n",
      "Iteration 70, loss = 1080.55854943\n",
      "Iteration 71, loss = 1062.08814301\n",
      "Iteration 72, loss = 1051.89709862\n",
      "Iteration 73, loss = 1046.60719502\n",
      "Iteration 74, loss = 1022.59124533\n",
      "Iteration 75, loss = 1008.27929595\n",
      "Iteration 76, loss = 991.23920714\n",
      "Iteration 77, loss = 975.58356510\n",
      "Iteration 78, loss = 967.42707170\n",
      "Iteration 79, loss = 950.68100717\n",
      "Iteration 80, loss = 940.06964739\n",
      "Iteration 81, loss = 928.39141030\n",
      "Iteration 82, loss = 912.57998137\n",
      "Iteration 83, loss = 898.14207361\n",
      "Iteration 84, loss = 889.34246952\n",
      "Iteration 85, loss = 874.42658654\n",
      "Iteration 86, loss = 862.40741058\n",
      "Iteration 87, loss = 862.33943464\n",
      "Iteration 88, loss = 849.10289598\n",
      "Iteration 89, loss = 833.11022341\n",
      "Iteration 90, loss = 822.38996865\n",
      "Iteration 91, loss = 815.29636823\n",
      "Iteration 92, loss = 802.54613645\n",
      "Iteration 93, loss = 796.35249690\n",
      "Iteration 94, loss = 783.21119744\n",
      "Iteration 95, loss = 771.39312655\n",
      "Iteration 96, loss = 769.39392570\n",
      "Iteration 97, loss = 758.97181052\n",
      "Iteration 98, loss = 745.95243782\n",
      "Iteration 99, loss = 734.96986243\n",
      "Iteration 100, loss = 734.06833871\n",
      "Iteration 101, loss = 734.04774026\n",
      "Iteration 102, loss = 715.08094595\n",
      "Iteration 103, loss = 709.76504228\n",
      "Iteration 104, loss = 695.69337875\n",
      "Iteration 105, loss = 701.43971887\n",
      "Iteration 106, loss = 686.45022681\n",
      "Iteration 107, loss = 680.68414392\n",
      "Iteration 108, loss = 671.59643410\n",
      "Iteration 109, loss = 668.73845040\n",
      "Iteration 110, loss = 662.26591607\n",
      "Iteration 111, loss = 653.26176049\n",
      "Iteration 112, loss = 645.13308457\n",
      "Iteration 113, loss = 644.51245610\n",
      "Iteration 114, loss = 637.22682567\n",
      "Iteration 115, loss = 628.39224266\n",
      "Iteration 116, loss = 624.25458291\n",
      "Iteration 117, loss = 614.62477608\n",
      "Iteration 118, loss = 611.33762607\n",
      "Iteration 119, loss = 607.15039847\n",
      "Iteration 120, loss = 606.34545717\n",
      "Iteration 121, loss = 603.61727403\n",
      "Iteration 122, loss = 589.96783607\n",
      "Iteration 123, loss = 587.54951307\n",
      "Iteration 124, loss = 582.16347011\n",
      "Iteration 125, loss = 582.54998313\n",
      "Iteration 126, loss = 570.98600030\n",
      "Iteration 127, loss = 562.75921601\n",
      "Iteration 128, loss = 560.88132748\n",
      "Iteration 129, loss = 557.58924324\n",
      "Iteration 130, loss = 554.10906425\n",
      "Iteration 131, loss = 553.85101615\n",
      "Iteration 132, loss = 553.03812163\n",
      "Iteration 133, loss = 540.26840625\n",
      "Iteration 134, loss = 545.23900920\n",
      "Iteration 135, loss = 528.84236063\n",
      "Iteration 136, loss = 526.79883154\n",
      "Iteration 137, loss = 522.86732293\n",
      "Iteration 138, loss = 526.10273357\n",
      "Iteration 139, loss = 518.82749361\n",
      "Iteration 140, loss = 513.42733359\n",
      "Iteration 141, loss = 512.03878329\n",
      "Iteration 142, loss = 510.03304298\n",
      "Iteration 143, loss = 498.73682069\n",
      "Iteration 144, loss = 501.45960584\n",
      "Iteration 145, loss = 492.05275674\n",
      "Iteration 146, loss = 490.84906835\n",
      "Iteration 147, loss = 483.43434274\n",
      "Iteration 148, loss = 486.25876723\n",
      "Iteration 149, loss = 480.73796516\n",
      "Iteration 150, loss = 478.98221438\n",
      "Iteration 151, loss = 480.05157374\n",
      "Iteration 152, loss = 475.31555942\n",
      "Iteration 153, loss = 469.98775441\n",
      "Iteration 154, loss = 467.10091054\n",
      "Iteration 155, loss = 466.81929990\n",
      "Iteration 156, loss = 458.98207062\n",
      "Iteration 157, loss = 460.20027020\n",
      "Iteration 158, loss = 456.43891765\n",
      "Iteration 159, loss = 455.25982322\n",
      "Iteration 160, loss = 449.56593671\n",
      "Iteration 161, loss = 443.94572839\n",
      "Iteration 162, loss = 439.05447550\n",
      "Iteration 163, loss = 441.59013145\n",
      "Iteration 164, loss = 443.27765026\n",
      "Iteration 165, loss = 438.93153651\n",
      "Iteration 166, loss = 435.56434493\n",
      "Iteration 167, loss = 428.01053688\n",
      "Iteration 168, loss = 432.78146881\n",
      "Iteration 169, loss = 424.19786690\n",
      "Iteration 170, loss = 421.23580688\n",
      "Iteration 171, loss = 416.59902764\n",
      "Iteration 172, loss = 416.25115617\n",
      "Iteration 173, loss = 413.39818118\n",
      "Iteration 174, loss = 411.95264529\n",
      "Iteration 175, loss = 415.77258069\n",
      "Iteration 176, loss = 405.94199989\n",
      "Iteration 177, loss = 400.34779344\n",
      "Iteration 178, loss = 406.39269117\n",
      "Iteration 179, loss = 404.93493922\n",
      "Iteration 180, loss = 398.88288535\n",
      "Iteration 181, loss = 398.48166149\n",
      "Iteration 182, loss = 394.29854200\n",
      "Iteration 183, loss = 389.48063216\n",
      "Iteration 184, loss = 395.10108816\n",
      "Iteration 185, loss = 389.81530928\n",
      "Iteration 186, loss = 385.92592041\n",
      "Iteration 187, loss = 383.60335750\n",
      "Iteration 188, loss = 384.22302107\n",
      "Iteration 189, loss = 381.93839005\n",
      "Iteration 190, loss = 378.63609155\n",
      "Iteration 191, loss = 382.07583565\n",
      "Iteration 192, loss = 377.76128595\n",
      "Iteration 193, loss = 375.80974882\n",
      "Iteration 194, loss = 378.33016006\n",
      "Iteration 195, loss = 368.72546879\n",
      "Iteration 196, loss = 365.74615391\n",
      "Iteration 197, loss = 366.01117572\n",
      "Iteration 198, loss = 364.31974977\n",
      "Iteration 199, loss = 361.11263059\n",
      "Iteration 200, loss = 357.66312877\n",
      "Iteration 201, loss = 360.91622599\n",
      "Iteration 202, loss = 367.52620833\n",
      "Iteration 203, loss = 358.19076618\n",
      "Iteration 204, loss = 360.35591414\n",
      "Iteration 205, loss = 353.31931200\n",
      "Iteration 206, loss = 353.63690929\n",
      "Iteration 207, loss = 351.09718293\n",
      "Iteration 208, loss = 353.47401192\n",
      "Iteration 209, loss = 347.18058018\n",
      "Iteration 210, loss = 344.87616646\n",
      "Iteration 211, loss = 342.58554109\n",
      "Iteration 212, loss = 340.04865381\n",
      "Iteration 213, loss = 339.21962298\n",
      "Iteration 214, loss = 338.85339503\n",
      "Iteration 215, loss = 340.91913869\n",
      "Iteration 216, loss = 330.90239172\n",
      "Iteration 217, loss = 337.65770329\n",
      "Iteration 218, loss = 333.06391585\n",
      "Iteration 219, loss = 329.23389156\n",
      "Iteration 220, loss = 334.72498099\n",
      "Iteration 221, loss = 330.97370265\n",
      "Iteration 222, loss = 327.73362397\n",
      "Iteration 223, loss = 326.36459030\n",
      "Iteration 224, loss = 326.39334627\n",
      "Iteration 225, loss = 322.27679709\n",
      "Iteration 226, loss = 316.98151426\n",
      "Iteration 227, loss = 319.78617984\n",
      "Iteration 228, loss = 319.17342346\n",
      "Iteration 229, loss = 315.46367809\n",
      "Iteration 230, loss = 315.52732199\n",
      "Iteration 231, loss = 316.61640968\n",
      "Iteration 232, loss = 315.69177270\n",
      "Iteration 233, loss = 313.67474186\n",
      "Iteration 234, loss = 311.16352744\n",
      "Iteration 235, loss = 311.67015895\n",
      "Iteration 236, loss = 306.35623907\n",
      "Iteration 237, loss = 307.55319194\n",
      "Iteration 238, loss = 306.11268395\n",
      "Iteration 239, loss = 303.60965298\n",
      "Iteration 240, loss = 303.05617337\n",
      "Iteration 241, loss = 307.93777058\n",
      "Iteration 242, loss = 297.19837274\n",
      "Iteration 243, loss = 303.79575062\n",
      "Iteration 244, loss = 296.93031528\n",
      "Iteration 245, loss = 296.80338414\n",
      "Iteration 246, loss = 300.54237373\n",
      "Iteration 247, loss = 301.96314582\n",
      "Iteration 248, loss = 291.01654034\n",
      "Iteration 249, loss = 291.89994590\n",
      "Iteration 250, loss = 296.75694501\n",
      "Iteration 251, loss = 299.95044675\n",
      "Iteration 252, loss = 292.07427955\n",
      "Iteration 253, loss = 284.33591583\n",
      "Iteration 254, loss = 284.59897320\n",
      "Iteration 255, loss = 281.14316112\n",
      "Iteration 256, loss = 285.08988846\n",
      "Iteration 257, loss = 282.54518568\n",
      "Iteration 258, loss = 278.64554285\n",
      "Iteration 259, loss = 283.14473024\n",
      "Iteration 260, loss = 286.74710355\n",
      "Iteration 261, loss = 280.33007669\n",
      "Iteration 262, loss = 280.03819865\n",
      "Iteration 263, loss = 276.31571834\n",
      "Iteration 264, loss = 277.69547997\n",
      "Iteration 265, loss = 276.56943428\n",
      "Iteration 266, loss = 277.69492072\n",
      "Iteration 267, loss = 274.94704995\n",
      "Iteration 268, loss = 275.82154971\n",
      "Iteration 269, loss = 273.77772454\n",
      "Iteration 270, loss = 268.64467803\n",
      "Iteration 271, loss = 269.66292039\n",
      "Iteration 272, loss = 268.94307256\n",
      "Iteration 273, loss = 273.07070939\n",
      "Iteration 274, loss = 267.90160251\n",
      "Iteration 275, loss = 266.87682823\n",
      "Iteration 276, loss = 262.72373612\n",
      "Iteration 277, loss = 267.27372513\n",
      "Iteration 278, loss = 267.07055513\n",
      "Iteration 279, loss = 259.01498776\n",
      "Iteration 280, loss = 266.41394272\n",
      "Iteration 281, loss = 261.89975086\n",
      "Iteration 282, loss = 262.60505550\n",
      "Iteration 283, loss = 259.37342988\n",
      "Iteration 284, loss = 257.14782437\n",
      "Iteration 285, loss = 258.22139321\n",
      "Iteration 286, loss = 261.15894837\n",
      "Iteration 287, loss = 257.71967305\n",
      "Iteration 288, loss = 255.46289143\n",
      "Iteration 289, loss = 261.13233561\n",
      "Iteration 290, loss = 260.25072075\n",
      "Iteration 291, loss = 254.53849967\n",
      "Iteration 292, loss = 252.90306630\n",
      "Iteration 293, loss = 251.01493111\n",
      "Iteration 294, loss = 250.52409319\n",
      "Iteration 295, loss = 248.38219247\n",
      "Iteration 296, loss = 246.38863894\n",
      "Iteration 297, loss = 254.17013962\n",
      "Iteration 298, loss = 258.34113553\n",
      "Iteration 299, loss = 248.65331873\n",
      "Iteration 300, loss = 245.21785970\n",
      "Iteration 301, loss = 243.30005252\n",
      "Iteration 302, loss = 245.11236102\n",
      "Iteration 303, loss = 243.13967048\n",
      "Iteration 304, loss = 241.53190257\n",
      "Iteration 305, loss = 241.01228681\n",
      "Iteration 306, loss = 244.44342159\n",
      "Iteration 307, loss = 243.42438690\n",
      "Iteration 308, loss = 241.29483540\n",
      "Iteration 309, loss = 240.44428185\n",
      "Iteration 310, loss = 240.58385956\n",
      "Iteration 311, loss = 238.69554120\n",
      "Iteration 312, loss = 238.49718082\n",
      "Iteration 313, loss = 234.20164282\n",
      "Iteration 314, loss = 238.83745042\n",
      "Iteration 315, loss = 232.11146217\n",
      "Iteration 316, loss = 233.33602569\n",
      "Iteration 317, loss = 233.10240613\n",
      "Iteration 318, loss = 231.63308017\n",
      "Iteration 319, loss = 231.83998661\n",
      "Iteration 320, loss = 232.31320556\n",
      "Iteration 321, loss = 231.74413594\n",
      "Iteration 322, loss = 232.88248839\n",
      "Iteration 323, loss = 230.47948665\n",
      "Iteration 324, loss = 225.90573487\n",
      "Iteration 325, loss = 227.36465344\n",
      "Iteration 326, loss = 229.23398260\n",
      "Iteration 327, loss = 230.03880188\n",
      "Iteration 328, loss = 229.62498890\n",
      "Iteration 329, loss = 230.54729925\n",
      "Iteration 330, loss = 231.05826387\n",
      "Iteration 331, loss = 229.89098361\n",
      "Iteration 332, loss = 221.04349084\n",
      "Iteration 333, loss = 224.93550849\n",
      "Iteration 334, loss = 224.03258398\n",
      "Iteration 335, loss = 225.16157609\n",
      "Iteration 336, loss = 223.68447354\n",
      "Iteration 337, loss = 220.61821944\n",
      "Iteration 338, loss = 219.59773310\n",
      "Iteration 339, loss = 219.20021281\n",
      "Iteration 340, loss = 216.98832674\n",
      "Iteration 341, loss = 218.12722095\n",
      "Iteration 342, loss = 216.72203335\n",
      "Iteration 343, loss = 218.04539673\n",
      "Iteration 344, loss = 218.80411513\n",
      "Iteration 345, loss = 217.77769924\n",
      "Iteration 346, loss = 215.42160859\n",
      "Iteration 347, loss = 214.63051512\n",
      "Iteration 348, loss = 212.89332162\n",
      "Iteration 349, loss = 214.74031374\n",
      "Iteration 350, loss = 210.90033833\n",
      "Iteration 351, loss = 213.73036956\n",
      "Iteration 352, loss = 210.94315875\n",
      "Iteration 353, loss = 209.30067750\n",
      "Iteration 354, loss = 209.54795287\n",
      "Iteration 355, loss = 208.86595327\n",
      "Iteration 356, loss = 210.22413446\n",
      "Iteration 357, loss = 211.77025456\n",
      "Iteration 358, loss = 211.13282545\n",
      "Iteration 359, loss = 207.21762391\n",
      "Iteration 360, loss = 208.29210383\n",
      "Iteration 361, loss = 205.95485240\n",
      "Iteration 362, loss = 204.85594803\n",
      "Iteration 363, loss = 212.72956029\n",
      "Iteration 364, loss = 210.68300991\n",
      "Iteration 365, loss = 205.00305708\n",
      "Iteration 366, loss = 206.62644027\n",
      "Iteration 367, loss = 203.86449755\n",
      "Iteration 368, loss = 204.64385662\n",
      "Iteration 369, loss = 207.12517459\n",
      "Iteration 370, loss = 203.67364072\n",
      "Iteration 371, loss = 205.90441507\n",
      "Iteration 372, loss = 201.25406555\n",
      "Iteration 373, loss = 198.85761705\n",
      "Iteration 374, loss = 201.56269871\n",
      "Iteration 375, loss = 199.14768245\n",
      "Iteration 376, loss = 199.31797851\n",
      "Iteration 377, loss = 199.69942103\n",
      "Iteration 378, loss = 198.81761476\n",
      "Iteration 379, loss = 199.01079870\n",
      "Iteration 380, loss = 197.48015073\n",
      "Iteration 381, loss = 195.00141190\n",
      "Iteration 382, loss = 196.58367723\n",
      "Iteration 383, loss = 198.75863694\n",
      "Iteration 384, loss = 195.99510271\n",
      "Iteration 385, loss = 197.17802536\n",
      "Iteration 386, loss = 194.38940720\n",
      "Iteration 387, loss = 194.68072537\n",
      "Iteration 388, loss = 193.62421695\n",
      "Iteration 389, loss = 194.48751200\n",
      "Iteration 390, loss = 193.81535897\n",
      "Iteration 391, loss = 197.48258008\n",
      "Iteration 392, loss = 194.05383361\n",
      "Iteration 393, loss = 188.49323159\n",
      "Iteration 394, loss = 188.34036222\n",
      "Iteration 395, loss = 188.45475812\n",
      "Iteration 396, loss = 193.59357085\n",
      "Iteration 397, loss = 188.14189026\n",
      "Iteration 398, loss = 191.47280056\n",
      "Iteration 399, loss = 188.10715358\n",
      "Iteration 400, loss = 187.17115825\n",
      "Iteration 401, loss = 185.92646495\n",
      "Iteration 402, loss = 186.63467428\n",
      "Iteration 403, loss = 189.16015090\n",
      "Iteration 404, loss = 186.16038508\n",
      "Iteration 405, loss = 187.40787863\n",
      "Iteration 406, loss = 183.80247479\n",
      "Iteration 407, loss = 183.04259765\n",
      "Iteration 408, loss = 183.36552851\n",
      "Iteration 409, loss = 185.92239218\n",
      "Iteration 410, loss = 185.48505201\n",
      "Iteration 411, loss = 181.16527587\n",
      "Iteration 412, loss = 186.29532672\n",
      "Iteration 413, loss = 184.05163369\n",
      "Iteration 414, loss = 183.24285110\n",
      "Iteration 415, loss = 178.99836955\n",
      "Iteration 416, loss = 183.89129698\n",
      "Iteration 417, loss = 182.57028368\n",
      "Iteration 418, loss = 179.56898153\n",
      "Iteration 419, loss = 178.07752227\n",
      "Iteration 420, loss = 179.86938615\n",
      "Iteration 421, loss = 179.08102698\n",
      "Iteration 422, loss = 176.90701319\n",
      "Iteration 423, loss = 178.82592763\n",
      "Iteration 424, loss = 177.88536455\n",
      "Iteration 425, loss = 179.20720924\n",
      "Iteration 426, loss = 178.74099480\n",
      "Iteration 427, loss = 172.73788679\n",
      "Iteration 428, loss = 174.50043165\n",
      "Iteration 429, loss = 178.34643007\n",
      "Iteration 430, loss = 175.98694078\n",
      "Iteration 431, loss = 173.70721420\n",
      "Iteration 432, loss = 176.59214609\n",
      "Iteration 433, loss = 176.81777918\n",
      "Iteration 434, loss = 179.32429892\n",
      "Iteration 435, loss = 184.02307028\n",
      "Iteration 436, loss = 178.46804460\n",
      "Iteration 437, loss = 172.75451154\n",
      "Iteration 438, loss = 172.03073144\n",
      "Iteration 439, loss = 169.31885527\n",
      "Iteration 440, loss = 168.91149164\n",
      "Iteration 441, loss = 170.89994805\n",
      "Iteration 442, loss = 170.46147838\n",
      "Iteration 443, loss = 172.09400664\n",
      "Iteration 444, loss = 173.87319765\n",
      "Iteration 445, loss = 170.79706339\n",
      "Iteration 446, loss = 176.11547030\n",
      "Iteration 447, loss = 167.66091545\n",
      "Iteration 448, loss = 171.83567209\n",
      "Iteration 449, loss = 169.21546841\n",
      "Iteration 450, loss = 165.41846166\n",
      "Iteration 451, loss = 168.01986766\n",
      "Iteration 452, loss = 165.48524261\n",
      "Iteration 453, loss = 167.31003499\n",
      "Iteration 454, loss = 167.57330294\n",
      "Iteration 455, loss = 167.42415320\n",
      "Iteration 456, loss = 165.33221333\n",
      "Iteration 457, loss = 165.93687343\n",
      "Iteration 458, loss = 167.36773159\n",
      "Iteration 459, loss = 167.46678936\n",
      "Iteration 460, loss = 166.42468403\n",
      "Iteration 461, loss = 165.81103548\n",
      "Iteration 462, loss = 161.93449703\n",
      "Iteration 463, loss = 161.42258097\n",
      "Iteration 464, loss = 163.07366036\n",
      "Iteration 465, loss = 165.20894022\n",
      "Iteration 466, loss = 162.61678536\n",
      "Iteration 467, loss = 161.24305707\n",
      "Iteration 468, loss = 165.32656588\n",
      "Iteration 469, loss = 163.26454596\n",
      "Iteration 470, loss = 160.96321512\n",
      "Iteration 471, loss = 157.95500142\n",
      "Iteration 472, loss = 161.23384639\n",
      "Iteration 473, loss = 159.53955227\n",
      "Iteration 474, loss = 161.19386038\n",
      "Iteration 475, loss = 165.31951798\n",
      "Iteration 476, loss = 160.14840936\n",
      "Iteration 477, loss = 167.04298600\n",
      "Iteration 478, loss = 160.93742249\n",
      "Iteration 479, loss = 158.34910564\n",
      "Iteration 480, loss = 158.07183406\n",
      "Iteration 481, loss = 157.38590455\n",
      "Iteration 482, loss = 160.47004322\n",
      "Iteration 483, loss = 158.70164467\n",
      "Iteration 484, loss = 156.95319912\n",
      "Iteration 485, loss = 162.32806456\n",
      "Iteration 486, loss = 156.38393741\n",
      "Iteration 487, loss = 154.35439990\n",
      "Iteration 488, loss = 157.60264352\n",
      "Iteration 489, loss = 156.52864186\n",
      "Iteration 490, loss = 155.88678071\n",
      "Iteration 491, loss = 155.04863496\n",
      "Iteration 492, loss = 156.05052294\n",
      "Iteration 493, loss = 157.90815005\n",
      "Iteration 494, loss = 155.86459220\n",
      "Iteration 495, loss = 151.77194862\n",
      "Iteration 496, loss = 153.90174857\n",
      "Iteration 497, loss = 154.57232869\n",
      "Iteration 498, loss = 154.83487739\n",
      "Iteration 499, loss = 157.65214756\n",
      "Iteration 500, loss = 157.54081587\n",
      "Iteration 501, loss = 158.26760904\n",
      "Iteration 502, loss = 153.51913881\n",
      "Iteration 503, loss = 152.12770818\n",
      "Iteration 504, loss = 161.49813167\n",
      "Iteration 505, loss = 151.47420465\n",
      "Iteration 506, loss = 152.99356081\n",
      "Iteration 507, loss = 153.50039927\n",
      "Iteration 508, loss = 150.25802154\n",
      "Iteration 509, loss = 159.99459608\n",
      "Iteration 510, loss = 153.17993722\n",
      "Iteration 511, loss = 149.75243509\n",
      "Iteration 512, loss = 148.74092174\n",
      "Iteration 513, loss = 150.09111577\n",
      "Iteration 514, loss = 149.78632394\n",
      "Iteration 515, loss = 147.99508715\n",
      "Iteration 516, loss = 154.20462034\n",
      "Iteration 517, loss = 146.88322487\n",
      "Iteration 518, loss = 147.19899154\n",
      "Iteration 519, loss = 150.95497452\n",
      "Iteration 520, loss = 153.34355149\n",
      "Iteration 521, loss = 149.58517746\n",
      "Iteration 522, loss = 144.98296090\n",
      "Iteration 523, loss = 146.49572248\n",
      "Iteration 524, loss = 146.65201833\n",
      "Iteration 525, loss = 149.38741632\n",
      "Iteration 526, loss = 147.44536307\n",
      "Iteration 527, loss = 145.61420935\n",
      "Iteration 528, loss = 146.60386950\n",
      "Iteration 529, loss = 146.22002516\n",
      "Iteration 530, loss = 145.64317815\n",
      "Iteration 531, loss = 143.48376424\n",
      "Iteration 532, loss = 147.49650674\n",
      "Iteration 533, loss = 145.86115853\n",
      "Iteration 534, loss = 150.83379024\n",
      "Iteration 535, loss = 144.21089223\n",
      "Iteration 536, loss = 146.45667221\n",
      "Iteration 537, loss = 144.00467937\n",
      "Iteration 538, loss = 145.39760990\n",
      "Iteration 539, loss = 144.50758835\n",
      "Iteration 540, loss = 144.66620442\n",
      "Iteration 541, loss = 142.17215385\n",
      "Iteration 542, loss = 140.01319724\n",
      "Iteration 543, loss = 143.71884388\n",
      "Iteration 544, loss = 144.53306260\n",
      "Iteration 545, loss = 144.85557094\n",
      "Iteration 546, loss = 144.24175031\n",
      "Iteration 547, loss = 144.60046835\n",
      "Iteration 548, loss = 147.13235964\n",
      "Iteration 549, loss = 144.26893168\n",
      "Iteration 550, loss = 139.54882930\n",
      "Iteration 551, loss = 140.25867958\n",
      "Iteration 552, loss = 140.61936085\n",
      "Iteration 553, loss = 141.18530552\n",
      "Iteration 554, loss = 141.40275100\n",
      "Iteration 555, loss = 139.14048040\n",
      "Iteration 556, loss = 146.33898203\n",
      "Iteration 557, loss = 141.61393065\n",
      "Iteration 558, loss = 141.86240334\n",
      "Iteration 559, loss = 140.78340149\n",
      "Iteration 560, loss = 137.66352467\n",
      "Iteration 561, loss = 138.71084284\n",
      "Iteration 562, loss = 135.16830353\n",
      "Iteration 563, loss = 155.39579948\n",
      "Iteration 564, loss = 140.75070958\n",
      "Iteration 565, loss = 138.58984443\n",
      "Iteration 566, loss = 135.09713966\n",
      "Iteration 567, loss = 135.89514029\n",
      "Iteration 568, loss = 135.71860647\n",
      "Iteration 569, loss = 135.72814699\n",
      "Iteration 570, loss = 133.76074315\n",
      "Iteration 571, loss = 134.81022182\n",
      "Iteration 572, loss = 134.08010259\n",
      "Iteration 573, loss = 139.13913619\n",
      "Iteration 574, loss = 137.03448260\n",
      "Iteration 575, loss = 135.50478617\n",
      "Iteration 576, loss = 132.24323444\n",
      "Iteration 577, loss = 135.73854718\n",
      "Iteration 578, loss = 135.19481107\n",
      "Iteration 579, loss = 135.17687854\n",
      "Iteration 580, loss = 134.12407859\n",
      "Iteration 581, loss = 136.00427483\n",
      "Iteration 582, loss = 133.64707722\n",
      "Iteration 583, loss = 135.55493552\n",
      "Iteration 584, loss = 138.92964089\n",
      "Iteration 585, loss = 133.00909759\n",
      "Iteration 586, loss = 131.70450484\n",
      "Iteration 587, loss = 133.04993894\n",
      "Iteration 588, loss = 132.34459275\n",
      "Iteration 589, loss = 134.15811259\n",
      "Iteration 590, loss = 132.27765578\n",
      "Iteration 591, loss = 132.20841401\n",
      "Iteration 592, loss = 134.46408795\n",
      "Iteration 593, loss = 131.61685514\n",
      "Iteration 594, loss = 131.53005025\n",
      "Iteration 595, loss = 131.92628805\n",
      "Iteration 596, loss = 131.21419436\n",
      "Iteration 597, loss = 131.89944580\n",
      "Iteration 598, loss = 129.74372102\n",
      "Iteration 599, loss = 132.36450831\n",
      "Iteration 600, loss = 130.94179072\n",
      "Iteration 601, loss = 131.56355044\n",
      "Iteration 602, loss = 132.08997175\n",
      "Iteration 603, loss = 132.06586717\n",
      "Iteration 604, loss = 134.53833757\n",
      "Iteration 605, loss = 131.58503611\n",
      "Iteration 606, loss = 129.49453386\n",
      "Iteration 607, loss = 130.71021495\n",
      "Iteration 608, loss = 129.53165846\n",
      "Iteration 609, loss = 129.46960210\n",
      "Iteration 610, loss = 131.99267539\n",
      "Iteration 611, loss = 128.63352640\n",
      "Iteration 612, loss = 128.02653136\n",
      "Iteration 613, loss = 127.49817470\n",
      "Iteration 614, loss = 129.27823697\n",
      "Iteration 615, loss = 125.72317747\n",
      "Iteration 616, loss = 129.05985464\n",
      "Iteration 617, loss = 124.44129942\n",
      "Iteration 618, loss = 127.92711961\n",
      "Iteration 619, loss = 128.53524486\n",
      "Iteration 620, loss = 126.32581156\n",
      "Iteration 621, loss = 126.37185782\n",
      "Iteration 622, loss = 128.59957824\n",
      "Iteration 623, loss = 126.95903213\n",
      "Iteration 624, loss = 126.81598205\n",
      "Iteration 625, loss = 124.45610920\n",
      "Iteration 626, loss = 126.55479357\n",
      "Iteration 627, loss = 126.27919929\n",
      "Iteration 628, loss = 125.12679172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "Predicting pipelines...\n",
      "Predicting pipelines for casual\n",
      "\n",
      "\n",
      "Predicting pipelines for registered\n",
      "\n",
      "\n",
      "Predicting pipelines for count\n",
      "\n",
      "\n",
      "                             mse        mae\n",
      "negative_casual       451.547204  12.052224\n",
      "positive_casual       588.734934  13.111705\n",
      "negative_registered  2738.840303  35.518597\n",
      "positive_registered  2631.898421  35.347177\n",
      "negative_count       3746.749772  41.560231\n",
      "positive_count       3940.795138  42.550231\n"
     ]
    }
   ],
   "source": [
    "columns_not_to_use = ['datetime', 'dataset', 'day', 'month', 'year', 'casual', 'registered', 'count']\n",
    "target_columns = ['casual', 'registered', 'count']\n",
    "directions = ['negative', 'positive']\n",
    "rf = True\n",
    "mlp = True\n",
    "trees = 50\n",
    "max_iter = 2000\n",
    "hidden_layer_sizes = (100, 100)\n",
    "\n",
    "# create the pipelines\n",
    "pipelines = create_pipelines(target_columns, directions, \n",
    "                             rf=rf, mlp=mlp, trees=trees, \n",
    "                             max_iter = 2000, hidden_layer_sizes = (100, 100))\n",
    "\n",
    "test_period = 4\n",
    "lag_period = 2\n",
    "maximum_day = 20\n",
    "\n",
    "# fit the pipelines\n",
    "fitted_pipelines = fit_pipelines(pipelines, training_data, \n",
    "                                 target_columns, columns_not_to_use,\n",
    "                                 directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# predict the pipelines\n",
    "predictions, dataframes = predict_pipelines(fitted_pipelines, training_data, \n",
    "                                target_columns, columns_not_to_use, \n",
    "                                test_period, lag_period, maximum_day,\n",
    "                                directions, rf=rf, mlp=mlp)\n",
    "\n",
    "# evaluate the pipelines\n",
    "evaluation = evaluate_pipelines(predictions, training_data, target_columns, directions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
